{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SemanticScholarSearch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMTxy6ZSJwt9VtRWRiyqhJi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BecomeAllan/S2Search/blob/main/SemanticScholarSearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S5xfBIUWDWZ"
      },
      "source": [
        "# Consumindo a API do SemanticScholar\n",
        "\n",
        "A seguir, tem uma classe chamada `Search()`, que ao instanciar-la em uma variável é possível fazer pesquisas sobre papers utilizando a api do SemanticScholar, dentre os parâmetros temos:\n",
        "\n",
        "- Buscar: Pesquisas sobre tópicos onde adicionar tópicos utiliza-se + (mais) e remover tópicos usamos - (menos)\n",
        "\n",
        "  ex. \"Machine+Medicine\"\n",
        "\n",
        "- Fields: O que será retornado como dados. Para utilizar, escolha dentre as opções sem utilizar espaço e separadas de virgulas:\n",
        "  - (str): externalIds\n",
        "  - (str): url\n",
        "  - (str): title\n",
        "  - (str): abstract\n",
        "  - (str): venue \n",
        "  - (str): year \n",
        "  - (str): referenceCount\n",
        "  - (str): citationCount\n",
        "  - (str): influentialCitationCount\n",
        "  - (str): isOpenAccess\n",
        "  - list (str): fieldsOfStudy\n",
        "  - list (str): authors \n",
        "\n",
        "  ex. \"title,abstract,isOpenAccess,fieldsOfStudy\"\n",
        "\n",
        "- Offset: Número que começa a puxar a partir da ordem dele a lista de papers. (0 seria o primeiro)\n",
        "\n",
        "- Limite: Número de papers a ser retornados (Máx. 10.000)\n",
        "\n",
        "**Obs:** A api do SemanticScholar disponibiliza 100 query's a cada 5 min, no qual apenas retorna no máx. 100 resutados (limite). Assim a cada 5 min, é possível puxar 10.000 papers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF-CJR6nVaZV",
        "cellView": "form"
      },
      "source": [
        "#@title Classe para pesquisa no SemanticScholar\n",
        "import IPython\n",
        "from google.colab import output\n",
        "import pandas as pd\n",
        "\n",
        "class Search():\n",
        "  def __init__(self, **kwargs):\n",
        "    self.data = \"\"\n",
        "    self.data_0 = \"\"\n",
        "\n",
        "    self.search = kwargs.get('search', None)\n",
        "    self.fields = kwargs.get('fields', None)\n",
        "    self.limit = kwargs.get('limit', None)\n",
        "    self.offset = kwargs.get('offset', None)\n",
        "\n",
        "    if self.search == None and self.fields == None and self.limit == None and self.offset == None:\n",
        "      self._start(False)\n",
        "    else:\n",
        "      self._start(True)\n",
        "  \n",
        "  def _start(self, *args):\n",
        "\n",
        "    output.register_callback('notebook.searching', self._searching)\n",
        "    output.register_callback('notebook.AddListItem', self._add_list_item)\n",
        "    output.register_callback('notebook.mergeData', self._merge_data)\n",
        "    output.register_callback('notebook.error', self._error)\n",
        "\n",
        "\n",
        "    boxs = ''' \n",
        "        <label for=\"query\">Buscar: </label>\n",
        "        <input type=\"text\" id=\"query\" value=\"Machine Learning+Deep Learning\" style=\"width: 400px;\"/>\n",
        "        <br/>\n",
        "        <br/>\n",
        "        \n",
        "        <label for=\"fields\">Fields: </label>\n",
        "        <input type=\"text\" id=\"fields\" value=\"title,abstract,isOpenAccess,fieldsOfStudy\" style=\"width: 400px;\"/>\n",
        "        <br/>\n",
        "        <br/>\n",
        " \n",
        "        <label for=\"limit\">Limite: </label>\n",
        "        <input type=\"text\" id=\"limit\" value=\"10\" style=\"width: 50px;\"/><br/>\n",
        "        <br/>\n",
        "\n",
        "        <label for=\"limit\">Offset: </label>\n",
        "        <input type=\"text\" id=\"offset\" value=\"0\" style=\"width: 50px;\"/><br/>\n",
        "        <br/>\n",
        "\n",
        "        <button id='button'>Pesquisar</button>\n",
        "        <br/>\n",
        "        <br/>\n",
        "           '''\n",
        "\n",
        "    button = ''' document.querySelector('#button').onclick = async () => ''' # {}\n",
        "\n",
        "    search_query = '''\n",
        "            var search = document.getElementById(\"query\").value\n",
        "            var fields = document.getElementById(\"fields\").value\n",
        "            var limit = parseInt(document.getElementById(\"limit\").value)\n",
        "            var offset = parseInt(document.getElementById(\"offset\").value)\n",
        "                  '''\n",
        "    search_params = '''\n",
        "            var search = \"{search}\"\n",
        "            var fields = \"{fields}\"\n",
        "            var limit = parseInt({limit})\n",
        "            var offset = parseInt({offset})\n",
        "                  '''\n",
        "    engine = '''\n",
        "            google.colab.kernel.invokeFunction('notebook.searching', [], {});\n",
        "\n",
        "            if (limit >100) {\n",
        "              var number = limit\n",
        "              var data = \"\"\n",
        "              var promises = []\n",
        "              var offsetSearch = 0\n",
        "              var rest = 0\n",
        "\n",
        "              for (let index = 0; index < Math.floor(limit/100); index++) {\n",
        "                offsetSearch = 100*(index) + offset + 1*(index!==0)\n",
        "\n",
        "\n",
        "                promises.push(\n",
        "                  fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offsetSearch}&limit=100&fields=${fields}`)\n",
        "    .then(res=> {return(res.json())})\n",
        "    .then(res=> {return(res)})\n",
        "                )\n",
        "              }\n",
        "              \n",
        "              if (limit%100 !== 0) { \n",
        "                rest= limit%100\n",
        "                offsetSearch = offsetSearch+100\n",
        "                \n",
        "                console.log(rest)\n",
        "                console.log(offsetSearch)\n",
        "\n",
        "                promises.push(\n",
        "                fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offsetSearch}&limit=${rest}&fields=${fields}`)\n",
        "    .then(res=> {return(res.json())})\n",
        "    .then(res=> {return(res)})\n",
        "                )}\n",
        "\n",
        "              await Promise.all(promises).then(data=>{\n",
        "                google.colab.kernel.invokeFunction('notebook.mergeData', [data], {})\n",
        "              })\n",
        "              .catch(err=> { return (google.colab.kernel.invokeFunction('notebook.error', [err], {})) })\n",
        "\n",
        "            } else {\n",
        "\n",
        "            await fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offset}&limit=${limit}&fields=${fields}`)\n",
        "    .then(res=> {return(res.json())})\n",
        "    .then(res=> {\n",
        "      console.log(res)\n",
        "      console.log(\"AQUIII\")\n",
        "      return(google.colab.kernel.invokeFunction('notebook.AddListItem', [res], {}))})\n",
        "    .catch(err=> { return (\n",
        "      google.colab.kernel.invokeFunction('notebook.error', [err], {})) })\n",
        "            }\n",
        "                  '''\n",
        "\n",
        "    asyncfun = \"async function asyncfun()\"\n",
        "\n",
        "    if args[0]:\n",
        "\n",
        "      main_app =  \"<script>\" + search_params.format(search=self.search, fields=self.fields, limit=self.limit, offset=self.offset) + asyncfun + \"{\" + engine + \"}\" + \"asyncfun()\" + \"</script>\"\n",
        "\n",
        "      display(IPython.display.HTML(main_app))\n",
        "      \n",
        "    else:\n",
        "      main_app = boxs + \"<script>\" + button + \"{\" + search_query + engine + \"}\" + \"</script>\"\n",
        "      \n",
        "      display(IPython.display.HTML(main_app))\n",
        "\n",
        "    \n",
        "\n",
        "  def _error(self,value):\n",
        "    try:\n",
        "      print(\"ERRO na API SemanticScholar:\\n\")\n",
        "      print(value)\n",
        "    except:\n",
        "      pass \n",
        "\n",
        "  def _searching(self):\n",
        "    with output.use_tags('some_outputs'):\n",
        "      print(\"\\n\\nPesquisando...\")\n",
        "      sys.stdout.flush();\n",
        "\n",
        "  def _merge_data(self, data):\n",
        "    output.clear(output_tags='some_outputs')\n",
        "    print(f\"Achou {data[0]['total']} papers.\\n\")\n",
        "    self.data_0 = data\n",
        "\n",
        "    self.data = pd.DataFrame(data[0]['data'])\n",
        "\n",
        "    try:\n",
        "      for x in data[1:len(data)]:\n",
        "        try:\n",
        "          self.merge(pd.DataFrame(x['data']))\n",
        "        except:\n",
        "          self._error(x)\n",
        "    except:\n",
        "      pass \n",
        "\n",
        "    print(f\"\\nApi devolveu >> {self.data.shape[0]} papers\\n\" )\n",
        "    print(self.data.head())\n",
        "\n",
        "\n",
        "  def merge(self, data):\n",
        "    self.data = pd.concat([self.data, data], ignore_index=True ) \n",
        "\n",
        "  def _add_list_item(self, value):\n",
        "    output.clear(output_tags='some_outputs')\n",
        "\n",
        "    print(f\"Achou {value['total']} papers.\\n\")\n",
        "\n",
        "    self.data = pd.DataFrame(value['data'])\n",
        "\n",
        "    print(f\"Api devolveu >> {self.data.shape[0]} papers\\n\" )\n",
        "    \n",
        "    print(self.data.head())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At8ZsoI1ZfbI"
      },
      "source": [
        "# Consumir a classe `Search()`\n",
        "\n",
        "A duas formas de pesquisar utilizando `Search()`:\n",
        "\n",
        "1. A primeira é utilizando parâmetros na propria classe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2dKxHXrkHvf"
      },
      "source": [
        "# \"Decision making\" AND \"optimization\" AND \"artificial intelligence\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCrYFOThaOR_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        },
        "outputId": "237cc7a0-1543-4122-b68d-8c5d176a7725"
      },
      "source": [
        "Resultados = Search(search = \"Machine Learning+Deep Learning\" , fields = \"title,abstract,citationCount,isOpenAccess,fieldsOfStudy\", limit = \"2000\", offset = \"0\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<script>\n",
              "            var search = \"Machine Learning\"\n",
              "            var fields = \"title,abstract,citationCount,isOpenAccess,fieldsOfStudy\"\n",
              "            var limit = parseInt(2000)\n",
              "            var offset = parseInt(0)\n",
              "                  async function asyncfun(){\n",
              "            google.colab.kernel.invokeFunction('notebook.searching', [], {});\n",
              "\n",
              "            if (limit >100) {\n",
              "              var number = limit\n",
              "              var data = \"\"\n",
              "              var promises = []\n",
              "              var offsetSearch = 0\n",
              "              var rest = 0\n",
              "\n",
              "              for (let index = 0; index < Math.floor(limit/100); index++) {\n",
              "                offsetSearch = 100*(index) + offset + 1*(index!==0)\n",
              "\n",
              "\n",
              "                promises.push(\n",
              "                  fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offsetSearch}&limit=100&fields=${fields}`)\n",
              "    .then(res=> {return(res.json())})\n",
              "    .then(res=> {return(res)})\n",
              "                )\n",
              "              }\n",
              "              \n",
              "              if (limit%100 !== 0) { \n",
              "                rest= limit%100\n",
              "                offsetSearch = offsetSearch+100\n",
              "                \n",
              "                console.log(rest)\n",
              "                console.log(offsetSearch)\n",
              "\n",
              "                promises.push(\n",
              "                fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offsetSearch}&limit=${rest}&fields=${fields}`)\n",
              "    .then(res=> {return(res.json())})\n",
              "    .then(res=> {return(res)})\n",
              "                )}\n",
              "\n",
              "              await Promise.all(promises).then(data=>{\n",
              "                google.colab.kernel.invokeFunction('notebook.mergeData', [data], {})\n",
              "              })\n",
              "              .catch(err=> { return (google.colab.kernel.invokeFunction('notebook.error', [err], {})) })\n",
              "\n",
              "            } else {\n",
              "\n",
              "            await fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offset}&limit=${limit}&fields=${fields}`)\n",
              "    .then(res=> {return(res.json())})\n",
              "    .then(res=> {\n",
              "      console.log(res)\n",
              "      console.log(\"AQUIII\")\n",
              "      return(google.colab.kernel.invokeFunction('notebook.AddListItem', [res], {}))})\n",
              "    .catch(err=> { return (\n",
              "      google.colab.kernel.invokeFunction('notebook.error', [err], {})) })\n",
              "            }\n",
              "                  }asyncfun()</script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Achou 5240039 papers.\n",
            "\n",
            "ERRO na API SemanticScholar:\n",
            "\n",
            "{'message': 'Internal Server Error'}\n",
            "ERRO na API SemanticScholar:\n",
            "\n",
            "{'message': 'Internal Server Error'}\n",
            "ERRO na API SemanticScholar:\n",
            "\n",
            "{'message': 'Internal Server Error'}\n",
            "ERRO na API SemanticScholar:\n",
            "\n",
            "{'message': 'Internal Server Error'}\n",
            "ERRO na API SemanticScholar:\n",
            "\n",
            "{'message': 'Internal Server Error'}\n",
            "ERRO na API SemanticScholar:\n",
            "\n",
            "{'message': 'Internal Server Error'}\n",
            "ERRO na API SemanticScholar:\n",
            "\n",
            "{'message': 'Internal Server Error'}\n",
            "ERRO na API SemanticScholar:\n",
            "\n",
            "{'message': 'Internal Server Error'}\n",
            "ERRO na API SemanticScholar:\n",
            "\n",
            "{'message': 'Internal Server Error'}\n",
            "\n",
            "Api devolveu >> 1009 papers\n",
            "\n",
            "                                    paperId  ...                    fieldsOfStudy\n",
            "0  46200b99c40e8586c8a0f588488ab6414119fb28  ...               [Computer Science]\n",
            "1  9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d  ...               [Computer Science]\n",
            "2  b42b1bfdc262bf99e9484e2e9df94df216b96374  ...               [Computer Science]\n",
            "3  25badc676197a70aaf9911865eb03469e402ba57  ...               [Computer Science]\n",
            "4  f9c602cc436a9ea2f9e7db48c77d924e09ce3c32  ...  [Computer Science, Mathematics]\n",
            "\n",
            "[5 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zotFl5-ficaO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "fa1bc71f-f243-4a1f-a967-172e638aa4df"
      },
      "source": [
        "# Os dados ficam na variável data, no qual é uma tabela do tipo pandas\n",
        "print(Resultados.data.columns)\n",
        "print(Resultados.data.sort_values(\"citationCount\", ascending = False ).head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paperId</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>citationCount</th>\n",
              "      <th>isOpenAccess</th>\n",
              "      <th>fieldsOfStudy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>754</th>\n",
              "      <td>34f25a8704614163c4095b3ee2fc969b60de4698</td>\n",
              "      <td>Dropout: a simple way to prevent neural networ...</td>\n",
              "      <td>Deep neural nets with a large number of parame...</td>\n",
              "      <td>24510</td>\n",
              "      <td>False</td>\n",
              "      <td>[Computer Science]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>a4cec122a08216fe8a3bc19b22e78fbaea096256</td>\n",
              "      <td>Deep Learning</td>\n",
              "      <td>Machine-learning technology powers many aspect...</td>\n",
              "      <td>19531</td>\n",
              "      <td>False</td>\n",
              "      <td>[Medicine, Computer Science]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>963</th>\n",
              "      <td>5d90f06bb70a0a3dced62413346235c02b1aa086</td>\n",
              "      <td>Learning Multiple Layers of Features from Tiny...</td>\n",
              "      <td>Groups at MIT and NYU have collected a dataset...</td>\n",
              "      <td>13232</td>\n",
              "      <td>False</td>\n",
              "      <td>[Computer Science]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>988</th>\n",
              "      <td>6bdb186ec4726e00a8051119636d4df3b94043b5</td>\n",
              "      <td>Caffe: Convolutional Architecture for Fast Fea...</td>\n",
              "      <td>Caffe provides multimedia scientists and pract...</td>\n",
              "      <td>13101</td>\n",
              "      <td>False</td>\n",
              "      <td>[Computer Science]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>46200b99c40e8586c8a0f588488ab6414119fb28</td>\n",
              "      <td>TensorFlow: A system for large-scale machine l...</td>\n",
              "      <td>TensorFlow is a machine learning system that o...</td>\n",
              "      <td>10598</td>\n",
              "      <td>False</td>\n",
              "      <td>[Computer Science]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      paperId  ...                 fieldsOfStudy\n",
              "754  34f25a8704614163c4095b3ee2fc969b60de4698  ...            [Computer Science]\n",
              "17   a4cec122a08216fe8a3bc19b22e78fbaea096256  ...  [Medicine, Computer Science]\n",
              "963  5d90f06bb70a0a3dced62413346235c02b1aa086  ...            [Computer Science]\n",
              "988  6bdb186ec4726e00a8051119636d4df3b94043b5  ...            [Computer Science]\n",
              "14   46200b99c40e8586c8a0f588488ab6414119fb28  ...            [Computer Science]\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwfe0ZCyZ8YD"
      },
      "source": [
        "2. A segunda é atravez da api de busca, searchBox, no qual é possivel colocar os campos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCZf9v2yzskK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "fba90541-642b-4c5d-ed98-323245132691"
      },
      "source": [
        "Resultados_2 = Search()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              " \n",
              "        <label for=\"query\">Buscar: </label>\n",
              "        <input type=\"text\" id=\"query\" value=\"Machine Learning+Deep Learning\" style=\"width: 400px;\"/>\n",
              "        <br/>\n",
              "        <br/>\n",
              "        \n",
              "        <label for=\"fields\">Fields: </label>\n",
              "        <input type=\"text\" id=\"fields\" value=\"title,abstract,isOpenAccess,fieldsOfStudy\" style=\"width: 400px;\"/>\n",
              "        <br/>\n",
              "        <br/>\n",
              " \n",
              "        <label for=\"limit\">Limite: </label>\n",
              "        <input type=\"text\" id=\"limit\" value=\"10\" style=\"width: 50px;\"/><br/>\n",
              "        <br/>\n",
              "\n",
              "        <label for=\"limit\">Offset: </label>\n",
              "        <input type=\"text\" id=\"offset\" value=\"0\" style=\"width: 50px;\"/><br/>\n",
              "        <br/>\n",
              "\n",
              "        <button id='button'>Pesquisar</button>\n",
              "        <br/>\n",
              "        <br/>\n",
              "           <script> document.querySelector('#button').onclick = async () => {\n",
              "            var search = document.getElementById(\"query\").value\n",
              "            var fields = document.getElementById(\"fields\").value\n",
              "            var limit = parseInt(document.getElementById(\"limit\").value)\n",
              "            var offset = parseInt(document.getElementById(\"offset\").value)\n",
              "                  \n",
              "            google.colab.kernel.invokeFunction('notebook.searching', [], {});\n",
              "\n",
              "            if (limit >100) {\n",
              "              var number = limit\n",
              "              var data = \"\"\n",
              "              var promises = []\n",
              "              var offsetSearch = 0\n",
              "              var rest = 0\n",
              "\n",
              "              for (let index = 0; index < Math.floor(limit/100); index++) {\n",
              "                offsetSearch = 100*(index) + offset + 1*(index!==0)\n",
              "\n",
              "\n",
              "                promises.push(\n",
              "                  fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offsetSearch}&limit=100&fields=${fields}`)\n",
              "    .then(res=> {return(res.json())})\n",
              "    .then(res=> {return(res)})\n",
              "                )\n",
              "              }\n",
              "              \n",
              "              if (limit%100 !== 0) { \n",
              "                rest= limit%100\n",
              "                offsetSearch = offsetSearch+100\n",
              "                \n",
              "                console.log(rest)\n",
              "                console.log(offsetSearch)\n",
              "\n",
              "                promises.push(\n",
              "                fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offsetSearch}&limit=${rest}&fields=${fields}`)\n",
              "    .then(res=> {return(res.json())})\n",
              "    .then(res=> {return(res)})\n",
              "                )}\n",
              "\n",
              "              await Promise.all(promises).then(data=>{\n",
              "                google.colab.kernel.invokeFunction('notebook.mergeData', [data], {})\n",
              "              })\n",
              "              .catch(err=> { return (google.colab.kernel.invokeFunction('notebook.error', [err], {})) })\n",
              "\n",
              "            } else {\n",
              "\n",
              "            await fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offset}&limit=${limit}&fields=${fields}`)\n",
              "    .then(res=> {return(res.json())})\n",
              "    .then(res=> {\n",
              "      console.log(res)\n",
              "      console.log(\"AQUIII\")\n",
              "      return(google.colab.kernel.invokeFunction('notebook.AddListItem', [res], {}))})\n",
              "    .catch(err=> { return (\n",
              "      google.colab.kernel.invokeFunction('notebook.error', [err], {})) })\n",
              "            }\n",
              "                  }</script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Achou 652200 papers.\n",
            "\n",
            "Api devolveu >> 10 papers\n",
            "\n",
            "                                    paperId  ...       fieldsOfStudy\n",
            "0  846ff7afb7670d62f88b4a8cc99d306ffb81b075  ...          [Medicine]\n",
            "1  5dc53e50148b01fe8b9536eb79fa6b1dce924174  ...          [Medicine]\n",
            "2  7cc2e148d27a7508dd23c4e35eb63cc9b3e6a58f  ...  [Computer Science]\n",
            "3  59444b096f7c8a561d540102e8b5bfb189edabc6  ...                None\n",
            "4  eee313380ccb45807ea0afa3c1df86f6b48b8867  ...  [Computer Science]\n",
            "\n",
            "[5 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8G0F7fEFz9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50abc23f-9658-4a53-ebbc-4f9c8065a050"
      },
      "source": [
        "print(Resultados_2.data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                    paperId  ...       fieldsOfStudy\n",
            "0  846ff7afb7670d62f88b4a8cc99d306ffb81b075  ...          [Medicine]\n",
            "1  5dc53e50148b01fe8b9536eb79fa6b1dce924174  ...          [Medicine]\n",
            "2  7cc2e148d27a7508dd23c4e35eb63cc9b3e6a58f  ...  [Computer Science]\n",
            "3  59444b096f7c8a561d540102e8b5bfb189edabc6  ...                None\n",
            "4  eee313380ccb45807ea0afa3c1df86f6b48b8867  ...  [Computer Science]\n",
            "5  46479bbea7749cb2db35b139206039531327053c  ...  [Computer Science]\n",
            "6  b69fe5a837277ddbea5215d6bacd3a902e9d11ce  ...          [Medicine]\n",
            "7  b0bf64ccbd651e8c7bc141d8aabaecff562e93a1  ...  [Computer Science]\n",
            "8  042ab08ec6782cf217f13175162bfd48f7350114  ...  [Computer Science]\n",
            "9  03e7832982986159400a8eeab148487ffcfabe56  ...  [Computer Science]\n",
            "\n",
            "[10 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNrq2PcNtb-f"
      },
      "source": [
        "# **ON BUILD...**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ppj9pGetiEH"
      },
      "source": [
        "#### NOT RUN\n",
        "\n",
        "## Para fazer a requisição de dados, é utilizado o metodo POST para HTTP\n",
        "import requests\n",
        "import json\n",
        "\n",
        "url = \"https://www.semanticscholar.org/api/1/search\"\n",
        "\n",
        "PARAMS_FOR_POST = '''{\n",
        "    \"queryString\": \"Machine Learning+Deep Learning\",\n",
        "    \"page\": 1,\n",
        "    \"pageSize\": 10,\n",
        "    \"sort\": \"total-citations\", #influence #\"pub-date\" #relevance\n",
        "    \"authors\": [],\n",
        "    \"coAuthors\": [],\n",
        "    \"venues\": [\n",
        "        \"PloS one\",\n",
        "        \"AAAI\",\n",
        "        \"Scientific reports\",\n",
        "        \"IEEE Access\",\n",
        "        \"ArXiv\",\n",
        "        \"Expert Syst. Appl.\",\n",
        "        \"ICML\",\n",
        "        \"Neurocomputing\",\n",
        "        \"Sensors\",\n",
        "        \"Remote. Sens.\"\n",
        "    ],\n",
        "    \"yearFilter\": {\n",
        "        \"min\": 2008,\n",
        "        \"max\": 2021\n",
        "    },\n",
        "    \"requireViewablePdf\": true,\n",
        "    \"publicationTypes\": [\n",
        "        \"ClinicalTrial\",\n",
        "        \"CaseReport\",\n",
        "        \"Editorial\",\n",
        "        \"Study\",\n",
        "        \"Book\",\n",
        "        \"News\",\n",
        "        \"Review\",\n",
        "        \"Conference\",\n",
        "        \"LettersAndComments\",\n",
        "        \"JournalArticle\"\n",
        "    ],\n",
        "    \"externalContentTypes\": [],\n",
        "    \"fieldsOfStudy\": [\n",
        "        \"biology\",\n",
        "        \"art\",\n",
        "        \"business\",\n",
        "        \"computer-science\",\n",
        "        \"chemistry\",\n",
        "        \"economics\",\n",
        "        \"engineering\",\n",
        "        \"environmental-science\",\n",
        "        \"geography\",\n",
        "        \"geology\",\n",
        "        \"history\",\n",
        "        \"materials-science\",\n",
        "        \"mathematics\",\n",
        "        \"medicine\",\n",
        "        \"philosophy\",\n",
        "        \"physics\",\n",
        "        \"political-science\",\n",
        "        \"psychology\",\n",
        "        \"sociology\"\n",
        "    ],\n",
        "    \"useFallbackRankerService\": false,\n",
        "    \"useFallbackSearchCluster\": false,\n",
        "    \"hydrateWithDdb\": true,\n",
        "    \"includeTldrs\": true,\n",
        "    \"performTitleMatch\": true,\n",
        "    \"includeBadges\": true,\n",
        "    \"tldrModelVersion\": \"v2.0.0\",\n",
        "    \"getQuerySuggestions\": false\n",
        "}\n",
        "'''\n",
        "# post = json.loads(data)\n",
        "# res = requests.post(url, json=post)\n",
        "# res.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_OkAhJHt3tS"
      },
      "source": [
        "import requests\n",
        "import json\n",
        "import multiprocessing as mp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from time import sleep, time\n",
        "\n",
        "\n",
        "def timer(fun):\n",
        "  def warper(*args,**kwargs):\n",
        "    start = time()\n",
        "    d = fun(*args,**kwargs)\n",
        "    end = time()\n",
        "    print(f\"[{fun.__name__}]>> Demorou {round(end-start,4)}s\")\n",
        "    return d\n",
        "  return warper\n",
        "\n",
        "\n",
        "\n",
        "class SearchWeb():\n",
        "  def __init__(self, search=\"Machine Learning+Deep Learning\", poolCPU = 4, sleeptry=5, save = False, **kwargs):\n",
        "     \n",
        "    self.sleeptry = sleeptry\n",
        "    self.poolCPU = poolCPU\n",
        "    self.badcall = []\n",
        "    self._start = True\n",
        "\n",
        "    self.saveFile = save\n",
        "    self._search = search\n",
        "    self._page = kwargs.get('page', 1)\n",
        "    self._sort = kwargs.get('sort', \"relevance\")\n",
        "    self._authors = kwargs.get('authors', [])\n",
        "    self._coAuthors = kwargs.get('coAuthors', [])\n",
        "    self._venues = kwargs.get('venues', ['PloS one', 'AAAI', 'Scientific reports', 'IEEE Access', 'ArXiv', 'Expert Syst. Appl.', 'ICML', 'Neurocomputing', 'Sensors', 'Remote. Sens.'])\n",
        "    self._yearFilter = kwargs.get('yearFilter', None) # {\"min\": 2008,\"max\": 2021}\n",
        "    self._requireViewablePdf = kwargs.get('requireViewablePdf', False)\n",
        "    self._publicationTypes = kwargs.get('publicationTypes', [\"ClinicalTrial\", \"CaseReport\", \"Editorial\",\"Study\",\"Book\",\"News\",\"Review\",\"Conference\",\"LettersAndComments\",\"JournalArticle\"])\n",
        "    self._fieldsOfStudy = kwargs.get('fieldsOfStudy', [\"biology\",\"art\",\"business\",\"computer-science\",\"chemistry\",\"economics\",\"engineering\",\"environmental-science\",\"geography\",\"geology\",\"history\",\"materials-science\",\"mathematics\",\"medicine\",\"philosophy\",\"physics\",\"political-science\",\"psychology\",\"sociology\"])\n",
        "    self._useFallbackRankerService = kwargs.get('useFallbackRankerService', False)\n",
        "    self._useFallbackSearchCluster = kwargs.get('useFallbackSearchCluster', False)\n",
        "    self._hydrateWithDdb = kwargs.get('hydrateWithDdb', True)\n",
        "    self._includeTldrs = kwargs.get('includeTldrs', True)\n",
        "    self._performTitleMatch = kwargs.get('performTitleMatch', True)\n",
        "    self._includeBadges = kwargs.get('includeBadges', True)\n",
        "    self._tldrModelVersion = kwargs.get('tldrModelVersion', 'v2.0.0')\n",
        "    self._getQuerySuggestions = kwargs.get('getQuerySuggestions', False)\n",
        "\n",
        "\n",
        "    self.post = {\n",
        "    \"page\": self._page, \n",
        "    \"pageSize\": 10,\n",
        "    \"queryString\": self._search,\n",
        "    \"sort\": self._sort,\n",
        "    \"authors\": self._authors,\n",
        "    \"coAuthors\": self._coAuthors,\n",
        "    \"venues\": self._venues,\n",
        "    \"yearFilter\": self._yearFilter,\n",
        "    \"requireViewablePdf\": self._requireViewablePdf,\n",
        "    \"publicationTypes\": self._publicationTypes,\n",
        "    \"externalContentTypes\": [],\n",
        "    \"fieldsOfStudy\": self._fieldsOfStudy,\n",
        "    \"useFallbackRankerService\": self._useFallbackRankerService,\n",
        "    \"useFallbackSearchCluster\": self._useFallbackSearchCluster,\n",
        "    \"hydrateWithDdb\": self._hydrateWithDdb,\n",
        "    \"includeTldrs\": self._includeTldrs,\n",
        "    \"performTitleMatch\": self._performTitleMatch,\n",
        "    \"includeBadges\": self._includeBadges,\n",
        "    \"tldrModelVersion\": \"v2.0.0\",\n",
        "    \"getQuerySuggestions\": self._getQuerySuggestions,\n",
        "    }\n",
        "\n",
        "    print('.post >>')\n",
        "    print(self.post)\n",
        "\n",
        "  def _query(self, page):\n",
        "    url = \"https://www.semanticscholar.org/api/1/search\"\n",
        "    post = self.post.copy()\n",
        "    post[\"page\"] = page\n",
        "    return [requests.post(url, json=post), page ]\n",
        "\n",
        "  def _json(self, res):\n",
        "    return json.loads(res.text).copy()\n",
        "  \n",
        "  def _paperExtract(self, data):\n",
        "    p = {\n",
        "        \"authors\": [author[0]['name'] for author in data.get('authors',[{'name':None},None])],\n",
        "        \"id\": data.get('id',None),\n",
        "        \"socialLinks\": data.get('socialLinks',None),\n",
        "        \"title\": data.get('title',{'text':None})['text'],\n",
        "        \"paperAbstract\": data.get('paperAbstract',{'text':None})['text'],\n",
        "        \"year\": data.get('year',{'text':None})['text'],\n",
        "        \"venue\": data.get('venue',{'text':None})['text'],\n",
        "        \"citationContexts\":data.get('citationContexts',None),\n",
        "        \"citationStats\": data.get('citationStats',None),\n",
        "        \"sources\":data.get('sources',None),\n",
        "        \"externalContentStats\":data.get('externalContentStats',None),\n",
        "        \"journal\":data.get('journal',None),\n",
        "        \"presentationUrls\":data.get('presentationUrls',None),\n",
        "        \"links\": data.get('links',None),\n",
        "        \"primaryPaperLink\": data.get('primaryPaperLink',None),\n",
        "        \"alternatePaperLinks\": data.get('alternatePaperLinks',None),\n",
        "        \"entities\": [author['name'] for author in data.get('entities',[{'name':None}])],\n",
        "        \"entityRelations\": data.get('entityRelations',None),\n",
        "        \"blogs\":data.get('blogs',None),\n",
        "        \"videos\":data.get('videos',None),\n",
        "        \"githubReferences\": data.get('githubReferences',None),\n",
        "        \"scorecardStats\": data.get('scorecardStats',None),\n",
        "        \"fieldsOfStudy\":data.get('fieldsOfStudy',None),\n",
        "        \"pubDate\":data.get('pubDate',None),\n",
        "        \"pubUpdateDate\":data.get('pubUpdateDate',None),\n",
        "        \"badges\":data.get('badges',None),\n",
        "        \"tldr\":data.get('tldr',None)\n",
        "        }\n",
        "    return p\n",
        "\n",
        "    \n",
        "\n",
        "    # c['querySuggestions']\n",
        "    # c['totalPages']\n",
        "    # c['totalResults']\n",
        "    \n",
        "  def _save(self):\n",
        "    with open('./DATA.json', 'w') as fp:\n",
        "      json.dump(self.all, fp)\n",
        "    print(\"[Save] >> Save ate current directory, ./DATA.json\")\n",
        "\n",
        "  @timer\n",
        "  def _extract(self, pool, data):\n",
        "    try:\n",
        "      self.papers_text = pool.map(self._json, data['Response'])\n",
        "      self.all[\"Results\"].extend([{\"N_Papers\":len(page['results']),\n",
        "                                   \"Page\": page['query']['page'],\n",
        "                                   \"Papers\": pool.map(self._paperExtract,\n",
        "                                                      page['results'])} for page in self.papers_text])\n",
        "      if self.saveFile:\n",
        "        try:\n",
        "          self._save()\n",
        "        except:\n",
        "          print(\"_save >> [Fail] to save.\")\n",
        "\n",
        "    except:\n",
        "      print(\"_extract>> [Fail], see .papers_text to reextract content.\")\n",
        "      self.badcall.append(self.papers_text)\n",
        "    \n",
        "    if self._start:\n",
        "      print(f\"Total Results: {self.papers_text[0]['totalPages']}\")\n",
        "      print(f\"Total Pages: {self.papers_text[0]['totalResults']}\")\n",
        "      print(f\"Query Suggestions: {self.papers_text[0]['querySuggestions']}\")\n",
        "      print('\\n')\n",
        "      self._start = False\n",
        "\n",
        "  def _data(self, data):\n",
        "    if type(self.datasource) == str:\n",
        "      self.datasource = data\n",
        "    else:\n",
        "      self.datasource = pd.concat([self.datasource, data])\n",
        "\n",
        "  @timer\n",
        "  def _runtime(self, pool, pages):\n",
        "    while True:\n",
        "      res = pool.map(self._query, pages)\n",
        "      self.codes = [[x[0], x[1],x[0].status_code] for x in res]\n",
        "      resultData = pd.DataFrame(self.codes, columns=[\"Response\", \"Page\", \"Code\"])\n",
        "      resultData.set_index(\"Page\")\n",
        "      \n",
        "      if resultData.query(\"Code !=200\").size == 0:\n",
        "        self._data(resultData)\n",
        "        self._extract(pool, resultData.query(\"Code ==200\"))\n",
        "        break\n",
        "      else:\n",
        "        print(\"Bad call of pages:\")\n",
        "        self._data(resultData.query(\"Code == 200\"))\n",
        "              # self.datasource.append(resultData.query(\"Code ==200\"))\n",
        "        pages = resultData.query(\"Code !=200\").index.values.tolist()\n",
        "        print(pages)\n",
        "        print(f\"Tentando de novo daqui a {self.sleeptry/60} min...\")\n",
        "        self._extract(pool, resultData.query(\"Code ==200\"))\n",
        "        sleep(self.sleeptry)\n",
        "\n",
        "    # self._extract(pool, self.datasource)\n",
        "\n",
        "\n",
        "  @timer\n",
        "  def get(self, n = 10):\n",
        "    self.post[\"pageSize\"] = 10\n",
        "    self.all = {\"Results\": []}\n",
        "    self.mult = ''\n",
        "    self.datasource = ''\n",
        "    print(\"Searching...\")\n",
        "    print(self.all)\n",
        "\n",
        "    with mp.Pool(self.poolCPU) as pool:\n",
        "      if n > 10:\n",
        "        pages = list(range(self._page, (n//10)+self._page))\n",
        "      # for page in range(n//10):\n",
        "        self._runtime(pool,pages)\n",
        "          \n",
        "        if n%10>0:\n",
        "          pages = [n//10+self._page]\n",
        "          self.post[\"pageSize\"] = n%10\n",
        "\n",
        "          self._runtime(pool, pages)\n",
        "          \n",
        "      else:\n",
        "        # pass\n",
        "        pages = [self._page]\n",
        "        self.post[\"page\"] = self._page\n",
        "        self.post[\"pageSize\"] = n\n",
        "\n",
        "        self._runtime(pool, pages)\n",
        "\n",
        "      # self._extract(pool, self.datasource)\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#   SearchWeb().get()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_j2uslvt8A2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c440787-5ddc-4841-abfc-d013a731f74b"
      },
      "source": [
        "from_Webpage = SearchWeb(search= \"Machine Learning+Deep Learning\", sort= \"total-citations\", page=1, save=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".post >>\n",
            "{'page': 1, 'pageSize': 10, 'queryString': 'Machine Learning+Deep Learning', 'sort': 'total-citations', 'authors': [], 'coAuthors': [], 'venues': ['PloS one', 'AAAI', 'Scientific reports', 'IEEE Access', 'ArXiv', 'Expert Syst. Appl.', 'ICML', 'Neurocomputing', 'Sensors', 'Remote. Sens.'], 'yearFilter': None, 'requireViewablePdf': False, 'publicationTypes': ['ClinicalTrial', 'CaseReport', 'Editorial', 'Study', 'Book', 'News', 'Review', 'Conference', 'LettersAndComments', 'JournalArticle'], 'externalContentTypes': [], 'fieldsOfStudy': ['biology', 'art', 'business', 'computer-science', 'chemistry', 'economics', 'engineering', 'environmental-science', 'geography', 'geology', 'history', 'materials-science', 'mathematics', 'medicine', 'philosophy', 'physics', 'political-science', 'psychology', 'sociology'], 'useFallbackRankerService': False, 'useFallbackSearchCluster': False, 'hydrateWithDdb': True, 'includeTldrs': True, 'performTitleMatch': True, 'includeBadges': True, 'tldrModelVersion': 'v2.0.0', 'getQuerySuggestions': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzUBus_Vt9-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57c2adee-b781-45f2-bb13-676c980a7356"
      },
      "source": [
        "# Retorna 2 papers com base nos parametros passados em SearchWeb() que constitui ().post\n",
        "from_Webpage.get(n = 12)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching...\n",
            "{'Results': []}\n",
            "[Save] >> Save ate current directory, ./DATA.json\n",
            "Total Results: 5102\n",
            "Total Pages: 51022\n",
            "Query Suggestions: []\n",
            "\n",
            "\n",
            "[_extract]>> Demorou 0.0682s\n",
            "[_runtime]>> Demorou 2.5923s\n",
            "[Save] >> Save ate current directory, ./DATA.json\n",
            "[_extract]>> Demorou 0.0175s\n",
            "[_runtime]>> Demorou 3.7995s\n",
            "[get]>> Demorou 6.4582s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_AxXsbJuARM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ab99d3f-86a5-46ac-ca39-846f371d4f05"
      },
      "source": [
        "# Tudo que vem com base em 1 paper (dict.)\n",
        "from_Webpage.all[\"Results\"][0][\"Papers\"][0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'alternatePaperLinks': [],\n",
              " 'authors': ['Martín Abadi',\n",
              "  'Ashish Agarwal',\n",
              "  'P. Barham',\n",
              "  'E. Brevdo',\n",
              "  'Z. Chen',\n",
              "  'C. Citro',\n",
              "  'G. Corrado',\n",
              "  'Andy Davis',\n",
              "  'J. Dean',\n",
              "  'M. Devin',\n",
              "  'S. Ghemawat',\n",
              "  'I. Goodfellow',\n",
              "  'Andrew Harp',\n",
              "  'Geoffrey Irving',\n",
              "  'M. Isard',\n",
              "  'Y. Jia',\n",
              "  'R. Józefowicz',\n",
              "  'Lukasz Kaiser',\n",
              "  'M. Kudlur',\n",
              "  'Josh Levenberg',\n",
              "  'Dandelion Mané',\n",
              "  'Rajat Monga',\n",
              "  'Sherry Moore',\n",
              "  'D. Murray',\n",
              "  'Christopher Olah',\n",
              "  'M. Schuster',\n",
              "  'Jonathon Shlens',\n",
              "  'Benoit Steiner',\n",
              "  'Ilya Sutskever',\n",
              "  'Kunal Talwar',\n",
              "  'P. Tucker',\n",
              "  'V. Vanhoucke',\n",
              "  'Vijay Vasudevan',\n",
              "  'F. Viégas',\n",
              "  'Oriol Vinyals',\n",
              "  'Pete Warden',\n",
              "  'M. Wattenberg',\n",
              "  'Martin Wicke',\n",
              "  'Yuan Yu',\n",
              "  'X. Zheng'],\n",
              " 'badges': [{'id': 'OPEN_ACCESS'}],\n",
              " 'blogs': [],\n",
              " 'citationContexts': [],\n",
              " 'citationStats': {'citationAcceleration': -0.5003223726627982,\n",
              "  'citationVelocity': 1463.3333333333333,\n",
              "  'citedByBuckets': [{'count': 2, 'endKey': 2014, 'startKey': 2014},\n",
              "   {'count': 9, 'endKey': 2015, 'startKey': 2015},\n",
              "   {'count': 311, 'endKey': 2016, 'startKey': 2016},\n",
              "   {'count': 1535, 'endKey': 2017, 'startKey': 2017},\n",
              "   {'count': 2451, 'endKey': 2018, 'startKey': 2018},\n",
              "   {'count': 2064, 'endKey': 2019, 'startKey': 2019},\n",
              "   {'count': 1551, 'endKey': 2020, 'startKey': 2020},\n",
              "   {'count': 775, 'endKey': 2021, 'startKey': 2021}],\n",
              "  'estNumCitations': 8547.237174633896,\n",
              "  'firstCitationVelocityYear': 2019,\n",
              "  'keyCitationRate': 0.10810190498049116,\n",
              "  'keyCitedByBuckets': [],\n",
              "  'lastCitationVelocityYear': 2021,\n",
              "  'numCitations': 8714,\n",
              "  'numKeyCitations': 942,\n",
              "  'numKeyReferences': 5,\n",
              "  'numReferences': 66,\n",
              "  'numViewableReferences': 66},\n",
              " 'entities': ['TensorFlow',\n",
              "  'Machine learning',\n",
              "  'Distributed computing',\n",
              "  'Natural language processing',\n",
              "  'Computer vision',\n",
              "  'Speech recognition',\n",
              "  'Information retrieval',\n",
              "  'Robotics',\n",
              "  'Reference implementation',\n",
              "  'Algorithm',\n",
              "  'Information extraction',\n",
              "  'Deep learning',\n",
              "  'Mobile device',\n",
              "  'Open-source software',\n",
              "  'Computation',\n",
              "  'Computer science',\n",
              "  'Application programming interface',\n",
              "  'Graphics processing unit'],\n",
              " 'entityRelations': [],\n",
              " 'externalContentStats': [{'contentType': {'id': 'GITHUB_REPO'}, 'count': 4}],\n",
              " 'fieldsOfStudy': ['Computer Science'],\n",
              " 'githubReferences': [],\n",
              " 'id': '9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d',\n",
              " 'journal': {'name': 'ArXiv', 'volume': 'abs/1603.04467'},\n",
              " 'links': [{'linkType': 'arxiv',\n",
              "   'url': 'https://arxiv.org/pdf/1603.04467.pdf'}],\n",
              " 'paperAbstract': 'TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.',\n",
              " 'presentationUrls': ['https://pdfs.semanticscholar.org/808d/897108fe136bebab51cb4f05f94bc7c1df43.pdf'],\n",
              " 'primaryPaperLink': {'linkType': 'arxiv',\n",
              "  'url': 'https://arxiv.org/pdf/1603.04467.pdf'},\n",
              " 'pubDate': '2016-03-14',\n",
              " 'pubUpdateDate': '2016-03-16',\n",
              " 'scorecardStats': [{'citationCount': 8714,\n",
              "   'keyCitationCount': 942,\n",
              "   'score': 10.0,\n",
              "   'typeKey': 'cited_by'}],\n",
              " 'socialLinks': [],\n",
              " 'sources': ['Anansi',\n",
              "  'Grobid',\n",
              "  'ScienceParseMerged',\n",
              "  'MergedPDFExtraction',\n",
              "  'Anansi',\n",
              "  'ScienceParseMerged',\n",
              "  'Grobid',\n",
              "  'Crawler',\n",
              "  'Crawler',\n",
              "  'ScienceParseMerged',\n",
              "  'Crawler',\n",
              "  'ArXiv',\n",
              "  'Anansi',\n",
              "  'DBLP',\n",
              "  'ScienceParseMerged',\n",
              "  'Anansi',\n",
              "  'Grobid',\n",
              "  'ScienceParseMerged',\n",
              "  'MAG'],\n",
              " 'title': 'TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems',\n",
              " 'tldr': {'abstractSimilarityScore': 46,\n",
              "  'text': 'The TensorFlow interface and an implementation of that interface that is built at Google are described, which has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields.'},\n",
              " 'venue': 'ArXiv',\n",
              " 'videos': [],\n",
              " 'year': '2016'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jHvxAB2yAlo"
      },
      "source": [
        "\n",
        "#     self.saveFile = save\n",
        "#     self._publicationTypes = kwargs.get('publicationTypes', [\"ClinicalTrial\", \"CaseReport\", \"Editorial\",\"Study\",\"Book\",\"News\",\"Review\",\"Conference\",\"LettersAndComments\",\"JournalArticle\"])\n",
        "#     self._fieldsOfStudy = kwargs.get('fieldsOfStudy', [\"biology\",\"art\",\"business\",\"computer-science\",\"chemistry\",\"economics\",\"engineering\",\"environmental-science\",\"geography\",\"geology\",\"history\",\"materials-science\",\"mathematics\",\"medicine\",\"philosophy\",\"physics\",\"political-science\",\"psychology\",\"sociology\"])\n",
        "#     self._useFallbackRankerService = kwargs.get('useFallbackRankerService', False)\n",
        "#     self._useFallbackSearchCluster = kwargs.get('useFallbackSearchCluster', False)\n",
        "#     self._hydrateWithDdb = kwargs.get('hydrateWithDdb', True)\n",
        "#     self._includeTldrs = kwargs.get('includeTldrs', True)\n",
        "#     self._performTitleMatch = kwargs.get('performTitleMatch', True)\n",
        "#     self._includeBadges = kwargs.get('includeBadges', True)\n",
        "#     self._tldrModelVersion = kwargs.get('tldrModelVersion', 'v2.0.0')\n",
        "#     self._getQuerySuggestions = kwargs.get('getQuerySuggestions', False)\n",
        "\n",
        "# search = str(input('Pesquisar(ex. \"Machine+Medicine\"): '))\n",
        "# page = int(input('Pagina de inicio a busca: '))\n",
        "# sleeptry = int(input('Tempo de recall: '))\n",
        "# print('Opções de filtro [Sort](ex. total-citations): \"total-citations\", \"influence\", \"pub-date\", \"relevance\"')\n",
        "# sort = str(input('Sort: '))\n",
        "# print(\"Opções de filtro [Venues](ex.[] ou ['.', '.']): ['PloS one', 'AAAI', 'Scientific reports', 'IEEE Access', 'ArXiv', 'Expert Syst. Appl.', 'ICML', 'Neurocomputing', 'Sensors', 'Remote. Sens.']\")\n",
        "# poolCPU = int(input('Quantos cores (CPU): '))\n",
        "# years = input('Periodo de tempo (ex.{\"min\": 1999, \"max\": 2000}) : ')\n",
        "# requireViewablePdf = input('Ter Pdf free (True or False):')\n",
        "# requireViewablePdf = input('Ter Pdf free (True or False): [\"ClinicalTrial\", \"CaseReport\", \"Editorial\",\"Study\",\"Book\",\"News\",\"Review\",\"Conference\",\"LettersAndComments\",\"JournalArticle\"]:')\n",
        "\n",
        "# \"False\" == False\n",
        "\n",
        "# # from_Webpage.papers_text[0][\"results\"][0]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}