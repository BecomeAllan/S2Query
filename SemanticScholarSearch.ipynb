{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SemanticScholarSearch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BecomeAllan/S2Search/blob/main/SemanticScholarSearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S5xfBIUWDWZ"
      },
      "source": [
        "# Consumindo a API do SemanticScholar\n",
        "\n",
        "A seguir, tem uma classe chamada `Search()`, que ao instanciar-la em uma variável é possível fazer pesquisas sobre papers utilizando a api do SemanticScholar, dentre os parâmetros temos:\n",
        "\n",
        "- Buscar: Pesquisas sobre tópicos onde adicionar tópicos utiliza-se + (mais) e remover tópicos usamos - (menos)\n",
        "\n",
        "  ex. \"Machine+Medicine\"\n",
        "\n",
        "- Fields: O que será retornado como dados. Para utilizar, escolha dentre as opções sem utilizar espaço e separadas de virgulas:\n",
        "  - (str): externalIds\n",
        "  - (str): url\n",
        "  - (str): title\n",
        "  - (str): abstract\n",
        "  - (str): venue \n",
        "  - (str): year \n",
        "  - (str): referenceCount\n",
        "  - (str): citationCount\n",
        "  - (str): influentialCitationCount\n",
        "  - (str): isOpenAccess\n",
        "  - list (str): fieldsOfStudy\n",
        "  - list (str): authors \n",
        "\n",
        "  ex. \"title,abstract,isOpenAccess,fieldsOfStudy\"\n",
        "\n",
        "- Offset: Número que começa a puxar a partir da ordem dele a lista de papers. (0 seria o primeiro)\n",
        "\n",
        "- Limite: Número de papers a ser retornados (Máx. 10.000)\n",
        "\n",
        "**Obs:** A api do SemanticScholar disponibiliza 100 query's a cada 5 min, no qual apenas retorna no máx. 100 resutados (limite). Assim a cada 5 min, é possível puxar 10.000 papers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNbvhyFvKOMZ"
      },
      "source": [
        "import requests\n",
        "import json\n",
        "import multiprocessing as mp\n",
        "import os\n",
        "# import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import ast\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "from time import sleep, time\n",
        "\n",
        "\n",
        "def timer(fun):\n",
        "  def warper(*args,**kwargs):\n",
        "    start = time()\n",
        "    d = fun(*args,**kwargs)\n",
        "    end = time()\n",
        "    print(f\"[{fun.__name__}]>> Demorou {round(end-start,2)}s\")\n",
        "    return d\n",
        "  return warper\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class SearchAPI():\n",
        "  def __init__(self, search=\"decision making+optimization+artificial intelligence\", poolCPU = 4, sleeptry=5, save = False, **kwargs):\n",
        "     \n",
        "    self.total = 0\n",
        "    self.sleeptry = sleeptry\n",
        "    self.poolCPU = poolCPU\n",
        "    self.saveName = kwargs.get('Savename', \"Data\")\n",
        "\n",
        "    self.badcall = []\n",
        "\n",
        "    self.saveFile = save\n",
        "\n",
        "    self._api = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "\n",
        "    self.params = {\n",
        "    \"query\": search, \n",
        "    \"limit\": 100,\n",
        "    \"fields\": kwargs.get('fields', \"title,abstract,isOpenAccess,fieldsOfStudy\"),\n",
        "    \"offset\": kwargs.get('offset', 0),\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "  def _query(self, offset):\n",
        "    # print(\"_query\")\n",
        "    params = self.params.copy()\n",
        "    if offset + params['limit'] > 10000:\n",
        "      params['limit'] = str(10000 - offset) \n",
        "    else:\n",
        "      params['offset'] = str(offset)\n",
        "    # print(params)\n",
        "    # post[\"page\"] = page\n",
        "    try:\n",
        "      # &\n",
        "      url = self._api +\"?query=\" + str(params['query']) + \"&limit=\" + str(params['limit']) + \"&fields=\" + str(params['fields']) + \"&offset=\" +  str(params['offset'])\n",
        "      # print(url)\n",
        "      res = requests.get(url, timeout=15)\n",
        "      # sleep(self.sleeptry)\n",
        "      print(res)\n",
        "      res.encoding = 'utf-8'\n",
        "      return [res, offset, res.status_code]\n",
        "    except:\n",
        "      return [None, offset, 400 ]\n",
        "\n",
        "  def _pandas(self, res):\n",
        "    dict_data = json.loads(res.text)\n",
        "    # print(len(dict_data['data']))\n",
        "\n",
        "    self.total= self.total+ len(dict_data['data'])\n",
        "    return pd.DataFrame(dict_data['data'])\n",
        "\n",
        "  def save(self, name, data):\n",
        "    # csvFile = Path(f\"./{name}.csv\")\n",
        "    \n",
        "    # if csvFile.is_file():\n",
        "    #   x = pd.read_csv(f'{name}.csv')\n",
        "    #   data = pd.concat([data,x])\n",
        "\n",
        "    try:\n",
        "      data.to_csv(f'{name}.csv')\n",
        "    except:\n",
        "      print(\"[Save]>> Error to save the data.\")\n",
        "\n",
        "\n",
        "\n",
        "  @timer\n",
        "  def _extract(self, pool, data):\n",
        "    try:\n",
        "      papers_list = pool.map(self._pandas, data['Response'].tolist())\n",
        "\n",
        "      self.all.extend(papers_list)\n",
        "\n",
        "      if self.saveFile:\n",
        "        self.save(self.saveName ,pd.concat(self.all).reset_index())\n",
        "\n",
        "    except:\n",
        "      print(\"_extract>> [Fail], see .badcall to reextract content.\")\n",
        "      # self.badcall.append(self.papers_text)\n",
        "      # print(self.badcall)\n",
        "   \n",
        "      \n",
        "\n",
        "\n",
        "  @timer\n",
        "  def _runtime(self, pool, offsets):\n",
        "    # self.totalPages = 0\n",
        "\n",
        "    # find = False\n",
        "    \n",
        "    while True:\n",
        "      # if self.saveFile:\n",
        "      #   close = self._startFile(find)\n",
        "      \n",
        "      print('\\n')\n",
        "      print('[_runtime]>> Start searching...')\n",
        "\n",
        "      try:\n",
        "        res = pool.map(self._query, offsets)\n",
        "        resultData = pd.DataFrame(res, columns=[\"Response\", \"Page\", \"Code\"])\n",
        "        # print(resultData[\"Code\"])\n",
        "\n",
        "        resultData.set_index(\"Page\")\n",
        "        \n",
        "        if resultData.query(\"Code !=200\").size == 0:\n",
        "          self._extract(pool, resultData.query(\"Code ==200\"))\n",
        "\n",
        "          break\n",
        "        else:\n",
        "\n",
        "          if resultData.query(\"Code ==200\").size != 0:\n",
        "            self._extract(pool, resultData.query(\"Code ==200\"))\n",
        "              \n",
        "          print(\"Bad call of pages:\")\n",
        "          # self._data(resultData.query(\"Code == 200\"))\n",
        "          # self.datasource.append(resultData.query(\"Code ==200\"))\n",
        "          offsets = resultData.query(\"Code !=200\")[\"Page\"].values.tolist()\n",
        "          err = resultData.query(\"Code !=200\")[\"Response\"].tolist()\n",
        "          err = [x.text for x in err]\n",
        "          print(err)\n",
        "          try:\n",
        "            with open(\"./BadCalls.text\", 'w', encoding='UTF-8') as fp:\n",
        "              fp.write(str(offsets))\n",
        "          except:\n",
        "            print(\"Fail to save badcalls\")\n",
        "          print(f\"Tentando de novo daqui a {self.sleeptry/60} min...\")\n",
        "\n",
        "\n",
        "          sleep(self.sleeptry)\n",
        "      except:\n",
        "        pass\n",
        "      print(\"---\")\n",
        "\n",
        "        \n",
        "        \n",
        "    \n",
        "    \n",
        "    # self._extract(pool, self.datasource)\n",
        "\n",
        "\n",
        "  @timer\n",
        "  def get(self, n = 10, offset = 0, papers = []):\n",
        "    self.n = n\n",
        "    self._offset = offset\n",
        "    # self.post[\"pageSize\"] = 10\n",
        "    # self.post[\"page\"] = page\n",
        "    self.all = []\n",
        "    # print('.post >>')\n",
        "    # print(self.post)\n",
        "    # self.datasource = ''\n",
        "    print(\"\\n\")\n",
        "    print(\"Searching...\")\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    with mp.Pool(self.poolCPU) as pool:\n",
        "      if self.n > 100:\n",
        "    \n",
        "        if len(papers) != 0:\n",
        "          self._offsets = papers\n",
        "        else:\n",
        "          self._offsets = list(range(self._offset, (self.n//100)+self._offset))\n",
        "          lista = [x*100 for x in self._offsets]\n",
        "          # lista.insert(0,self._offsets[0])\n",
        "          # print(lista)\n",
        "          offsets = lista\n",
        "          \n",
        "          print(\"offsets: \")\n",
        "          print(offsets)\n",
        "        \n",
        "        # print(self._offsets)\n",
        "      # for page in range(n//10):\n",
        "        self._runtime(pool, offsets)\n",
        "          \n",
        "        if n%100>0:\n",
        "          self.params['limit'] = n%100\n",
        "          self._offsets = [lista[-1] + 100]\n",
        "          # print(self.params)\n",
        "          # print(self._offsets)\n",
        "\n",
        "          self._runtime(pool, self._offsets)\n",
        "          \n",
        "      else:\n",
        "        # pass\n",
        "        if len(papers) != 0:\n",
        "          self._offsets = papers\n",
        "        else:  \n",
        "          offsets = [self._offset]\n",
        "        \n",
        "        self.params['limit'] = n\n",
        "        self._runtime(pool, offsets)\n",
        "\n",
        "    self.all = pd.concat(self.all, ignore_index=True)\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#   SearchAPI(sleeptry = 1*20, save=True, Savename = \"dataAPI\").get(10000)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AATNxAOsKOMg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "13d09fb6-2e58-43b7-b1fa-49625a4bb35f"
      },
      "source": [
        "res_api = SearchAPI(\n",
        "    search=\"decision making+optimization+artificial intelligence\",\n",
        "    sleeptry = 1*20,\n",
        "    save=False)\n",
        "\n",
        "res_api.get(10)\n",
        "\n",
        "res_api.all\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Searching...\n",
            "\n",
            "\n",
            "[_runtime]>> Start searching...\n",
            "<Response [200]>\n",
            "[_extract]>> Demorou 0.01s\n",
            "[_runtime]>> Demorou 0.76s\n",
            "[get]>> Demorou 0.85s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paperId</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>isOpenAccess</th>\n",
              "      <th>fieldsOfStudy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>d1e2126a46dfbf53580453c2f666f128afd7bb26</td>\n",
              "      <td>Multi-Objective Optimization Algorithm and Pre...</td>\n",
              "      <td>The operating mechanism of the biological immu...</td>\n",
              "      <td>False</td>\n",
              "      <td>[Computer Science]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20c19e2d76a9dc0e51860c174cfdac74aadf0369</td>\n",
              "      <td>Ship structural safety optimization: an integr...</td>\n",
              "      <td>ABSTRACT Majority of the world cargoes is tran...</td>\n",
              "      <td>False</td>\n",
              "      <td>[Computer Science]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4e8284cb075a8f0b8a9329f499fdba7b3c2d24fe</td>\n",
              "      <td>Artificial intelligence based commuter behavio...</td>\n",
              "      <td>Road traffic environments are highly dynamic a...</td>\n",
              "      <td>False</td>\n",
              "      <td>[Computer Science]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9563b0a34a76d6cf74c91d82583327ed97155668</td>\n",
              "      <td>Developing an artificial intelligence-based de...</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "      <td>[Engineering]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ddbbc8b2b3bdbf8b6329d1fbb19372d8e12812fb</td>\n",
              "      <td>An Artificial Intelligence Platform for Asset ...</td>\n",
              "      <td>An Artificial Intelligence system was develo...</td>\n",
              "      <td>False</td>\n",
              "      <td>[Computer Science, Medicine]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>b4cb44551a0e08bf2b5b332df9204e2ba10ff0af</td>\n",
              "      <td>The artificial intelligence and optimization o...</td>\n",
              "      <td>The problem of complicated dynamic system opti...</td>\n",
              "      <td>False</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>c3dae6d967186a3fce07246e156cbfb4eaf473da</td>\n",
              "      <td>Adaptive Simulation-Based Training of Artifici...</td>\n",
              "      <td>This work studies how an artifical-intelligenc...</td>\n",
              "      <td>False</td>\n",
              "      <td>[Computer Science]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8a86870043137e8f803906161ebaec5f4b4afaf0</td>\n",
              "      <td>Structuring an artificial intelligence based d...</td>\n",
              "      <td>Abstract Cyclic steam stimulation (CSS) is one...</td>\n",
              "      <td>False</td>\n",
              "      <td>[Engineering]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>fd4082c64e54e5aad62e14076fa7c8fc52d830f1</td>\n",
              "      <td>Collective behavior of artificial intelligence...</td>\n",
              "      <td>Collective behavior in the resource allocation...</td>\n",
              "      <td>False</td>\n",
              "      <td>[Computer Science]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>05ce03fcef7b71d4ec5c8f9db8527ec7b2f70ac0</td>\n",
              "      <td>Artificial Intelligence Enabling Water Desalin...</td>\n",
              "      <td>Recently, water desalination has been developi...</td>\n",
              "      <td>False</td>\n",
              "      <td>[Computer Science]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    paperId  ...                 fieldsOfStudy\n",
              "0  d1e2126a46dfbf53580453c2f666f128afd7bb26  ...            [Computer Science]\n",
              "1  20c19e2d76a9dc0e51860c174cfdac74aadf0369  ...            [Computer Science]\n",
              "2  4e8284cb075a8f0b8a9329f499fdba7b3c2d24fe  ...            [Computer Science]\n",
              "3  9563b0a34a76d6cf74c91d82583327ed97155668  ...                 [Engineering]\n",
              "4  ddbbc8b2b3bdbf8b6329d1fbb19372d8e12812fb  ...  [Computer Science, Medicine]\n",
              "5  b4cb44551a0e08bf2b5b332df9204e2ba10ff0af  ...                          None\n",
              "6  c3dae6d967186a3fce07246e156cbfb4eaf473da  ...            [Computer Science]\n",
              "7  8a86870043137e8f803906161ebaec5f4b4afaf0  ...                 [Engineering]\n",
              "8  fd4082c64e54e5aad62e14076fa7c8fc52d830f1  ...            [Computer Science]\n",
              "9  05ce03fcef7b71d4ec5c8f9db8527ec7b2f70ac0  ...            [Computer Science]\n",
              "\n",
              "[10 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At8ZsoI1ZfbI"
      },
      "source": [
        "# Consumir a classe `Search()`\n",
        "\n",
        "A duas formas de pesquisar utilizando `Search()`:\n",
        "\n",
        "1. A primeira é utilizando parâmetros na propria classe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF-CJR6nVaZV",
        "cellView": "form"
      },
      "source": [
        "#@title Classe para pesquisa no SemanticScholar\n",
        "import IPython\n",
        "from google.colab import output\n",
        "import pandas as pd\n",
        "\n",
        "class Search():\n",
        "  def __init__(self, **kwargs):\n",
        "    self.data = \"\"\n",
        "    self.data_0 = \"\"\n",
        "\n",
        "    self.search = kwargs.get('search', None)\n",
        "    self.fields = kwargs.get('fields', None)\n",
        "    self.limit = kwargs.get('limit', None)\n",
        "    self.offset = kwargs.get('offset', None)\n",
        "\n",
        "    if self.search == None and self.fields == None and self.limit == None and self.offset == None:\n",
        "      self._start(False)\n",
        "    else:\n",
        "      self._start(True)\n",
        "  \n",
        "  def _start(self, *args):\n",
        "\n",
        "    output.register_callback('notebook.searching', self._searching)\n",
        "    output.register_callback('notebook.AddListItem', self._add_list_item)\n",
        "    output.register_callback('notebook.mergeData', self._merge_data)\n",
        "    output.register_callback('notebook.error', self._error)\n",
        "\n",
        "\n",
        "    boxs = ''' \n",
        "        <label for=\"query\">Buscar: </label>\n",
        "        <input type=\"text\" id=\"query\" value=\"Machine Learning+Deep Learning\" style=\"width: 400px;\"/>\n",
        "        <br/>\n",
        "        <br/>\n",
        "        \n",
        "        <label for=\"fields\">Fields: </label>\n",
        "        <input type=\"text\" id=\"fields\" value=\"title,abstract,isOpenAccess,fieldsOfStudy\" style=\"width: 400px;\"/>\n",
        "        <br/>\n",
        "        <br/>\n",
        " \n",
        "        <label for=\"limit\">Limite: </label>\n",
        "        <input type=\"text\" id=\"limit\" value=\"10\" style=\"width: 50px;\"/><br/>\n",
        "        <br/>\n",
        "\n",
        "        <label for=\"limit\">Offset: </label>\n",
        "        <input type=\"text\" id=\"offset\" value=\"0\" style=\"width: 50px;\"/><br/>\n",
        "        <br/>\n",
        "\n",
        "        <button id='button'>Pesquisar</button>\n",
        "        <br/>\n",
        "        <br/>\n",
        "           '''\n",
        "\n",
        "    button = ''' document.querySelector('#button').onclick = async () => ''' # {}\n",
        "\n",
        "    search_query = '''\n",
        "            var search = document.getElementById(\"query\").value\n",
        "            var fields = document.getElementById(\"fields\").value\n",
        "            var limit = parseInt(document.getElementById(\"limit\").value)\n",
        "            var offset = parseInt(document.getElementById(\"offset\").value)\n",
        "                  '''\n",
        "    search_params = '''\n",
        "            var search = \"{search}\"\n",
        "            var fields = \"{fields}\"\n",
        "            var limit = parseInt({limit})\n",
        "            var offset = parseInt({offset})\n",
        "                  '''\n",
        "    engine = '''\n",
        "            google.colab.kernel.invokeFunction('notebook.searching', [], {});\n",
        "\n",
        "            if (limit >100) {\n",
        "              var number = limit\n",
        "              var data = \"\"\n",
        "              var promises = []\n",
        "              var offsetSearch = 0\n",
        "              var rest = 0\n",
        "\n",
        "              for (let index = 0; index < Math.floor(limit/100); index++) {\n",
        "                offsetSearch = 100*(index) + offset + 1*(index!==0)\n",
        "\n",
        "\n",
        "                promises.push(\n",
        "                  fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offsetSearch}&limit=100&fields=${fields}`)\n",
        "    .then(res=> {return(res.json())})\n",
        "    .then(res=> {return(res)})\n",
        "                )\n",
        "              }\n",
        "              \n",
        "              if (limit%100 !== 0) { \n",
        "                rest= limit%100\n",
        "                offsetSearch = offsetSearch+100\n",
        "                \n",
        "                console.log(rest)\n",
        "                console.log(offsetSearch)\n",
        "\n",
        "                promises.push(\n",
        "                fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offsetSearch}&limit=${rest}&fields=${fields}`)\n",
        "    .then(res=> {return(res.json())})\n",
        "    .then(res=> {return(res)})\n",
        "                )}\n",
        "\n",
        "              await Promise.all(promises).then(data=>{\n",
        "                google.colab.kernel.invokeFunction('notebook.mergeData', [data], {})\n",
        "              })\n",
        "              .catch(err=> { return (google.colab.kernel.invokeFunction('notebook.error', [err], {})) })\n",
        "\n",
        "            } else {\n",
        "\n",
        "            await fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offset}&limit=${limit}&fields=${fields}`)\n",
        "    .then(res=> {return(res.json())})\n",
        "    .then(res=> {\n",
        "      console.log(res)\n",
        "      console.log(\"AQUIII\")\n",
        "      return(google.colab.kernel.invokeFunction('notebook.AddListItem', [res], {}))})\n",
        "    .catch(err=> { return (\n",
        "      google.colab.kernel.invokeFunction('notebook.error', [err], {})) })\n",
        "            }\n",
        "                  '''\n",
        "\n",
        "    asyncfun = \"async function asyncfun()\"\n",
        "\n",
        "    if args[0]:\n",
        "\n",
        "      main_app =  \"<script>\" + search_params.format(search=self.search, fields=self.fields, limit=self.limit, offset=self.offset) + asyncfun + \"{\" + engine + \"}\" + \"asyncfun()\" + \"</script>\"\n",
        "\n",
        "      display(IPython.display.HTML(main_app))\n",
        "      \n",
        "    else:\n",
        "      main_app = boxs + \"<script>\" + button + \"{\" + search_query + engine + \"}\" + \"</script>\"\n",
        "      \n",
        "      display(IPython.display.HTML(main_app))\n",
        "\n",
        "    \n",
        "\n",
        "  def _error(self,value):\n",
        "    try:\n",
        "      print(\"ERRO na API SemanticScholar:\\n\")\n",
        "      print(value)\n",
        "    except:\n",
        "      pass \n",
        "\n",
        "  def _searching(self):\n",
        "    with output.use_tags('some_outputs'):\n",
        "      print(\"\\n\\nPesquisando...\")\n",
        "      sys.stdout.flush();\n",
        "\n",
        "  def _merge_data(self, data):\n",
        "    output.clear(output_tags='some_outputs')\n",
        "    print(f\"Achou {data[0]['total']} papers.\\n\")\n",
        "    self.data_0 = data\n",
        "\n",
        "    self.data = pd.DataFrame(data[0]['data'])\n",
        "\n",
        "    try:\n",
        "      for x in data[1:len(data)]:\n",
        "        try:\n",
        "          self.merge(pd.DataFrame(x['data']))\n",
        "        except:\n",
        "          self._error(x)\n",
        "    except:\n",
        "      pass \n",
        "\n",
        "    print(f\"\\nApi devolveu >> {self.data.shape[0]} papers\\n\" )\n",
        "    print(self.data.head())\n",
        "\n",
        "\n",
        "  def merge(self, data):\n",
        "    self.data = pd.concat([self.data, data], ignore_index=True ) \n",
        "\n",
        "  def _add_list_item(self, value):\n",
        "    output.clear(output_tags='some_outputs')\n",
        "\n",
        "    print(f\"Achou {value['total']} papers.\\n\")\n",
        "\n",
        "    self.data = pd.DataFrame(value['data'])\n",
        "\n",
        "    print(f\"Api devolveu >> {self.data.shape[0]} papers\\n\" )\n",
        "    \n",
        "    print(self.data.head())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCrYFOThaOR_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "691f62ac-eea8-4f30-e1ca-8760c974abce"
      },
      "source": [
        "Resultados = Search(search = \"Machine Learning+Deep Learning\" , fields = \"title,abstract,citationCount,isOpenAccess,fieldsOfStudy\", limit = \"200\", offset = \"0\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<script>\n",
              "            var search = \"Machine Learning+Deep Learning\"\n",
              "            var fields = \"title,abstract,citationCount,isOpenAccess,fieldsOfStudy\"\n",
              "            var limit = parseInt(200)\n",
              "            var offset = parseInt(0)\n",
              "                  async function asyncfun(){\n",
              "            google.colab.kernel.invokeFunction('notebook.searching', [], {});\n",
              "\n",
              "            if (limit >100) {\n",
              "              var number = limit\n",
              "              var data = \"\"\n",
              "              var promises = []\n",
              "              var offsetSearch = 0\n",
              "              var rest = 0\n",
              "\n",
              "              for (let index = 0; index < Math.floor(limit/100); index++) {\n",
              "                offsetSearch = 100*(index) + offset + 1*(index!==0)\n",
              "\n",
              "\n",
              "                promises.push(\n",
              "                  fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offsetSearch}&limit=100&fields=${fields}`)\n",
              "    .then(res=> {return(res.json())})\n",
              "    .then(res=> {return(res)})\n",
              "                )\n",
              "              }\n",
              "              \n",
              "              if (limit%100 !== 0) { \n",
              "                rest= limit%100\n",
              "                offsetSearch = offsetSearch+100\n",
              "                \n",
              "                console.log(rest)\n",
              "                console.log(offsetSearch)\n",
              "\n",
              "                promises.push(\n",
              "                fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offsetSearch}&limit=${rest}&fields=${fields}`)\n",
              "    .then(res=> {return(res.json())})\n",
              "    .then(res=> {return(res)})\n",
              "                )}\n",
              "\n",
              "              await Promise.all(promises).then(data=>{\n",
              "                google.colab.kernel.invokeFunction('notebook.mergeData', [data], {})\n",
              "              })\n",
              "              .catch(err=> { return (google.colab.kernel.invokeFunction('notebook.error', [err], {})) })\n",
              "\n",
              "            } else {\n",
              "\n",
              "            await fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offset}&limit=${limit}&fields=${fields}`)\n",
              "    .then(res=> {return(res.json())})\n",
              "    .then(res=> {\n",
              "      console.log(res)\n",
              "      console.log(\"AQUIII\")\n",
              "      return(google.colab.kernel.invokeFunction('notebook.AddListItem', [res], {}))})\n",
              "    .catch(err=> { return (\n",
              "      google.colab.kernel.invokeFunction('notebook.error', [err], {})) })\n",
              "            }\n",
              "                  }asyncfun()</script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Achou 656970 papers.\n",
            "\n",
            "\n",
            "Api devolveu >> 200 papers\n",
            "\n",
            "                                    paperId  ...       fieldsOfStudy\n",
            "0  846ff7afb7670d62f88b4a8cc99d306ffb81b075  ...          [Medicine]\n",
            "1  5dc53e50148b01fe8b9536eb79fa6b1dce924174  ...          [Medicine]\n",
            "2  7cc2e148d27a7508dd23c4e35eb63cc9b3e6a58f  ...  [Computer Science]\n",
            "3  59444b096f7c8a561d540102e8b5bfb189edabc6  ...                None\n",
            "4  eee313380ccb45807ea0afa3c1df86f6b48b8867  ...  [Computer Science]\n",
            "\n",
            "[5 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zotFl5-ficaO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d071c473-732e-42db-f615-6e0d5b3dce9e"
      },
      "source": [
        "# Os dados ficam na variável data, no qual é uma tabela do tipo pandas\n",
        "print(Resultados.data.columns)\n",
        "print(Resultados.data.sort_values(\"citationCount\", ascending = False ).head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['paperId', 'title', 'abstract', 'citationCount', 'isOpenAccess',\n",
            "       'fieldsOfStudy'],\n",
            "      dtype='object')\n",
            "                                     paperId  ...                    fieldsOfStudy\n",
            "17  a4cec122a08216fe8a3bc19b22e78fbaea096256  ...     [Medicine, Computer Science]\n",
            "14  46200b99c40e8586c8a0f588488ab6414119fb28  ...               [Computer Science]\n",
            "18  193edd20cae92c6759c18ce93eeea96afd9528eb  ...     [Computer Science, Medicine]\n",
            "16  9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d  ...               [Computer Science]\n",
            "11  3c8a456509e6c0805354bd40a35e3f2dbf8069b1  ...  [Computer Science, Mathematics]\n",
            "\n",
            "[5 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwfe0ZCyZ8YD"
      },
      "source": [
        "2. A segunda é atravez da api de busca, searchBox, no qual é possivel colocar os campos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCZf9v2yzskK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "f9b10f61-5183-476a-c0ef-70e39a75c4cd"
      },
      "source": [
        "Resultados_2 = Search()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              " \n",
              "        <label for=\"query\">Buscar: </label>\n",
              "        <input type=\"text\" id=\"query\" value=\"Machine Learning+Deep Learning\" style=\"width: 400px;\"/>\n",
              "        <br/>\n",
              "        <br/>\n",
              "        \n",
              "        <label for=\"fields\">Fields: </label>\n",
              "        <input type=\"text\" id=\"fields\" value=\"title,abstract,isOpenAccess,fieldsOfStudy\" style=\"width: 400px;\"/>\n",
              "        <br/>\n",
              "        <br/>\n",
              " \n",
              "        <label for=\"limit\">Limite: </label>\n",
              "        <input type=\"text\" id=\"limit\" value=\"10\" style=\"width: 50px;\"/><br/>\n",
              "        <br/>\n",
              "\n",
              "        <label for=\"limit\">Offset: </label>\n",
              "        <input type=\"text\" id=\"offset\" value=\"0\" style=\"width: 50px;\"/><br/>\n",
              "        <br/>\n",
              "\n",
              "        <button id='button'>Pesquisar</button>\n",
              "        <br/>\n",
              "        <br/>\n",
              "           <script> document.querySelector('#button').onclick = async () => {\n",
              "            var search = document.getElementById(\"query\").value\n",
              "            var fields = document.getElementById(\"fields\").value\n",
              "            var limit = parseInt(document.getElementById(\"limit\").value)\n",
              "            var offset = parseInt(document.getElementById(\"offset\").value)\n",
              "                  \n",
              "            google.colab.kernel.invokeFunction('notebook.searching', [], {});\n",
              "\n",
              "            if (limit >100) {\n",
              "              var number = limit\n",
              "              var data = \"\"\n",
              "              var promises = []\n",
              "              var offsetSearch = 0\n",
              "              var rest = 0\n",
              "\n",
              "              for (let index = 0; index < Math.floor(limit/100); index++) {\n",
              "                offsetSearch = 100*(index) + offset + 1*(index!==0)\n",
              "\n",
              "\n",
              "                promises.push(\n",
              "                  fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offsetSearch}&limit=100&fields=${fields}`)\n",
              "    .then(res=> {return(res.json())})\n",
              "    .then(res=> {return(res)})\n",
              "                )\n",
              "              }\n",
              "              \n",
              "              if (limit%100 !== 0) { \n",
              "                rest= limit%100\n",
              "                offsetSearch = offsetSearch+100\n",
              "                \n",
              "                console.log(rest)\n",
              "                console.log(offsetSearch)\n",
              "\n",
              "                promises.push(\n",
              "                fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offsetSearch}&limit=${rest}&fields=${fields}`)\n",
              "    .then(res=> {return(res.json())})\n",
              "    .then(res=> {return(res)})\n",
              "                )}\n",
              "\n",
              "              await Promise.all(promises).then(data=>{\n",
              "                google.colab.kernel.invokeFunction('notebook.mergeData', [data], {})\n",
              "              })\n",
              "              .catch(err=> { return (google.colab.kernel.invokeFunction('notebook.error', [err], {})) })\n",
              "\n",
              "            } else {\n",
              "\n",
              "            await fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offset}&limit=${limit}&fields=${fields}`)\n",
              "    .then(res=> {return(res.json())})\n",
              "    .then(res=> {\n",
              "      console.log(res)\n",
              "      console.log(\"AQUIII\")\n",
              "      return(google.colab.kernel.invokeFunction('notebook.AddListItem', [res], {}))})\n",
              "    .catch(err=> { return (\n",
              "      google.colab.kernel.invokeFunction('notebook.error', [err], {})) })\n",
              "            }\n",
              "                  }</script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Achou 656969 papers.\n",
            "\n",
            "Api devolveu >> 10 papers\n",
            "\n",
            "                                    paperId  ...       fieldsOfStudy\n",
            "0  846ff7afb7670d62f88b4a8cc99d306ffb81b075  ...          [Medicine]\n",
            "1  5dc53e50148b01fe8b9536eb79fa6b1dce924174  ...          [Medicine]\n",
            "2  7cc2e148d27a7508dd23c4e35eb63cc9b3e6a58f  ...  [Computer Science]\n",
            "3  59444b096f7c8a561d540102e8b5bfb189edabc6  ...                None\n",
            "4  eee313380ccb45807ea0afa3c1df86f6b48b8867  ...  [Computer Science]\n",
            "\n",
            "[5 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8G0F7fEFz9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "844386d5-c59e-4d41-c49a-0bb255374e7c"
      },
      "source": [
        "print(Resultados_2.data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                    paperId  ...       fieldsOfStudy\n",
            "0  846ff7afb7670d62f88b4a8cc99d306ffb81b075  ...          [Medicine]\n",
            "1  5dc53e50148b01fe8b9536eb79fa6b1dce924174  ...          [Medicine]\n",
            "2  7cc2e148d27a7508dd23c4e35eb63cc9b3e6a58f  ...  [Computer Science]\n",
            "3  59444b096f7c8a561d540102e8b5bfb189edabc6  ...                None\n",
            "4  eee313380ccb45807ea0afa3c1df86f6b48b8867  ...  [Computer Science]\n",
            "5  46479bbea7749cb2db35b139206039531327053c  ...  [Computer Science]\n",
            "6  b69fe5a837277ddbea5215d6bacd3a902e9d11ce  ...          [Medicine]\n",
            "7  b0bf64ccbd651e8c7bc141d8aabaecff562e93a1  ...  [Computer Science]\n",
            "8  042ab08ec6782cf217f13175162bfd48f7350114  ...  [Computer Science]\n",
            "9  03e7832982986159400a8eeab148487ffcfabe56  ...  [Computer Science]\n",
            "\n",
            "[10 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNrq2PcNtb-f"
      },
      "source": [
        "\n",
        "# **SearchWeb()**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_OkAhJHt3tS"
      },
      "source": [
        "import requests\n",
        "import json\n",
        "import multiprocessing as mp\n",
        "import os\n",
        "# import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import ast\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "from time import sleep, time\n",
        "\n",
        "\n",
        "def timer(fun):\n",
        "  def warper(*args,**kwargs):\n",
        "    start = time()\n",
        "    d = fun(*args,**kwargs)\n",
        "    end = time()\n",
        "    print(f\"[{fun.__name__}]>> Demorou {round(end-start,2)}s\")\n",
        "    return d\n",
        "  return warper\n",
        "\n",
        "\n",
        "\n",
        "class SearchWeb():\n",
        "  def __init__(self, search=\"Machine Learning+Deep Learning\", poolCPU = 4, sleeptry=5, save = False, **kwargs):\n",
        "     \n",
        "    self.sleeptry = sleeptry\n",
        "    self.poolCPU = poolCPU\n",
        "    self.saveName = kwargs.get('Savename', \"Data\")\n",
        "\n",
        "    self.badcall = []\n",
        "    self._start = True\n",
        "\n",
        "    self.saveFile = save\n",
        "    self._search = search\n",
        "    self._sort = kwargs.get('sort', \"relevance\")\n",
        "    self._authors = kwargs.get('authors', [])\n",
        "    self._coAuthors = kwargs.get('coAuthors', [])\n",
        "    self._venues = kwargs.get('venues', ['PloS one', 'AAAI', 'Scientific reports', 'IEEE Access', 'ArXiv', 'Expert Syst. Appl.', 'ICML', 'Neurocomputing', 'Sensors', 'Remote. Sens.'])\n",
        "    self._yearFilter = kwargs.get('yearFilter', None) # {\"min\": 2008,\"max\": 2021}\n",
        "    self._requireViewablePdf = kwargs.get('requireViewablePdf', False)\n",
        "    self._publicationTypes = kwargs.get('publicationTypes', [\"ClinicalTrial\", \"CaseReport\", \"Editorial\",\"Study\",\"Book\",\"News\",\"Review\",\"Conference\",\"LettersAndComments\",\"JournalArticle\"])\n",
        "    self._fieldsOfStudy = kwargs.get('fieldsOfStudy', [\"biology\",\"art\",\"business\",\"computer-science\",\"chemistry\",\"economics\",\"engineering\",\"environmental-science\",\"geography\",\"geology\",\"history\",\"materials-science\",\"mathematics\",\"medicine\",\"philosophy\",\"physics\",\"political-science\",\"psychology\",\"sociology\"])\n",
        "    self._useFallbackRankerService = kwargs.get('useFallbackRankerService', False)\n",
        "    self._useFallbackSearchCluster = kwargs.get('useFallbackSearchCluster', False)\n",
        "    self._hydrateWithDdb = kwargs.get('hydrateWithDdb', True)\n",
        "    self._includeTldrs = kwargs.get('includeTldrs', True)\n",
        "    self._performTitleMatch = kwargs.get('performTitleMatch', True)\n",
        "    self._includeBadges = kwargs.get('includeBadges', True)\n",
        "    self._tldrModelVersion = kwargs.get('tldrModelVersion', 'v2.0.0')\n",
        "    self._getQuerySuggestions = kwargs.get('getQuerySuggestions', False)\n",
        "\n",
        "\n",
        "    self.post = {\n",
        "    \"page\": 1, \n",
        "    \"pageSize\": 10,\n",
        "    \"queryString\": self._search,\n",
        "    \"sort\": self._sort,\n",
        "    \"authors\": self._authors,\n",
        "    \"coAuthors\": self._coAuthors,\n",
        "    \"venues\": self._venues,\n",
        "    \"yearFilter\": self._yearFilter,\n",
        "    \"requireViewablePdf\": self._requireViewablePdf,\n",
        "    \"publicationTypes\": self._publicationTypes,\n",
        "    \"externalContentTypes\": [],\n",
        "    \"fieldsOfStudy\": self._fieldsOfStudy,\n",
        "    \"useFallbackRankerService\": self._useFallbackRankerService,\n",
        "    \"useFallbackSearchCluster\": self._useFallbackSearchCluster,\n",
        "    \"hydrateWithDdb\": self._hydrateWithDdb,\n",
        "    \"includeTldrs\": self._includeTldrs,\n",
        "    \"performTitleMatch\": self._performTitleMatch,\n",
        "    \"includeBadges\": self._includeBadges,\n",
        "    \"tldrModelVersion\": \"v2.0.0\",\n",
        "    \"getQuerySuggestions\": self._getQuerySuggestions,\n",
        "    }\n",
        "\n",
        "  \n",
        "  \n",
        "  def _paperExtract(self, data):\n",
        "    p = {\n",
        "        \"authors\": [author[0]['name'] for author in data.get('authors',[{'name':None},None])],\n",
        "        \"id\": data.get('id',None),\n",
        "        \"socialLinks\": data.get('socialLinks',None),\n",
        "        \"title\": data.get('title',{'text':None})['text'],\n",
        "        \"paperAbstract\": data.get('paperAbstract',{'text':None})['text'],\n",
        "        \"year\": data.get('year',{'text':None})['text'],\n",
        "        \"venue\": data.get('venue',{'text':None})['text'],\n",
        "        \"citationContexts\":data.get('citationContexts',None),\n",
        "        \"citationStats\": data.get('citationStats',None),\n",
        "        \"sources\":data.get('sources',None),\n",
        "        \"externalContentStats\":data.get('externalContentStats',None),\n",
        "        \"journal\":data.get('journal',None),\n",
        "        \"presentationUrls\":data.get('presentationUrls',None),\n",
        "        \"links\": data.get('links',None),\n",
        "        \"primaryPaperLink\": data.get('primaryPaperLink',None),\n",
        "        \"alternatePaperLinks\": data.get('alternatePaperLinks',None),\n",
        "        \"entities\": [author['name'] for author in data.get('entities',[{'name':None}])],\n",
        "        \"entityRelations\": data.get('entityRelations',None),\n",
        "        \"blogs\":data.get('blogs',None),\n",
        "        \"videos\":data.get('videos',None),\n",
        "        \"githubReferences\": data.get('githubReferences',None),\n",
        "        \"scorecardStats\": data.get('scorecardStats',None),\n",
        "        \"fieldsOfStudy\":data.get('fieldsOfStudy',None),\n",
        "        \"pubDate\":data.get('pubDate',None),\n",
        "        \"pubUpdateDate\":data.get('pubUpdateDate',None),\n",
        "        \"badges\":data.get('badges',None),\n",
        "        \"tldr\":data.get('tldr',None)\n",
        "        }\n",
        "    return p\n",
        "\n",
        "  def _query(self, page):\n",
        "    url = \"https://www.semanticscholar.org/api/1/search\"\n",
        "    post = self.post.copy()\n",
        "    post[\"page\"] = page\n",
        "    try:\n",
        "      res = requests.post(url, json=post, timeout=15)\n",
        "      res.encoding = 'utf-8'\n",
        "      return [res, page, res.status_code]\n",
        "    except:\n",
        "      return [None, page, 400 ]\n",
        "      \n",
        "\n",
        "  def _json(self, res):\n",
        "#     print(res.text)\n",
        "    return json.loads(res.text).copy()\n",
        "\n",
        "    # c['querySuggestions']\n",
        "    # c['totalPages']\n",
        "    # c['totalResults']\n",
        "  def save(self, name, data):\n",
        "    try:\n",
        "      with open(f'./{name}.json', 'w',encoding='UTF-8') as fp:\n",
        "          json.dump(data, fp)\n",
        "    except:\n",
        "      print(\"[Save]>> Error to save the data.\")\n",
        "  \n",
        "  def load_json(self, path):\n",
        "    try:\n",
        "      with open(f'{path}', 'r', encoding='UTF-8') as fp:\n",
        "          return json.load(fp)\n",
        "    except:\n",
        "      print(\"[load_json]>> Error to load json file.\")\n",
        "    \n",
        "  def _startFile(self, find):\n",
        "    jsonFile = Path(f\"./{self.saveName}.json\")\n",
        "    textFile = Path(f\"./{self.saveName}.text\")\n",
        "    \n",
        "    if jsonFile.is_file():\n",
        "      if find:\n",
        "        try:\n",
        "          print(f\"[_startFile] >> Loading ./{self.saveName}.json\")\n",
        "          with open(f'./{self.saveName}.json', 'r',encoding='UTF-8') as f:\n",
        "            data = json.load(f)\n",
        "          print(f\"[Create] >> Creating a ./{self.saveName}.text file to save data.\")\n",
        "          with open(f'./{self.saveName}.text', 'w',encoding='UTF-8') as fp:\n",
        "            fp.write(\"{\\\"Results\\\": [\")\n",
        "        \n",
        "        # print(data['Results'])\n",
        "          self._save(data['Results'])\n",
        "        except:\n",
        "          print(f\"[_startFile] >> Fail to load ./{self.saveName}.json\")\n",
        "          try:\n",
        "            # print(f\"[Create] >> Creating a ./{self.saveName}.text file to save data.\")\n",
        "            with open(f'./{self.saveName}.text', 'w', encoding='UTF-8') as fp:\n",
        "              fp.write(\"{\\\"Results\\\": [\")\n",
        "          except:\n",
        "            print(\"[Create] >> Fail\")\n",
        "      else:\n",
        "        return False\n",
        "    else:\n",
        "      try:\n",
        "        print(f\"[Create] >> Creating a ./{self.saveName}.text file to save data.\")\n",
        "        with open(f'./{self.saveName}.text', 'w',encoding='UTF-8') as fp:\n",
        "          fp.write(\"{\\\"Results\\\": [\")\n",
        "      except:\n",
        "        print(\"[Create] >> Fail\")\n",
        "    return True\n",
        "\n",
        "\n",
        "  def _save(self, check_point):\n",
        "    if str(check_point) == '[]' or str(check_point) == '[,]':\n",
        "      return _\n",
        "    else:\n",
        "      text = str(check_point)\n",
        "\n",
        "      text = re.sub('^\\[', '', text)\n",
        "      text = re.sub('\\]$', '', text)\n",
        "\n",
        "      \n",
        "      with open(f'./{self.saveName}.text', 'a', encoding='utf-8') as fp:\n",
        "        fp.write(text)\n",
        "          # json.dump(self.all['Results'], fp)\n",
        "      print(f\"[Save] >> Saving check_point at current directory, ./{self.saveName}.text\")\n",
        "      \n",
        "\n",
        "  def _endFile(self):\n",
        "    # ast.literal_eval(text)\n",
        "    try:\n",
        "      with open(f'./{self.saveName}.text', 'a', encoding='UTF-8') as fp:\n",
        "        fp.write(']}')\n",
        "      \n",
        "      try:\n",
        "        with open(f'./{self.saveName}.text', 'r', encoding='UTF-8') as fp:\n",
        "          text = fp.read()\n",
        "          text_dict = ast.literal_eval(text)\n",
        "        \n",
        "      \n",
        "      # os.rename(f'./{self.saveName}.text', f'./{self.saveName}.json')\n",
        "      # os.remove(f\"./{self.saveName}.text\")\n",
        "      # print(text_dict)\n",
        "        with open(f'./{self.saveName}.json', 'w', encoding='UTF-8') as fp:\n",
        "          json.dump(text_dict, fp)\n",
        "        print(f\"[Close] >> Closed and save in ./{self.saveName}.json file the data.\")\n",
        "      except:\n",
        "        print(f\"[Close] >> Fail to save the data ./{self.saveName}.json file.\")\n",
        "\n",
        "    except:\n",
        "      print('[Close] >> Fail')\n",
        "\n",
        "  \n",
        "\n",
        "  @timer\n",
        "  def _extract(self, pool, data):\n",
        "    try:\n",
        "      # print(\"data\")\n",
        "      # print(data)\n",
        "\n",
        "      # print(\"data['Response'].tolist()\")\n",
        "      # print(data['Response'].tolist())\n",
        "      self.papers_text = pool.map(self._json, data['Response'].tolist())\n",
        "      # print(\"self.papers_text\")\n",
        "      # print(self.papers_text)\n",
        "\n",
        "      if self._start:\n",
        "        print('\\n ---')\n",
        "        print(f\"Total Results: {self.papers_text[0]['totalResults']}\")\n",
        "        print(f\"Total Pages: {self.papers_text[0]['totalPages']}\")\n",
        "        print(f\"Query Suggestions: {self.papers_text[0]['querySuggestions']}\")\n",
        "        print('--- \\n')\n",
        "        self.totalPages = self.papers_text[0]['totalPages']\n",
        "        self.totalResults = self.papers_text[0]['totalResults']\n",
        "        self._start = False\n",
        "\n",
        "      \n",
        "      print(\"[_extract] >> extracting relevant data.\")\n",
        "      check_point= [{\"Page\": {\"N_Page\":page['query']['page'],\n",
        "                                   \"N_Papers\":len(page['results']),\n",
        "                                   \"Papers\": pool.map(self._paperExtract,\n",
        "                                                      page['results'])}} for page in self.papers_text]\n",
        "\n",
        "\n",
        "      # print(check_point)\n",
        "      if self.saveFile:\n",
        "        try:\n",
        "          self._save(check_point)\n",
        "        except:\n",
        "          print(\"_save >> [Fail] to save.\")\n",
        "          print(\"_extract>> [Fail], see .badcall to reextract content.\")\n",
        "          self.badcall.append(self.papers_text)\n",
        "          # print(self.badcall)\n",
        "      else:\n",
        "        self.all[\"Results\"].extend(check_point)\n",
        "\n",
        "\n",
        "    except:\n",
        "      print(\"_extract>> [Fail], see .badcall to reextract content.\")\n",
        "      self.badcall.append(self.papers_text)\n",
        "      print(self.badcall)\n",
        "    \n",
        "    \n",
        "\n",
        "  # def _data(self, data):\n",
        "  #   if type(self.datasource) == str:\n",
        "  #     self.datasource = data\n",
        "  #   else:\n",
        "  #     self.datasource = pd.concat([self.datasource, data])\n",
        "\n",
        "  @timer\n",
        "  def _runtime(self, pool, pages):\n",
        "    self.totalPages = 0\n",
        "\n",
        "    find = False\n",
        "    \n",
        "    while True:\n",
        "      if self.saveFile:\n",
        "        close = self._startFile(find)\n",
        "      \n",
        "      print('\\n')\n",
        "      print('[_runtime]>> Start searching...')\n",
        "      # print(self.totalPages)\n",
        "      # print(self.totalResults)\n",
        "      # print(self.n)\n",
        "\n",
        "      if self.totalResults < self.n:\n",
        "        self.n = self.totalResults\n",
        "        pages = list(range(self._page, self.totalPages))\n",
        "      \n",
        "      try:\n",
        "        res = pool.map(self._query, pages)\n",
        "        # print(res)\n",
        "        resultData = pd.DataFrame(res, columns=[\"Response\", \"Page\", \"Code\"])\n",
        "        resultData.set_index(\"Page\")\n",
        "        \n",
        "        if resultData.query(\"Code !=200\").size == 0:\n",
        "          # self._data(resultData)\n",
        "          self._extract(pool, resultData.query(\"Code ==200\"))\n",
        "          if self.saveFile:\n",
        "            if close:\n",
        "              self._endFile()\n",
        "              find = True\n",
        "          break\n",
        "        else:\n",
        "          find = False\n",
        "\n",
        "          if resultData.query(\"Code ==200\").size != 0:\n",
        "            self._extract(pool, resultData.query(\"Code ==200\"))\n",
        "            if self.saveFile:\n",
        "              if close:\n",
        "                find = True\n",
        "                self._endFile()\n",
        "              \n",
        "          print(\"Bad call of pages:\")\n",
        "          # self._data(resultData.query(\"Code == 200\"))\n",
        "          # self.datasource.append(resultData.query(\"Code ==200\"))\n",
        "          pages = resultData.query(\"Code !=200\")[\"Page\"].values.tolist()\n",
        "          print(pages)\n",
        "          try:\n",
        "            with open(\"./BadCalls.text\", 'w', encoding='UTF-8') as fp:\n",
        "              fp.write(str(pages))\n",
        "          except:\n",
        "            print(\"Fail to save badcalls\")\n",
        "          print(f\"Tentando de novo daqui a {self.sleeptry/60} min...\")\n",
        "\n",
        "\n",
        "          sleep(self.sleeptry)\n",
        "      except:\n",
        "        pass\n",
        "      print(\"---\")\n",
        "        \n",
        "        \n",
        "    \n",
        "    \n",
        "    # self._extract(pool, self.datasource)\n",
        "\n",
        "\n",
        "  @timer\n",
        "  def get(self, n = 10, page = 1, pages = []):\n",
        "    self._pages = pages\n",
        "    self.n = n\n",
        "    self._page = page\n",
        "    self.totalResults = 1000000000000000000000\n",
        "    self.post[\"pageSize\"] = 10\n",
        "    self.post[\"page\"] = page\n",
        "    self.all = {\"Results\": []}\n",
        "    print('.post >>')\n",
        "    print(self.post)\n",
        "    # self.datasource = ''\n",
        "    print(\"\\n\")\n",
        "    print(\"Searching...\")\n",
        "    print(self.all)\n",
        "\n",
        "    \n",
        "\n",
        "    with mp.Pool(self.poolCPU) as pool:\n",
        "      if self.n > 10:\n",
        "    \n",
        "        if len(pages) != 0:\n",
        "          self._pages = pages\n",
        "        else:  \n",
        "          self._pages = list(range(self._page, (self.n//10)+self._page))\n",
        "      # for page in range(n//10):\n",
        "        self._runtime(pool, self._pages)\n",
        "          \n",
        "        if n%10>0:\n",
        "          self._pages = [self.n//10+self._page]\n",
        "          self.post[\"pageSize\"] = self.n%10\n",
        "\n",
        "          self._runtime(pool, self._pages)\n",
        "          \n",
        "      else:\n",
        "        # pass\n",
        "        self._pages = [self._page]\n",
        "        self.post[\"page\"] = self._page\n",
        "        self.post[\"pageSize\"] = self.n\n",
        "\n",
        "        self._runtime(pool, self._pages)\n",
        "\n",
        "    \n",
        "      # self._extract(pool, self.datasource)\n",
        "\n",
        "\n",
        "##### Description #####\n",
        "# ex. {\"params\": value} \n",
        "#  \n",
        "##### Params that can pass in SearchWeb().get(params = value): #####\n",
        "#     {\n",
        "#     \"n\": 1000 (how much papers)\n",
        "#     \"page\": 1, (where start search)\n",
        "#      }\n",
        "##### Params that can pass in SearchWeb(params = value): #####\n",
        "# data = '''{\n",
        "#     \"Savename\": 'Data'\n",
        "#     \"sleeptry\": 3 (seconds)\n",
        "#     \"poolCPU\": 4 (Number of clusters, CPU)\n",
        "#     \"save\": False\n",
        "#     \"queryString\": \"Machine Learning+Deep Learning\",\n",
        "#     \"sort\": \"total-citations\", #influence #\"pub-date\" #relevance\n",
        "#     \"authors\": [],\n",
        "#     \"coAuthors\": [],\n",
        "#     \"venues\": [\n",
        "#         \"PloS one\",\n",
        "#         \"AAAI\",\n",
        "#         \"Scientific reports\",\n",
        "#         \"IEEE Access\",\n",
        "#         \"ArXiv\",\n",
        "#         \"Expert Syst. Appl.\",\n",
        "#         \"ICML\",\n",
        "#         \"Neurocomputing\",\n",
        "#         \"Sensors\",\n",
        "#         \"Remote. Sens.\"\n",
        "#     ],\n",
        "#     \"yearFilter\": {\n",
        "#         \"min\": 2008,\n",
        "#         \"max\": 2021\n",
        "#     },\n",
        "#     \"requireViewablePdf\": True,\n",
        "#     \"publicationTypes\": [\n",
        "#         \"ClinicalTrial\",\n",
        "#         \"CaseReport\",\n",
        "#         \"Editorial\",\n",
        "#         \"Study\",\n",
        "#         \"Book\",\n",
        "#         \"News\",\n",
        "#         \"Review\",\n",
        "#         \"Conference\",\n",
        "#         \"LettersAndComments\",\n",
        "#         \"JournalArticle\"\n",
        "#     ],\n",
        "#     \"externalContentTypes\": [],\n",
        "#     \"fieldsOfStudy\": [\n",
        "#         \"biology\",\n",
        "#         \"art\",\n",
        "#         \"business\",\n",
        "#         \"computer-science\",\n",
        "#         \"chemistry\",\n",
        "#         \"economics\",\n",
        "#         \"engineering\",\n",
        "#         \"environmental-science\",\n",
        "#         \"geography\",\n",
        "#         \"geology\",\n",
        "#         \"history\",\n",
        "#         \"materials-science\",\n",
        "#         \"mathematics\",\n",
        "#         \"medicine\",\n",
        "#         \"philosophy\",\n",
        "#         \"physics\",\n",
        "#         \"political-science\",\n",
        "#         \"psychology\",\n",
        "#         \"sociology\"\n",
        "#     ],\n",
        "#     \"useFallbackRankerService\": False,\n",
        "#     \"useFallbackSearchCluster\": False,\n",
        "#     \"hydrateWithDdb\": True,\n",
        "#     \"includeTldrs\": True,\n",
        "#     \"performTitleMatch\": True,\n",
        "#     \"includeBadges\": True,\n",
        "#     \"tldrModelVersion\": \"v2.0.0\",\n",
        "#     \"getQuerySuggestions\": False\n",
        "# }\n",
        "# '''\n",
        "# '''\n",
        "# Obs. Params that have a list can be a empty list\n",
        "# Ex. {\"venues\": []}\n",
        "\n",
        "### Discoment here to have a script\n",
        "# if __name__ == '__main__':\n",
        "#   SearchWeb(\n",
        "#     search= \"decision making+optimization+artificial intelligence\",\n",
        "#     sort= \"influence\",\n",
        "#     Savename = \"influence_data\",\n",
        "#     save=True,\n",
        "#     poolCPU = 4,\n",
        "#     sleeptry = 3.5*60,\n",
        "#     venues = [],\n",
        "#     publicationTypes = ['JournalArticle'],\n",
        "#     fieldsOfStudy = [],\n",
        "#     getQuerySuggestions = True\n",
        "#     ).get(20000, page = 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_j2uslvt8A2"
      },
      "source": [
        "from_Webpage = SearchWeb(search= \"Machine Learning+Deep Learning\", sort= \"total-citations\", save=True, saveName = \"Data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzUBus_Vt9-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66c4cdfe-8a5c-485c-9a4c-af7dbd88e9e8"
      },
      "source": [
        "# Retorna 100 papers a partir da pag. 2 com base nos parametros passados em SearchWeb() que constitui ().post\n",
        "from_Webpage.get(100, page = 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".post >>\n",
            "{'page': 2, 'pageSize': 10, 'queryString': 'Machine Learning+Deep Learning', 'sort': 'total-citations', 'authors': [], 'coAuthors': [], 'venues': ['PloS one', 'AAAI', 'Scientific reports', 'IEEE Access', 'ArXiv', 'Expert Syst. Appl.', 'ICML', 'Neurocomputing', 'Sensors', 'Remote. Sens.'], 'yearFilter': None, 'requireViewablePdf': False, 'publicationTypes': ['ClinicalTrial', 'CaseReport', 'Editorial', 'Study', 'Book', 'News', 'Review', 'Conference', 'LettersAndComments', 'JournalArticle'], 'externalContentTypes': [], 'fieldsOfStudy': ['biology', 'art', 'business', 'computer-science', 'chemistry', 'economics', 'engineering', 'environmental-science', 'geography', 'geology', 'history', 'materials-science', 'mathematics', 'medicine', 'philosophy', 'physics', 'political-science', 'psychology', 'sociology'], 'useFallbackRankerService': False, 'useFallbackSearchCluster': False, 'hydrateWithDdb': True, 'includeTldrs': True, 'performTitleMatch': True, 'includeBadges': True, 'tldrModelVersion': 'v2.0.0', 'getQuerySuggestions': False}\n",
            "\n",
            "\n",
            "Searching...\n",
            "{'Results': []}\n",
            "[Create] >> Creating a ./Data.text file to save data.\n",
            "\n",
            "\n",
            "[_runtime]>> Start searching...\n",
            "\n",
            " ---\n",
            "Total Results: 51050\n",
            "Total Pages: 5105\n",
            "Query Suggestions: []\n",
            "--- \n",
            "\n",
            "[_extract] >> extracting relevant data.\n",
            "[Save] >> Saving check_point at current directory, ./Data.text\n",
            "[_extract]>> Demorou 1.23s\n",
            "[Close] >> Closed and save in ./Data.json file the data.\n",
            "[_runtime]>> Demorou 7.57s\n",
            "[get]>> Demorou 7.69s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_AxXsbJuARM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eaae51d-4942-40e9-c415-dc9191cd86cd"
      },
      "source": [
        "# Tudo que vem com base em 1 paper (dict.)\n",
        "\n",
        "json_file = SearchWeb().load_json(\"Data.json\")\n",
        "\n",
        "json_file[\"Results\"][0]['Page'][\"Papers\"][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'alternatePaperLinks': [],\n",
              " 'authors': ['E. Samaniego',\n",
              "  'C. Anitescu',\n",
              "  'S. Goswami',\n",
              "  'Vien Minh Nguyen-Thanh',\n",
              "  'Hongwei Guo',\n",
              "  'Khader M. Hamdia',\n",
              "  'T. Rabczuk',\n",
              "  'X. Zhuang'],\n",
              " 'badges': [{'id': 'OPEN_ACCESS'}],\n",
              " 'blogs': [],\n",
              " 'citationContexts': [],\n",
              " 'citationStats': {'citationAcceleration': 0.703125,\n",
              "  'citationVelocity': 58.333333333333336,\n",
              "  'citedByBuckets': [{'count': 2, 'endKey': 2019, 'startKey': 2019},\n",
              "   {'count': 64, 'endKey': 2020, 'startKey': 2020},\n",
              "   {'count': 109, 'endKey': 2021, 'startKey': 2021}],\n",
              "  'estNumCitations': 84.08133939843164,\n",
              "  'firstCitationVelocityYear': 2019,\n",
              "  'keyCitationRate': 0.0,\n",
              "  'keyCitedByBuckets': [],\n",
              "  'lastCitationVelocityYear': 2021,\n",
              "  'numCitations': 175,\n",
              "  'numKeyCitations': 0,\n",
              "  'numKeyReferences': 2,\n",
              "  'numReferences': 48,\n",
              "  'numViewableReferences': 48},\n",
              " 'entities': ['Machine learning',\n",
              "  'Computational mechanics',\n",
              "  'Finite element method',\n",
              "  'Isogeometric analysis',\n",
              "  'Computation',\n",
              "  'Loss function',\n",
              "  'Collocation',\n",
              "  'Approximation algorithm',\n",
              "  'Discretization',\n",
              "  'MIT Engineering Systems Division',\n",
              "  'In-game advertising'],\n",
              " 'entityRelations': [],\n",
              " 'externalContentStats': [{'contentType': {'id': 'GITHUB_REPO'}, 'count': 1}],\n",
              " 'fieldsOfStudy': ['Computer Science', 'Mathematics'],\n",
              " 'githubReferences': [],\n",
              " 'id': '7af9fde25baab82e034ac0f9ef21fe1cbd19fdc1',\n",
              " 'journal': {'name': 'ArXiv', 'volume': 'abs/1908.10407'},\n",
              " 'links': [{'linkType': 'arxiv',\n",
              "   'url': 'https://arxiv.org/pdf/1908.10407.pdf'}],\n",
              " 'paperAbstract': 'Partial Differential Equations (PDE) are fundamental to model different phenomena in science and engineering mathematically. Solving them is a crucial step towards a precise knowledge of the behaviour of natural and engineered systems. In general, in order to solve PDEs that represent real systems to an acceptable degree, analytical methods are usually not enough. One has to resort to discretization methods. For engineering problems, probably the best known option is the finite element method (FEM). However, powerful alternatives such as mesh-free methods and Isogeometric Analysis (IGA) are also available. The fundamental idea is to approximate the solution of the PDE by means of functions specifically built to have some desirable properties. In this contribution, we explore Deep Neural Networks (DNNs) as an option for approximation. They have shown impressive results in areas such as visual recognition. DNNs are regarded here as function approximation machines. There is great flexibility to define their structure and important advances in the architecture and the efficiency of the algorithms to implement them make DNNs a very interesting alternative to approximate the solution of a PDE. We concentrate in applications that have an interest for Computational Mechanics. Most contributions that have decided to explore this possibility have adopted a collocation strategy. In this contribution, we concentrate in mechanical problems and analyze the energetic format of the PDE. The energy of a mechanical system seems to be the natural loss function for a machine learning method to approach a mechanical problem. As proofs of concept, we deal with several problems and explore the capabilities of the method for applications in engineering.',\n",
              " 'presentationUrls': [],\n",
              " 'primaryPaperLink': {'linkType': 'arxiv',\n",
              "  'url': 'https://arxiv.org/pdf/1908.10407.pdf'},\n",
              " 'pubDate': '2019-08-27',\n",
              " 'pubUpdateDate': '2020-04-15',\n",
              " 'scorecardStats': [{'citationCount': 175,\n",
              "   'keyCitationCount': 0,\n",
              "   'score': 10.0,\n",
              "   'typeKey': 'cited_by'}],\n",
              " 'socialLinks': [],\n",
              " 'sources': ['Anansi',\n",
              "  'MAG',\n",
              "  'DBLP',\n",
              "  'ScienceParseMerged',\n",
              "  'Crossref',\n",
              "  'MAG',\n",
              "  'Anansi',\n",
              "  'Unpaywall',\n",
              "  'ScienceParseMerged',\n",
              "  'ScienceParseMerged',\n",
              "  'MergedPDFExtraction',\n",
              "  'ArXiv',\n",
              "  'ScienceParseMerged'],\n",
              " 'title': 'An Energy Approach to the Solution of Partial Differential Equations in Computational Mechanics via Machine Learning: Concepts, Implementation and Applications',\n",
              " 'tldr': {'abstractSimilarityScore': 42,\n",
              "  'text': 'This contribution focuses in mechanical problems and analyze the energetic format of the PDE, where the energy of a mechanical system seems to be the natural loss function for a machine learning method to approach a mechanical problem.'},\n",
              " 'venue': 'ArXiv',\n",
              " 'videos': [],\n",
              " 'year': '2019'}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}