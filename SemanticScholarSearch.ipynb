{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SemanticScholarSearch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMPQLyYFeaUVzuQVt2M2U+z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BecomeAllan/S2Search/blob/main/SemanticScholarSearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S5xfBIUWDWZ"
      },
      "source": [
        "# Consumindo a API do SemanticScholar\n",
        "\n",
        "A seguir, tem uma classe chamada `Search()`, que ao instanciar-la em uma variável é possível fazer pesquisas sobre papers utilizando a api do SemanticScholar, dentre os parâmetros temos:\n",
        "\n",
        "- Buscar: Pesquisas sobre tópicos onde adicionar tópicos utiliza-se + (mais) e remover tópicos usamos - (menos)\n",
        "\n",
        "  ex. \"Machine+Medicine\"\n",
        "\n",
        "- Fields: O que será retornado como dados. Para utilizar, escolha dentre as opções sem utilizar espaço e separadas de virgulas:\n",
        "  - (str): externalIds\n",
        "  - (str): url\n",
        "  - (str): title\n",
        "  - (str): abstract\n",
        "  - (str): venue \n",
        "  - (str): year \n",
        "  - (str): referenceCount\n",
        "  - (str): citationCount\n",
        "  - (str): influentialCitationCount\n",
        "  - (str): isOpenAccess\n",
        "  - list (str): fieldsOfStudy\n",
        "  - list (str): authors \n",
        "\n",
        "  ex. \"title,abstract,isOpenAccess,fieldsOfStudy\"\n",
        "\n",
        "- Offset: Número que começa a puxar a partir da ordem dele a lista de papers. (0 seria o primeiro)\n",
        "\n",
        "- Limite: Número de papers a ser retornados (Máx. 10.000)\n",
        "\n",
        "**Obs:** A api do SemanticScholar disponibiliza 100 query's a cada 5 min, no qual apenas retorna no máx. 100 resutados (limite). Assim a cada 5 min, é possível puxar 10.000 papers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF-CJR6nVaZV",
        "cellView": "form"
      },
      "source": [
        "#@title Classe para pesquisa no SemanticScholar\n",
        "import IPython\n",
        "from google.colab import output\n",
        "import pandas as pd\n",
        "\n",
        "class Search():\n",
        "  def __init__(self, **kwargs):\n",
        "    self.data = \"\"\n",
        "    self.data_0 = \"\"\n",
        "\n",
        "    self.search = kwargs.get('search', None)\n",
        "    self.fields = kwargs.get('fields', None)\n",
        "    self.limit = kwargs.get('limit', None)\n",
        "    self.offset = kwargs.get('offset', None)\n",
        "\n",
        "    if self.search == None and self.fields == None and self.limit == None and self.offset == None:\n",
        "      self._start(False)\n",
        "    else:\n",
        "      self._start(True)\n",
        "  \n",
        "  def _start(self, *args):\n",
        "\n",
        "    output.register_callback('notebook.searching', self._searching)\n",
        "    output.register_callback('notebook.AddListItem', self._add_list_item)\n",
        "    output.register_callback('notebook.mergeData', self._merge_data)\n",
        "    output.register_callback('notebook.error', self._error)\n",
        "\n",
        "\n",
        "    boxs = ''' \n",
        "        <label for=\"query\">Buscar: </label>\n",
        "        <input type=\"text\" id=\"query\" value=\"Machine Learning+Deep Learning\" style=\"width: 400px;\"/>\n",
        "        <br/>\n",
        "        <br/>\n",
        "        \n",
        "        <label for=\"fields\">Fields: </label>\n",
        "        <input type=\"text\" id=\"fields\" value=\"title,abstract,isOpenAccess,fieldsOfStudy\" style=\"width: 400px;\"/>\n",
        "        <br/>\n",
        "        <br/>\n",
        " \n",
        "        <label for=\"limit\">Limite: </label>\n",
        "        <input type=\"text\" id=\"limit\" value=\"10\" style=\"width: 50px;\"/><br/>\n",
        "        <br/>\n",
        "\n",
        "        <label for=\"limit\">Offset: </label>\n",
        "        <input type=\"text\" id=\"offset\" value=\"0\" style=\"width: 50px;\"/><br/>\n",
        "        <br/>\n",
        "\n",
        "        <button id='button'>Pesquisar</button>\n",
        "        <br/>\n",
        "        <br/>\n",
        "           '''\n",
        "\n",
        "    button = ''' document.querySelector('#button').onclick = async () => ''' # {}\n",
        "\n",
        "    search_query = '''\n",
        "            var search = document.getElementById(\"query\").value\n",
        "            var fields = document.getElementById(\"fields\").value\n",
        "            var limit = parseInt(document.getElementById(\"limit\").value)\n",
        "            var offset = parseInt(document.getElementById(\"offset\").value)\n",
        "                  '''\n",
        "    search_params = '''\n",
        "            var search = \"{search}\"\n",
        "            var fields = \"{fields}\"\n",
        "            var limit = parseInt({limit})\n",
        "            var offset = parseInt({offset})\n",
        "                  '''\n",
        "    engine = '''\n",
        "            google.colab.kernel.invokeFunction('notebook.searching', [], {});\n",
        "\n",
        "            if (limit >100) {\n",
        "              var number = limit\n",
        "              var data = \"\"\n",
        "              var promises = []\n",
        "              var offsetSearch = 0\n",
        "              var rest = 0\n",
        "\n",
        "              for (let index = 0; index < Math.floor(limit/100); index++) {\n",
        "                offsetSearch = 100*(index) + offset + 1*(index!==0)\n",
        "\n",
        "\n",
        "                promises.push(\n",
        "                  fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offsetSearch}&limit=100&fields=${fields}`)\n",
        "    .then(res=> {return(res.json())})\n",
        "    .then(res=> {return(res)})\n",
        "                )\n",
        "              }\n",
        "              \n",
        "              if (limit%100 !== 0) { \n",
        "                rest= limit%100\n",
        "                offsetSearch = offsetSearch+100\n",
        "                \n",
        "                console.log(rest)\n",
        "                console.log(offsetSearch)\n",
        "\n",
        "                promises.push(\n",
        "                fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offsetSearch}&limit=${rest}&fields=${fields}`)\n",
        "    .then(res=> {return(res.json())})\n",
        "    .then(res=> {return(res)})\n",
        "                )}\n",
        "\n",
        "              await Promise.all(promises).then(data=>{\n",
        "                google.colab.kernel.invokeFunction('notebook.mergeData', [data], {})\n",
        "              })\n",
        "              .catch(err=> { return (google.colab.kernel.invokeFunction('notebook.error', [err], {})) })\n",
        "\n",
        "            } else {\n",
        "\n",
        "            await fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offset}&limit=${limit}&fields=${fields}`)\n",
        "    .then(res=> {return(res.json())})\n",
        "    .then(res=> {\n",
        "      console.log(res)\n",
        "      console.log(\"AQUIII\")\n",
        "      return(google.colab.kernel.invokeFunction('notebook.AddListItem', [res], {}))})\n",
        "    .catch(err=> { return (\n",
        "      google.colab.kernel.invokeFunction('notebook.error', [err], {})) })\n",
        "            }\n",
        "                  '''\n",
        "\n",
        "    asyncfun = \"async function asyncfun()\"\n",
        "\n",
        "    if args[0]:\n",
        "\n",
        "      main_app =  \"<script>\" + search_params.format(search=self.search, fields=self.fields, limit=self.limit, offset=self.offset) + asyncfun + \"{\" + engine + \"}\" + \"asyncfun()\" + \"</script>\"\n",
        "\n",
        "      display(IPython.display.HTML(main_app))\n",
        "      \n",
        "    else:\n",
        "      main_app = boxs + \"<script>\" + button + \"{\" + search_query + engine + \"}\" + \"</script>\"\n",
        "      \n",
        "      display(IPython.display.HTML(main_app))\n",
        "\n",
        "    \n",
        "\n",
        "  def _error(self,value):\n",
        "    try:\n",
        "      print(\"ERRO na API SemanticScholar:\\n\")\n",
        "      print(value)\n",
        "    except:\n",
        "      pass \n",
        "\n",
        "  def _searching(self):\n",
        "    with output.use_tags('some_outputs'):\n",
        "      print(\"\\n\\nPesquisando...\")\n",
        "      sys.stdout.flush();\n",
        "\n",
        "  def _merge_data(self, data):\n",
        "    output.clear(output_tags='some_outputs')\n",
        "    print(f\"Achou {data[0]['total']} papers.\\n\")\n",
        "    self.data_0 = data\n",
        "\n",
        "    self.data = pd.DataFrame(data[0]['data'])\n",
        "\n",
        "    try:\n",
        "      for x in data[1:len(data)]:\n",
        "        try:\n",
        "          self.merge(pd.DataFrame(x['data']))\n",
        "        except:\n",
        "          self._error(x)\n",
        "    except:\n",
        "      pass \n",
        "\n",
        "    print(f\"\\nApi devolveu >> {self.data.shape[0]} papers\\n\" )\n",
        "    print(self.data.head())\n",
        "\n",
        "\n",
        "  def merge(self, data):\n",
        "    self.data = pd.concat([self.data, data], ignore_index=True ) \n",
        "\n",
        "  def _add_list_item(self, value):\n",
        "    output.clear(output_tags='some_outputs')\n",
        "\n",
        "    print(f\"Achou {value['total']} papers.\\n\")\n",
        "\n",
        "    self.data = pd.DataFrame(value['data'])\n",
        "\n",
        "    print(f\"Api devolveu >> {self.data.shape[0]} papers\\n\" )\n",
        "    \n",
        "    print(self.data.head())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At8ZsoI1ZfbI"
      },
      "source": [
        "# Consumir a classe `Search()`\n",
        "\n",
        "A duas formas de pesquisar utilizando `Search()`:\n",
        "\n",
        "1. A primeira é utilizando parâmetros na propria classe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2dKxHXrkHvf"
      },
      "source": [
        "# \"Decision making\" AND \"optimization\" AND \"artificial intelligence\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCrYFOThaOR_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        },
        "outputId": "237cc7a0-1543-4122-b68d-8c5d176a7725"
      },
      "source": [
        "Resultados = Search(search = \"Machine Learning+Deep Learning\" , fields = \"title,abstract,citationCount,isOpenAccess,fieldsOfStudy\", limit = \"2000\", offset = \"0\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<script>\n",
              "            var search = \"Machine Learning\"\n",
              "            var fields = \"title,abstract,citationCount,isOpenAccess,fieldsOfStudy\"\n",
              "            var limit = parseInt(2000)\n",
              "            var offset = parseInt(0)\n",
              "                  async function asyncfun(){\n",
              "            google.colab.kernel.invokeFunction('notebook.searching', [], {});\n",
              "\n",
              "            if (limit >100) {\n",
              "              var number = limit\n",
              "              var data = \"\"\n",
              "              var promises = []\n",
              "              var offsetSearch = 0\n",
              "              var rest = 0\n",
              "\n",
              "              for (let index = 0; index < Math.floor(limit/100); index++) {\n",
              "                offsetSearch = 100*(index) + offset + 1*(index!==0)\n",
              "\n",
              "\n",
              "                promises.push(\n",
              "                  fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offsetSearch}&limit=100&fields=${fields}`)\n",
              "    .then(res=> {return(res.json())})\n",
              "    .then(res=> {return(res)})\n",
              "                )\n",
              "              }\n",
              "              \n",
              "              if (limit%100 !== 0) { \n",
              "                rest= limit%100\n",
              "                offsetSearch = offsetSearch+100\n",
              "                \n",
              "                console.log(rest)\n",
              "                console.log(offsetSearch)\n",
              "\n",
              "                promises.push(\n",
              "                fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offsetSearch}&limit=${rest}&fields=${fields}`)\n",
              "    .then(res=> {return(res.json())})\n",
              "    .then(res=> {return(res)})\n",
              "                )}\n",
              "\n",
              "              await Promise.all(promises).then(data=>{\n",
              "                google.colab.kernel.invokeFunction('notebook.mergeData', [data], {})\n",
              "              })\n",
              "              .catch(err=> { return (google.colab.kernel.invokeFunction('notebook.error', [err], {})) })\n",
              "\n",
              "            } else {\n",
              "\n",
              "            await fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offset}&limit=${limit}&fields=${fields}`)\n",
              "    .then(res=> {return(res.json())})\n",
              "    .then(res=> {\n",
              "      console.log(res)\n",
              "      console.log(\"AQUIII\")\n",
              "      return(google.colab.kernel.invokeFunction('notebook.AddListItem', [res], {}))})\n",
              "    .catch(err=> { return (\n",
              "      google.colab.kernel.invokeFunction('notebook.error', [err], {})) })\n",
              "            }\n",
              "                  }asyncfun()</script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Achou 5240039 papers.\n",
            "\n",
            "ERRO na API SemanticScholar:\n",
            "\n",
            "{'message': 'Internal Server Error'}\n",
            "ERRO na API SemanticScholar:\n",
            "\n",
            "{'message': 'Internal Server Error'}\n",
            "ERRO na API SemanticScholar:\n",
            "\n",
            "{'message': 'Internal Server Error'}\n",
            "ERRO na API SemanticScholar:\n",
            "\n",
            "{'message': 'Internal Server Error'}\n",
            "ERRO na API SemanticScholar:\n",
            "\n",
            "{'message': 'Internal Server Error'}\n",
            "ERRO na API SemanticScholar:\n",
            "\n",
            "{'message': 'Internal Server Error'}\n",
            "ERRO na API SemanticScholar:\n",
            "\n",
            "{'message': 'Internal Server Error'}\n",
            "ERRO na API SemanticScholar:\n",
            "\n",
            "{'message': 'Internal Server Error'}\n",
            "ERRO na API SemanticScholar:\n",
            "\n",
            "{'message': 'Internal Server Error'}\n",
            "\n",
            "Api devolveu >> 1009 papers\n",
            "\n",
            "                                    paperId  ...                    fieldsOfStudy\n",
            "0  46200b99c40e8586c8a0f588488ab6414119fb28  ...               [Computer Science]\n",
            "1  9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d  ...               [Computer Science]\n",
            "2  b42b1bfdc262bf99e9484e2e9df94df216b96374  ...               [Computer Science]\n",
            "3  25badc676197a70aaf9911865eb03469e402ba57  ...               [Computer Science]\n",
            "4  f9c602cc436a9ea2f9e7db48c77d924e09ce3c32  ...  [Computer Science, Mathematics]\n",
            "\n",
            "[5 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zotFl5-ficaO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "fa1bc71f-f243-4a1f-a967-172e638aa4df"
      },
      "source": [
        "# Os dados ficam na variável data, no qual é uma tabela do tipo pandas\n",
        "print(Resultados.data.columns)\n",
        "print(Resultados.data.sort_values(\"citationCount\", ascending = False ).head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paperId</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>citationCount</th>\n",
              "      <th>isOpenAccess</th>\n",
              "      <th>fieldsOfStudy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>754</th>\n",
              "      <td>34f25a8704614163c4095b3ee2fc969b60de4698</td>\n",
              "      <td>Dropout: a simple way to prevent neural networ...</td>\n",
              "      <td>Deep neural nets with a large number of parame...</td>\n",
              "      <td>24510</td>\n",
              "      <td>False</td>\n",
              "      <td>[Computer Science]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>a4cec122a08216fe8a3bc19b22e78fbaea096256</td>\n",
              "      <td>Deep Learning</td>\n",
              "      <td>Machine-learning technology powers many aspect...</td>\n",
              "      <td>19531</td>\n",
              "      <td>False</td>\n",
              "      <td>[Medicine, Computer Science]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>963</th>\n",
              "      <td>5d90f06bb70a0a3dced62413346235c02b1aa086</td>\n",
              "      <td>Learning Multiple Layers of Features from Tiny...</td>\n",
              "      <td>Groups at MIT and NYU have collected a dataset...</td>\n",
              "      <td>13232</td>\n",
              "      <td>False</td>\n",
              "      <td>[Computer Science]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>988</th>\n",
              "      <td>6bdb186ec4726e00a8051119636d4df3b94043b5</td>\n",
              "      <td>Caffe: Convolutional Architecture for Fast Fea...</td>\n",
              "      <td>Caffe provides multimedia scientists and pract...</td>\n",
              "      <td>13101</td>\n",
              "      <td>False</td>\n",
              "      <td>[Computer Science]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>46200b99c40e8586c8a0f588488ab6414119fb28</td>\n",
              "      <td>TensorFlow: A system for large-scale machine l...</td>\n",
              "      <td>TensorFlow is a machine learning system that o...</td>\n",
              "      <td>10598</td>\n",
              "      <td>False</td>\n",
              "      <td>[Computer Science]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      paperId  ...                 fieldsOfStudy\n",
              "754  34f25a8704614163c4095b3ee2fc969b60de4698  ...            [Computer Science]\n",
              "17   a4cec122a08216fe8a3bc19b22e78fbaea096256  ...  [Medicine, Computer Science]\n",
              "963  5d90f06bb70a0a3dced62413346235c02b1aa086  ...            [Computer Science]\n",
              "988  6bdb186ec4726e00a8051119636d4df3b94043b5  ...            [Computer Science]\n",
              "14   46200b99c40e8586c8a0f588488ab6414119fb28  ...            [Computer Science]\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwfe0ZCyZ8YD"
      },
      "source": [
        "2. A segunda é atravez da api de busca, searchBox, no qual é possivel colocar os campos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCZf9v2yzskK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "fba90541-642b-4c5d-ed98-323245132691"
      },
      "source": [
        "Resultados_2 = Search()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              " \n",
              "        <label for=\"query\">Buscar: </label>\n",
              "        <input type=\"text\" id=\"query\" value=\"Machine Learning+Deep Learning\" style=\"width: 400px;\"/>\n",
              "        <br/>\n",
              "        <br/>\n",
              "        \n",
              "        <label for=\"fields\">Fields: </label>\n",
              "        <input type=\"text\" id=\"fields\" value=\"title,abstract,isOpenAccess,fieldsOfStudy\" style=\"width: 400px;\"/>\n",
              "        <br/>\n",
              "        <br/>\n",
              " \n",
              "        <label for=\"limit\">Limite: </label>\n",
              "        <input type=\"text\" id=\"limit\" value=\"10\" style=\"width: 50px;\"/><br/>\n",
              "        <br/>\n",
              "\n",
              "        <label for=\"limit\">Offset: </label>\n",
              "        <input type=\"text\" id=\"offset\" value=\"0\" style=\"width: 50px;\"/><br/>\n",
              "        <br/>\n",
              "\n",
              "        <button id='button'>Pesquisar</button>\n",
              "        <br/>\n",
              "        <br/>\n",
              "           <script> document.querySelector('#button').onclick = async () => {\n",
              "            var search = document.getElementById(\"query\").value\n",
              "            var fields = document.getElementById(\"fields\").value\n",
              "            var limit = parseInt(document.getElementById(\"limit\").value)\n",
              "            var offset = parseInt(document.getElementById(\"offset\").value)\n",
              "                  \n",
              "            google.colab.kernel.invokeFunction('notebook.searching', [], {});\n",
              "\n",
              "            if (limit >100) {\n",
              "              var number = limit\n",
              "              var data = \"\"\n",
              "              var promises = []\n",
              "              var offsetSearch = 0\n",
              "              var rest = 0\n",
              "\n",
              "              for (let index = 0; index < Math.floor(limit/100); index++) {\n",
              "                offsetSearch = 100*(index) + offset + 1*(index!==0)\n",
              "\n",
              "\n",
              "                promises.push(\n",
              "                  fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offsetSearch}&limit=100&fields=${fields}`)\n",
              "    .then(res=> {return(res.json())})\n",
              "    .then(res=> {return(res)})\n",
              "                )\n",
              "              }\n",
              "              \n",
              "              if (limit%100 !== 0) { \n",
              "                rest= limit%100\n",
              "                offsetSearch = offsetSearch+100\n",
              "                \n",
              "                console.log(rest)\n",
              "                console.log(offsetSearch)\n",
              "\n",
              "                promises.push(\n",
              "                fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offsetSearch}&limit=${rest}&fields=${fields}`)\n",
              "    .then(res=> {return(res.json())})\n",
              "    .then(res=> {return(res)})\n",
              "                )}\n",
              "\n",
              "              await Promise.all(promises).then(data=>{\n",
              "                google.colab.kernel.invokeFunction('notebook.mergeData', [data], {})\n",
              "              })\n",
              "              .catch(err=> { return (google.colab.kernel.invokeFunction('notebook.error', [err], {})) })\n",
              "\n",
              "            } else {\n",
              "\n",
              "            await fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offset}&limit=${limit}&fields=${fields}`)\n",
              "    .then(res=> {return(res.json())})\n",
              "    .then(res=> {\n",
              "      console.log(res)\n",
              "      console.log(\"AQUIII\")\n",
              "      return(google.colab.kernel.invokeFunction('notebook.AddListItem', [res], {}))})\n",
              "    .catch(err=> { return (\n",
              "      google.colab.kernel.invokeFunction('notebook.error', [err], {})) })\n",
              "            }\n",
              "                  }</script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Achou 652200 papers.\n",
            "\n",
            "Api devolveu >> 10 papers\n",
            "\n",
            "                                    paperId  ...       fieldsOfStudy\n",
            "0  846ff7afb7670d62f88b4a8cc99d306ffb81b075  ...          [Medicine]\n",
            "1  5dc53e50148b01fe8b9536eb79fa6b1dce924174  ...          [Medicine]\n",
            "2  7cc2e148d27a7508dd23c4e35eb63cc9b3e6a58f  ...  [Computer Science]\n",
            "3  59444b096f7c8a561d540102e8b5bfb189edabc6  ...                None\n",
            "4  eee313380ccb45807ea0afa3c1df86f6b48b8867  ...  [Computer Science]\n",
            "\n",
            "[5 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8G0F7fEFz9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50abc23f-9658-4a53-ebbc-4f9c8065a050"
      },
      "source": [
        "print(Resultados_2.data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                    paperId  ...       fieldsOfStudy\n",
            "0  846ff7afb7670d62f88b4a8cc99d306ffb81b075  ...          [Medicine]\n",
            "1  5dc53e50148b01fe8b9536eb79fa6b1dce924174  ...          [Medicine]\n",
            "2  7cc2e148d27a7508dd23c4e35eb63cc9b3e6a58f  ...  [Computer Science]\n",
            "3  59444b096f7c8a561d540102e8b5bfb189edabc6  ...                None\n",
            "4  eee313380ccb45807ea0afa3c1df86f6b48b8867  ...  [Computer Science]\n",
            "5  46479bbea7749cb2db35b139206039531327053c  ...  [Computer Science]\n",
            "6  b69fe5a837277ddbea5215d6bacd3a902e9d11ce  ...          [Medicine]\n",
            "7  b0bf64ccbd651e8c7bc141d8aabaecff562e93a1  ...  [Computer Science]\n",
            "8  042ab08ec6782cf217f13175162bfd48f7350114  ...  [Computer Science]\n",
            "9  03e7832982986159400a8eeab148487ffcfabe56  ...  [Computer Science]\n",
            "\n",
            "[10 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNrq2PcNtb-f"
      },
      "source": [
        "\n",
        "# **SearchWeb()**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_OkAhJHt3tS"
      },
      "source": [
        "import requests\n",
        "import json\n",
        "import multiprocessing as mp\n",
        "import os\n",
        "# import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import ast\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "from time import sleep, time\n",
        "\n",
        "\n",
        "def timer(fun):\n",
        "  def warper(*args,**kwargs):\n",
        "    start = time()\n",
        "    d = fun(*args,**kwargs)\n",
        "    end = time()\n",
        "    print(f\"[{fun.__name__}]>> Demorou {round(end-start,2)}s\")\n",
        "    return d\n",
        "  return warper\n",
        "\n",
        "\n",
        "\n",
        "class SearchWeb():\n",
        "  def __init__(self, search=\"Machine Learning+Deep Learning\", poolCPU = 4, sleeptry=5, save = False, **kwargs):\n",
        "     \n",
        "    self.sleeptry = sleeptry\n",
        "    self.poolCPU = poolCPU\n",
        "    self.badcall = []\n",
        "    self._start = True\n",
        "\n",
        "    self.saveName = kwargs.get('Savename', \"Data\")\n",
        "    self.saveFile = save\n",
        "    self._search = search\n",
        "    self._sort = kwargs.get('sort', \"relevance\")\n",
        "    self._authors = kwargs.get('authors', [])\n",
        "    self._coAuthors = kwargs.get('coAuthors', [])\n",
        "    self._venues = kwargs.get('venues', ['PloS one', 'AAAI', 'Scientific reports', 'IEEE Access', 'ArXiv', 'Expert Syst. Appl.', 'ICML', 'Neurocomputing', 'Sensors', 'Remote. Sens.'])\n",
        "    self._yearFilter = kwargs.get('yearFilter', None) # {\"min\": 2008,\"max\": 2021}\n",
        "    self._requireViewablePdf = kwargs.get('requireViewablePdf', False)\n",
        "    self._publicationTypes = kwargs.get('publicationTypes', [\"ClinicalTrial\", \"CaseReport\", \"Editorial\",\"Study\",\"Book\",\"News\",\"Review\",\"Conference\",\"LettersAndComments\",\"JournalArticle\"])\n",
        "    self._fieldsOfStudy = kwargs.get('fieldsOfStudy', [\"biology\",\"art\",\"business\",\"computer-science\",\"chemistry\",\"economics\",\"engineering\",\"environmental-science\",\"geography\",\"geology\",\"history\",\"materials-science\",\"mathematics\",\"medicine\",\"philosophy\",\"physics\",\"political-science\",\"psychology\",\"sociology\"])\n",
        "    self._useFallbackRankerService = kwargs.get('useFallbackRankerService', False)\n",
        "    self._useFallbackSearchCluster = kwargs.get('useFallbackSearchCluster', False)\n",
        "    self._hydrateWithDdb = kwargs.get('hydrateWithDdb', True)\n",
        "    self._includeTldrs = kwargs.get('includeTldrs', True)\n",
        "    self._performTitleMatch = kwargs.get('performTitleMatch', True)\n",
        "    self._includeBadges = kwargs.get('includeBadges', True)\n",
        "    self._tldrModelVersion = kwargs.get('tldrModelVersion', 'v2.0.0')\n",
        "    self._getQuerySuggestions = kwargs.get('getQuerySuggestions', False)\n",
        "\n",
        "\n",
        "    self.post = {\n",
        "    \"page\": 1, \n",
        "    \"pageSize\": 10,\n",
        "    \"queryString\": self._search,\n",
        "    \"sort\": self._sort,\n",
        "    \"authors\": self._authors,\n",
        "    \"coAuthors\": self._coAuthors,\n",
        "    \"venues\": self._venues,\n",
        "    \"yearFilter\": self._yearFilter,\n",
        "    \"requireViewablePdf\": self._requireViewablePdf,\n",
        "    \"publicationTypes\": self._publicationTypes,\n",
        "    \"externalContentTypes\": [],\n",
        "    \"fieldsOfStudy\": self._fieldsOfStudy,\n",
        "    \"useFallbackRankerService\": self._useFallbackRankerService,\n",
        "    \"useFallbackSearchCluster\": self._useFallbackSearchCluster,\n",
        "    \"hydrateWithDdb\": self._hydrateWithDdb,\n",
        "    \"includeTldrs\": self._includeTldrs,\n",
        "    \"performTitleMatch\": self._performTitleMatch,\n",
        "    \"includeBadges\": self._includeBadges,\n",
        "    \"tldrModelVersion\": \"v2.0.0\",\n",
        "    \"getQuerySuggestions\": self._getQuerySuggestions,\n",
        "    }\n",
        "\n",
        "    print('.post >>')\n",
        "    print(self.post)\n",
        "\n",
        "  def _query(self, page):\n",
        "    url = \"https://www.semanticscholar.org/api/1/search\"\n",
        "    post = self.post.copy()\n",
        "    post[\"page\"] = page\n",
        "    return [requests.post(url, json=post), page ]\n",
        "\n",
        "  def _json(self, res):\n",
        "    return json.loads(res.text).copy()\n",
        "  \n",
        "  def _paperExtract(self, data):\n",
        "    p = {\n",
        "        \"authors\": [author[0]['name'] for author in data.get('authors',[{'name':None},None])],\n",
        "        \"id\": data.get('id',None),\n",
        "        \"socialLinks\": data.get('socialLinks',None),\n",
        "        \"title\": data.get('title',{'text':None})['text'],\n",
        "        \"paperAbstract\": data.get('paperAbstract',{'text':None})['text'],\n",
        "        \"year\": data.get('year',{'text':None})['text'],\n",
        "        \"venue\": data.get('venue',{'text':None})['text'],\n",
        "        \"citationContexts\":data.get('citationContexts',None),\n",
        "        \"citationStats\": data.get('citationStats',None),\n",
        "        \"sources\":data.get('sources',None),\n",
        "        \"externalContentStats\":data.get('externalContentStats',None),\n",
        "        \"journal\":data.get('journal',None),\n",
        "        \"presentationUrls\":data.get('presentationUrls',None),\n",
        "        \"links\": data.get('links',None),\n",
        "        \"primaryPaperLink\": data.get('primaryPaperLink',None),\n",
        "        \"alternatePaperLinks\": data.get('alternatePaperLinks',None),\n",
        "        \"entities\": [author['name'] for author in data.get('entities',[{'name':None}])],\n",
        "        \"entityRelations\": data.get('entityRelations',None),\n",
        "        \"blogs\":data.get('blogs',None),\n",
        "        \"videos\":data.get('videos',None),\n",
        "        \"githubReferences\": data.get('githubReferences',None),\n",
        "        \"scorecardStats\": data.get('scorecardStats',None),\n",
        "        \"fieldsOfStudy\":data.get('fieldsOfStudy',None),\n",
        "        \"pubDate\":data.get('pubDate',None),\n",
        "        \"pubUpdateDate\":data.get('pubUpdateDate',None),\n",
        "        \"badges\":data.get('badges',None),\n",
        "        \"tldr\":data.get('tldr',None)\n",
        "        }\n",
        "    return p\n",
        "\n",
        "    \n",
        "\n",
        "    # c['querySuggestions']\n",
        "    # c['totalPages']\n",
        "    # c['totalResults']\n",
        "  def save(self, name, data):\n",
        "    try:\n",
        "      with open(f'./{name}.json', 'w') as fp:\n",
        "          json.dump(data, fp)\n",
        "    except:\n",
        "      print(\"[Save]>> Error to save the data.\")\n",
        "  \n",
        "  def load_json(self, path):\n",
        "    try:\n",
        "      with open('path', 'r') as fp:\n",
        "          return json.dump(data, fp)\n",
        "    except:\n",
        "      print(\"[load_json]>> Error to load json file.\")\n",
        "    \n",
        "  def _startFile(self):\n",
        "    my_file = Path(f\"./{self.saveName}.json\")\n",
        "    print()\n",
        "    \n",
        "    if my_file.is_file():\n",
        "      try:\n",
        "        with open(f'./{self.saveName}.json') as f:\n",
        "          print(f\"[_startFile] >> Loading ./{self.saveName}.json\")\n",
        "          data = json.load(f)\n",
        "        with open(f'./{self.saveName}.text', 'w') as fp:\n",
        "          fp.write(\"{\\\"Results\\\": [\")\n",
        "        print(f\"[Create] >> Creating a ./{self.saveName}.text file to save data.\")\n",
        "        \n",
        "        self._save(data['Results'])\n",
        "      except:\n",
        "        print(f\"[_startFile] >> Fail to load ./{self.saveName}.json\")\n",
        "    else:\n",
        "      try:\n",
        "        with open(f'./{self.saveName}.text', 'w') as fp:\n",
        "          fp.write(\"{\\\"Results\\\": [\")\n",
        "        print(f\"[Create] >> Creating a ./{self.saveName}.text file to save data.\")\n",
        "      except:\n",
        "        print(\"[Create] >> Fail\")\n",
        "\n",
        "\n",
        "  def _save(self, check_point):\n",
        "    text = str(check_point)\n",
        "\n",
        "    text = re.sub('^\\[', '', text)\n",
        "    text = re.sub('\\]$', ',', text)\n",
        "\n",
        "    with open(f'./{self.saveName}.text', 'a') as fp:\n",
        "      fp.write(text)\n",
        "      # json.dump(self.all['Results'], fp)\n",
        "    print(f\"[Save] >> Save at current directory, ./{self.saveName}.text\")\n",
        "\n",
        "  def _endFile(self):\n",
        "    # ast.literal_eval(text)\n",
        "    try:\n",
        "      with open(f'./{self.saveName}.text', 'a') as fp:\n",
        "        fp.write(']}')\n",
        "      \n",
        "      with open(f'./{self.saveName}.text', 'r') as fp:\n",
        "        text = fp.read()\n",
        "        text_dict = ast.literal_eval(text)\n",
        "        \n",
        "      \n",
        "      os.rename(f'./{self.saveName}.text', f'./{self.saveName}.json')\n",
        "      # os.remove(f\"./{self.saveName}.text\")\n",
        "      # print(text_dict)\n",
        "\n",
        "      with open(f'./{self.saveName}.json', 'w') as fp:\n",
        "        json.dump(text_dict, fp)\n",
        "\n",
        "\n",
        "      print(f\"[Close] >> Close and save ./{self.saveName}.json file to save data.\")\n",
        "\n",
        "    except:\n",
        "      print('[Close] >> Fail')\n",
        "\n",
        "  \n",
        "\n",
        "  @timer\n",
        "  def _extract(self, pool, data):\n",
        "    try:\n",
        "      self.papers_text = pool.map(self._json, data['Response'])\n",
        "      check_point= [{\"Page\": {\"N_Page\":page['query']['page'],\n",
        "                                   \"N_Papers\":len(page['results']),\n",
        "                                   \"Papers\": pool.map(self._paperExtract,\n",
        "                                                      page['results'])}} for page in self.papers_text]\n",
        "\n",
        "\n",
        "      if self.saveFile:\n",
        "        try:\n",
        "          self._save(check_point)\n",
        "        except:\n",
        "          print(\"_save >> [Fail] to save.\")\n",
        "      else:\n",
        "        self.all[\"Results\"].extend(check_point)\n",
        "\n",
        "    except:\n",
        "      print(\"_extract>> [Fail], see .papers_text to reextract content.\")\n",
        "      self.badcall.append(self.papers_text)\n",
        "    \n",
        "    if self._start:\n",
        "      print('\\n ---')\n",
        "      print(f\"Total Results: {self.papers_text[0]['totalResults']}\")\n",
        "      print(f\"Total Pages: {self.papers_text[0]['totalPages']}\")\n",
        "      print(f\"Query Suggestions: {self.papers_text[0]['querySuggestions']}\")\n",
        "      print('--- \\n')\n",
        "      self._start = False\n",
        "\n",
        "  # def _data(self, data):\n",
        "  #   if type(self.datasource) == str:\n",
        "  #     self.datasource = data\n",
        "  #   else:\n",
        "  #     self.datasource = pd.concat([self.datasource, data])\n",
        "\n",
        "  @timer\n",
        "  def _runtime(self, pool, pages):\n",
        "\n",
        "    while True:\n",
        "      if self.saveFile:\n",
        "        self._startFile()\n",
        "      \n",
        "      res = pool.map(self._query, pages)\n",
        "      self.codes = [[x[0], x[1],x[0].status_code] for x in res]\n",
        "      resultData = pd.DataFrame(self.codes, columns=[\"Response\", \"Page\", \"Code\"])\n",
        "      resultData.set_index(\"Page\")\n",
        "      \n",
        "      if resultData.query(\"Code !=200\").size == 0:\n",
        "        # self._data(resultData)\n",
        "        self._extract(pool, resultData.query(\"Code ==200\"))\n",
        "        if self.saveFile:\n",
        "          self._endFile()\n",
        "        break\n",
        "      else:\n",
        "        print(\"Bad call of pages:\")\n",
        "        self._data(resultData.query(\"Code == 200\"))\n",
        "              # self.datasource.append(resultData.query(\"Code ==200\"))\n",
        "        pages = resultData.query(\"Code !=200\").index.values.tolist()\n",
        "        print(pages)\n",
        "        print(f\"Tentando de novo daqui a {self.sleeptry/60} min...\")\n",
        "        self._extract(pool, resultData.query(\"Code ==200\"))\n",
        "        if self.saveFile:\n",
        "          self._endFile()\n",
        "        sleep(self.sleeptry)\n",
        "        \n",
        "    \n",
        "    \n",
        "    # self._extract(pool, self.datasource)\n",
        "\n",
        "\n",
        "  @timer\n",
        "  def get(self, n = 10, page = 1):\n",
        "    self._page = page\n",
        "    self.post[\"pageSize\"] = 10\n",
        "    self.post[\"page\"] = page\n",
        "    self.all = {\"Results\": []}\n",
        "    # self.datasource = ''\n",
        "    print(\"Searching...\")\n",
        "    print(self.all)\n",
        "\n",
        "    \n",
        "\n",
        "    with mp.Pool(self.poolCPU) as pool:\n",
        "      if n > 10:\n",
        "        pages = list(range(self._page, (n//10)+self._page))\n",
        "      # for page in range(n//10):\n",
        "        self._runtime(pool,pages)\n",
        "          \n",
        "        if n%10>0:\n",
        "          pages = [n//10+self._page]\n",
        "          self.post[\"pageSize\"] = n%10\n",
        "\n",
        "          self._runtime(pool, pages)\n",
        "          \n",
        "      else:\n",
        "        # pass\n",
        "        pages = [self._page]\n",
        "        self.post[\"page\"] = self._page\n",
        "        self.post[\"pageSize\"] = n\n",
        "\n",
        "        self._runtime(pool, pages)\n",
        "\n",
        "    \n",
        "      # self._extract(pool, self.datasource)\n",
        "\n",
        "##### Description #####\n",
        "# ex. {\"params\": value} \n",
        "#  \n",
        "##### Params that can pass in SearchWeb().get(params = value): #####\n",
        "#     {\n",
        "#     \"n\": 1000 (how much papers)\n",
        "#     \"page\": 1, (where start search)\n",
        "#      }\n",
        "##### Params that can pass in SearchWeb(params = value): #####\n",
        "# data = '''{\n",
        "#     \"save\": False\n",
        "#     \"queryString\": \"Machine Learning+Deep Learning\",\n",
        "#     \"sort\": \"total-citations\", #influence #\"pub-date\" #relevance\n",
        "#     \"authors\": [],\n",
        "#     \"coAuthors\": [],\n",
        "#     \"venues\": [\n",
        "#         \"PloS one\",\n",
        "#         \"AAAI\",\n",
        "#         \"Scientific reports\",\n",
        "#         \"IEEE Access\",\n",
        "#         \"ArXiv\",\n",
        "#         \"Expert Syst. Appl.\",\n",
        "#         \"ICML\",\n",
        "#         \"Neurocomputing\",\n",
        "#         \"Sensors\",\n",
        "#         \"Remote. Sens.\"\n",
        "#     ],\n",
        "#     \"yearFilter\": {\n",
        "#         \"min\": 2008,\n",
        "#         \"max\": 2021\n",
        "#     },\n",
        "#     \"requireViewablePdf\": True,\n",
        "#     \"publicationTypes\": [\n",
        "#         \"ClinicalTrial\",\n",
        "#         \"CaseReport\",\n",
        "#         \"Editorial\",\n",
        "#         \"Study\",\n",
        "#         \"Book\",\n",
        "#         \"News\",\n",
        "#         \"Review\",\n",
        "#         \"Conference\",\n",
        "#         \"LettersAndComments\",\n",
        "#         \"JournalArticle\"\n",
        "#     ],\n",
        "#     \"externalContentTypes\": [],\n",
        "#     \"fieldsOfStudy\": [\n",
        "#         \"biology\",\n",
        "#         \"art\",\n",
        "#         \"business\",\n",
        "#         \"computer-science\",\n",
        "#         \"chemistry\",\n",
        "#         \"economics\",\n",
        "#         \"engineering\",\n",
        "#         \"environmental-science\",\n",
        "#         \"geography\",\n",
        "#         \"geology\",\n",
        "#         \"history\",\n",
        "#         \"materials-science\",\n",
        "#         \"mathematics\",\n",
        "#         \"medicine\",\n",
        "#         \"philosophy\",\n",
        "#         \"physics\",\n",
        "#         \"political-science\",\n",
        "#         \"psychology\",\n",
        "#         \"sociology\"\n",
        "#     ],\n",
        "#     \"useFallbackRankerService\": False,\n",
        "#     \"useFallbackSearchCluster\": False,\n",
        "#     \"hydrateWithDdb\": True,\n",
        "#     \"includeTldrs\": True,\n",
        "#     \"performTitleMatch\": True,\n",
        "#     \"includeBadges\": True,\n",
        "#     \"tldrModelVersion\": \"v2.0.0\",\n",
        "#     \"getQuerySuggestions\": False\n",
        "# }\n",
        "# '''\n",
        "# '''\n",
        "# Obs. Params that have a list can be a empty list\n",
        "# Ex. {\"venues\": []}\n",
        "\n",
        "### Discoment here to have a script\n",
        "# if __name__ == '__main__':\n",
        "#   SearchWeb().get()\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_j2uslvt8A2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a6a0b66-157d-4751-a28f-bf7b2689b32d"
      },
      "source": [
        "from_Webpage = SearchWeb(search= \"Machine Learning+Deep Learning\", sort= \"total-citations\", save=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".post >>\n",
            "{'page': 1, 'pageSize': 10, 'queryString': 'Machine Learning+Deep Learning', 'sort': 'total-citations', 'authors': [], 'coAuthors': [], 'venues': ['PloS one', 'AAAI', 'Scientific reports', 'IEEE Access', 'ArXiv', 'Expert Syst. Appl.', 'ICML', 'Neurocomputing', 'Sensors', 'Remote. Sens.'], 'yearFilter': None, 'requireViewablePdf': False, 'publicationTypes': ['ClinicalTrial', 'CaseReport', 'Editorial', 'Study', 'Book', 'News', 'Review', 'Conference', 'LettersAndComments', 'JournalArticle'], 'externalContentTypes': [], 'fieldsOfStudy': ['biology', 'art', 'business', 'computer-science', 'chemistry', 'economics', 'engineering', 'environmental-science', 'geography', 'geology', 'history', 'materials-science', 'mathematics', 'medicine', 'philosophy', 'physics', 'political-science', 'psychology', 'sociology'], 'useFallbackRankerService': False, 'useFallbackSearchCluster': False, 'hydrateWithDdb': True, 'includeTldrs': True, 'performTitleMatch': True, 'includeBadges': True, 'tldrModelVersion': 'v2.0.0', 'getQuerySuggestions': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzUBus_Vt9-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01fa0752-72f1-4726-ba46-40de43f88030"
      },
      "source": [
        "# Retorna 100 papers a partir da pag. 2 com base nos parametros passados em SearchWeb() que constitui ().post\n",
        "from_Webpage.get(100, page = 2)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching...\n",
            "{'Results': []}\n",
            "\n",
            "[Create] >> Creating a ./Data.text file to save data.\n",
            "[Save] >> Save at current directory, ./Data.text\n",
            "\n",
            " ---\n",
            "Total Results: 51139\n",
            "Total Pages: 5113\n",
            "Query Suggestions: []\n",
            "--- \n",
            "\n",
            "[_extract]>> Demorou 2.15s\n",
            "[Close] >> Close and save ./Data.json file to save data.\n",
            "[_runtime]>> Demorou 7.51s\n",
            "[get]>> Demorou 7.68s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_AxXsbJuARM"
      },
      "source": [
        "# Tudo que vem com base em 1 paper (dict.)\n",
        "from_Webpage.all[\"Results\"][0][\"Papers\"][0]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}