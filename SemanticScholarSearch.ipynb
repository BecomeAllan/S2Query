{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "SemanticScholarSearch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMPQLyYFeaUVzuQVt2M2U+z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/BecomeAllan/S2Search/blob/main/SemanticScholarSearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Consumindo a API do SemanticScholar\n",
        "\n",
        "A seguir, tem uma classe chamada `Search()`, que ao instanciar-la em uma variável é possível fazer pesquisas sobre papers utilizando a api do SemanticScholar, dentre os parâmetros temos:\n",
        "\n",
        "- Buscar: Pesquisas sobre tópicos onde adicionar tópicos utiliza-se + (mais) e remover tópicos usamos - (menos)\n",
        "\n",
        "  ex. \"Machine+Medicine\"\n",
        "\n",
        "- Fields: O que será retornado como dados. Para utilizar, escolha dentre as opções sem utilizar espaço e separadas de virgulas:\n",
        "  - (str): externalIds\n",
        "  - (str): url\n",
        "  - (str): title\n",
        "  - (str): abstract\n",
        "  - (str): venue \n",
        "  - (str): year \n",
        "  - (str): referenceCount\n",
        "  - (str): citationCount\n",
        "  - (str): influentialCitationCount\n",
        "  - (str): isOpenAccess\n",
        "  - list (str): fieldsOfStudy\n",
        "  - list (str): authors \n",
        "\n",
        "  ex. \"title,abstract,isOpenAccess,fieldsOfStudy\"\n",
        "\n",
        "- Offset: Número que começa a puxar a partir da ordem dele a lista de papers. (0 seria o primeiro)\n",
        "\n",
        "- Limite: Número de papers a ser retornados (Máx. 10.000)\n",
        "\n",
        "**Obs:** A api do SemanticScholar disponibiliza 100 query's a cada 5 min, no qual apenas retorna no máx. 100 resutados (limite). Assim a cada 5 min, é possível puxar 10.000 papers."
      ],
      "metadata": {
        "id": "5S5xfBIUWDWZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import requests\r\n",
        "import json\r\n",
        "import multiprocessing as mp\r\n",
        "import os\r\n",
        "# import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import re\r\n",
        "import ast\r\n",
        "from pathlib import Path\r\n",
        "\r\n",
        "\r\n",
        "from time import sleep, time\r\n",
        "\r\n",
        "\r\n",
        "def timer(fun):\r\n",
        "  def warper(*args,**kwargs):\r\n",
        "    start = time()\r\n",
        "    d = fun(*args,**kwargs)\r\n",
        "    end = time()\r\n",
        "    print(f\"[{fun.__name__}]>> Demorou {round(end-start,2)}s\")\r\n",
        "    return d\r\n",
        "  return warper\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class SearchAPI():\r\n",
        "  def __init__(self, search=\"decision making+optimization+artificial intelligence\", poolCPU = 4, sleeptry=5, save = False, **kwargs):\r\n",
        "     \r\n",
        "    self.total = 0\r\n",
        "    self.sleeptry = sleeptry\r\n",
        "    self.poolCPU = poolCPU\r\n",
        "    self.saveName = kwargs.get('Savename', \"Data\")\r\n",
        "\r\n",
        "    self.badcall = []\r\n",
        "\r\n",
        "    self.saveFile = save\r\n",
        "\r\n",
        "    self._api = \"https://api.semanticscholar.org/graph/v1/paper/search\"\r\n",
        "\r\n",
        "    self.params = {\r\n",
        "    \"query\": search, \r\n",
        "    \"limit\": 100,\r\n",
        "    \"fields\": kwargs.get('fields', \"title,abstract,isOpenAccess,fieldsOfStudy\"),\r\n",
        "    \"offset\": kwargs.get('offset', 0),\r\n",
        "    }\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  def _query(self, offset):\r\n",
        "    # print(\"_query\")\r\n",
        "    params = self.params.copy()\r\n",
        "    if offset + params['limit'] > 10000:\r\n",
        "      params['limit'] = 10000 - offset \r\n",
        "    else:\r\n",
        "      params['offset'] = offset\r\n",
        "    # print(params)\r\n",
        "    # post[\"page\"] = page\r\n",
        "    try:\r\n",
        "      res = requests.get(self._api, params, timeout=15)\r\n",
        "      # sleep(self.sleeptry)\r\n",
        "      print(res)\r\n",
        "      res.encoding = 'utf-8'\r\n",
        "      return [res, offset, res.status_code]\r\n",
        "    except:\r\n",
        "      return [None, offset, 400 ]\r\n",
        "\r\n",
        "  def _pandas(self, res):\r\n",
        "    dict_data = json.loads(res.text)\r\n",
        "    # print(len(dict_data['data']))\r\n",
        "\r\n",
        "    self.total= self.total+ len(dict_data['data'])\r\n",
        "    return pd.DataFrame(dict_data['data'])\r\n",
        "\r\n",
        "  def save(self, name, data):\r\n",
        "    # csvFile = Path(f\"./{name}.csv\")\r\n",
        "    \r\n",
        "    # if csvFile.is_file():\r\n",
        "    #   x = pd.read_csv(f'{name}.csv')\r\n",
        "    #   data = pd.concat([data,x])\r\n",
        "\r\n",
        "    try:\r\n",
        "      data.to_csv(f'{name}.csv')\r\n",
        "    except:\r\n",
        "      print(\"[Save]>> Error to save the data.\")\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  @timer\r\n",
        "  def _extract(self, pool, data):\r\n",
        "    try:\r\n",
        "      papers_list = pool.map(self._pandas, data['Response'].tolist())\r\n",
        "\r\n",
        "      self.all.extend(papers_list)\r\n",
        "\r\n",
        "      if self.saveFile:\r\n",
        "        self.save(self.saveName ,pd.concat(self.all).reset_index())\r\n",
        "\r\n",
        "    except:\r\n",
        "      print(\"_extract>> [Fail], see .badcall to reextract content.\")\r\n",
        "      # self.badcall.append(self.papers_text)\r\n",
        "      # print(self.badcall)\r\n",
        "   \r\n",
        "      \r\n",
        "\r\n",
        "\r\n",
        "  @timer\r\n",
        "  def _runtime(self, pool, offsets):\r\n",
        "    # self.totalPages = 0\r\n",
        "\r\n",
        "    # find = False\r\n",
        "    \r\n",
        "    while True:\r\n",
        "      # if self.saveFile:\r\n",
        "      #   close = self._startFile(find)\r\n",
        "      \r\n",
        "      print('\\n')\r\n",
        "      print('[_runtime]>> Start searching...')\r\n",
        "\r\n",
        "      try:\r\n",
        "        res = pool.map(self._query, offsets)\r\n",
        "        resultData = pd.DataFrame(res, columns=[\"Response\", \"Page\", \"Code\"])\r\n",
        "        # print(resultData[\"Code\"])\r\n",
        "\r\n",
        "        resultData.set_index(\"Page\")\r\n",
        "        \r\n",
        "        if resultData.query(\"Code !=200\").size == 0:\r\n",
        "          self._extract(pool, resultData.query(\"Code ==200\"))\r\n",
        "\r\n",
        "          break\r\n",
        "        else:\r\n",
        "\r\n",
        "          if resultData.query(\"Code ==200\").size != 0:\r\n",
        "            self._extract(pool, resultData.query(\"Code ==200\"))\r\n",
        "              \r\n",
        "          print(\"Bad call of pages:\")\r\n",
        "          # self._data(resultData.query(\"Code == 200\"))\r\n",
        "          # self.datasource.append(resultData.query(\"Code ==200\"))\r\n",
        "          offsets = resultData.query(\"Code !=200\")[\"Page\"].values.tolist()\r\n",
        "          err = resultData.query(\"Code !=200\")[\"Response\"].tolist()\r\n",
        "          err = [x.text for x in err]\r\n",
        "          print(err)\r\n",
        "          try:\r\n",
        "            with open(\"./BadCalls.text\", 'w', encoding='UTF-8') as fp:\r\n",
        "              fp.write(str(offsets))\r\n",
        "          except:\r\n",
        "            print(\"Fail to save badcalls\")\r\n",
        "          print(f\"Tentando de novo daqui a {self.sleeptry/60} min...\")\r\n",
        "\r\n",
        "\r\n",
        "          sleep(self.sleeptry)\r\n",
        "      except:\r\n",
        "        pass\r\n",
        "      print(\"---\")\r\n",
        "\r\n",
        "        \r\n",
        "        \r\n",
        "    \r\n",
        "    \r\n",
        "    # self._extract(pool, self.datasource)\r\n",
        "\r\n",
        "\r\n",
        "  @timer\r\n",
        "  def get(self, n = 10, offset = 0, papers = []):\r\n",
        "    self.n = n\r\n",
        "    self._offset = offset\r\n",
        "    # self.post[\"pageSize\"] = 10\r\n",
        "    # self.post[\"page\"] = page\r\n",
        "    self.all = []\r\n",
        "    # print('.post >>')\r\n",
        "    # print(self.post)\r\n",
        "    # self.datasource = ''\r\n",
        "    print(\"\\n\")\r\n",
        "    print(\"Searching...\")\r\n",
        "\r\n",
        "\r\n",
        "    \r\n",
        "\r\n",
        "    with mp.Pool(self.poolCPU) as pool:\r\n",
        "      if self.n > 100:\r\n",
        "    \r\n",
        "        if len(papers) != 0:\r\n",
        "          self._offsets = papers\r\n",
        "        else:\r\n",
        "          self._offsets = list(range(self._offset, (self.n//100)+self._offset))\r\n",
        "          lista = [x*100 + 1 for x in self._offsets[1:]]\r\n",
        "          lista.insert(0,self._offsets[0])\r\n",
        "          offsets = lista\r\n",
        "          \r\n",
        "          print(\"offsets: \")\r\n",
        "          print(offsets)\r\n",
        "        \r\n",
        "        # print(self._offsets)\r\n",
        "      # for page in range(n//10):\r\n",
        "        self._runtime(pool, offsets)\r\n",
        "          \r\n",
        "        if n%100>0:\r\n",
        "          self.params['limit'] = n%100\r\n",
        "          self._offsets = [lista[-1] + 100]\r\n",
        "          # print(self.params)\r\n",
        "          # print(self._offsets)\r\n",
        "\r\n",
        "          self._runtime(pool, self._offsets)\r\n",
        "          \r\n",
        "      else:\r\n",
        "        # pass\r\n",
        "        if len(papers) != 0:\r\n",
        "          self._offsets = papers\r\n",
        "        else:  \r\n",
        "          offsets = [self._offset]\r\n",
        "        \r\n",
        "        self.params['limit'] = n\r\n",
        "        self._runtime(pool, offsets)\r\n",
        "\r\n",
        "    self.all = pd.concat(self.all, ignore_index=True)\r\n",
        "\r\n",
        "\r\n",
        "# if __name__ == '__main__':\r\n",
        "#   SearchAPI(sleeptry = 1*20, save=True, Savename = \"dataAPI\").get(10000)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "res_api = SearchAPI(sleeptry = 1*20, save=False)\r\n",
        "\r\n",
        "res_api.get(250)\r\n",
        "\r\n",
        "res_api.all.head()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Classe para pesquisa no SemanticScholar\r\n",
        "import IPython\r\n",
        "from google.colab import output\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "class Search():\r\n",
        "  def __init__(self, **kwargs):\r\n",
        "    self.data = \"\"\r\n",
        "    self.data_0 = \"\"\r\n",
        "\r\n",
        "    self.search = kwargs.get('search', None)\r\n",
        "    self.fields = kwargs.get('fields', None)\r\n",
        "    self.limit = kwargs.get('limit', None)\r\n",
        "    self.offset = kwargs.get('offset', None)\r\n",
        "\r\n",
        "    if self.search == None and self.fields == None and self.limit == None and self.offset == None:\r\n",
        "      self._start(False)\r\n",
        "    else:\r\n",
        "      self._start(True)\r\n",
        "  \r\n",
        "  def _start(self, *args):\r\n",
        "\r\n",
        "    output.register_callback('notebook.searching', self._searching)\r\n",
        "    output.register_callback('notebook.AddListItem', self._add_list_item)\r\n",
        "    output.register_callback('notebook.mergeData', self._merge_data)\r\n",
        "    output.register_callback('notebook.error', self._error)\r\n",
        "\r\n",
        "\r\n",
        "    boxs = ''' \r\n",
        "        <label for=\"query\">Buscar: </label>\r\n",
        "        <input type=\"text\" id=\"query\" value=\"Machine Learning+Deep Learning\" style=\"width: 400px;\"/>\r\n",
        "        <br/>\r\n",
        "        <br/>\r\n",
        "        \r\n",
        "        <label for=\"fields\">Fields: </label>\r\n",
        "        <input type=\"text\" id=\"fields\" value=\"title,abstract,isOpenAccess,fieldsOfStudy\" style=\"width: 400px;\"/>\r\n",
        "        <br/>\r\n",
        "        <br/>\r\n",
        " \r\n",
        "        <label for=\"limit\">Limite: </label>\r\n",
        "        <input type=\"text\" id=\"limit\" value=\"10\" style=\"width: 50px;\"/><br/>\r\n",
        "        <br/>\r\n",
        "\r\n",
        "        <label for=\"limit\">Offset: </label>\r\n",
        "        <input type=\"text\" id=\"offset\" value=\"0\" style=\"width: 50px;\"/><br/>\r\n",
        "        <br/>\r\n",
        "\r\n",
        "        <button id='button'>Pesquisar</button>\r\n",
        "        <br/>\r\n",
        "        <br/>\r\n",
        "           '''\r\n",
        "\r\n",
        "    button = ''' document.querySelector('#button').onclick = async () => ''' # {}\r\n",
        "\r\n",
        "    search_query = '''\r\n",
        "            var search = document.getElementById(\"query\").value\r\n",
        "            var fields = document.getElementById(\"fields\").value\r\n",
        "            var limit = parseInt(document.getElementById(\"limit\").value)\r\n",
        "            var offset = parseInt(document.getElementById(\"offset\").value)\r\n",
        "                  '''\r\n",
        "    search_params = '''\r\n",
        "            var search = \"{search}\"\r\n",
        "            var fields = \"{fields}\"\r\n",
        "            var limit = parseInt({limit})\r\n",
        "            var offset = parseInt({offset})\r\n",
        "                  '''\r\n",
        "    engine = '''\r\n",
        "            google.colab.kernel.invokeFunction('notebook.searching', [], {});\r\n",
        "\r\n",
        "            if (limit >100) {\r\n",
        "              var number = limit\r\n",
        "              var data = \"\"\r\n",
        "              var promises = []\r\n",
        "              var offsetSearch = 0\r\n",
        "              var rest = 0\r\n",
        "\r\n",
        "              for (let index = 0; index < Math.floor(limit/100); index++) {\r\n",
        "                offsetSearch = 100*(index) + offset + 1*(index!==0)\r\n",
        "\r\n",
        "\r\n",
        "                promises.push(\r\n",
        "                  fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offsetSearch}&limit=100&fields=${fields}`)\r\n",
        "    .then(res=> {return(res.json())})\r\n",
        "    .then(res=> {return(res)})\r\n",
        "                )\r\n",
        "              }\r\n",
        "              \r\n",
        "              if (limit%100 !== 0) { \r\n",
        "                rest= limit%100\r\n",
        "                offsetSearch = offsetSearch+100\r\n",
        "                \r\n",
        "                console.log(rest)\r\n",
        "                console.log(offsetSearch)\r\n",
        "\r\n",
        "                promises.push(\r\n",
        "                fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offsetSearch}&limit=${rest}&fields=${fields}`)\r\n",
        "    .then(res=> {return(res.json())})\r\n",
        "    .then(res=> {return(res)})\r\n",
        "                )}\r\n",
        "\r\n",
        "              await Promise.all(promises).then(data=>{\r\n",
        "                google.colab.kernel.invokeFunction('notebook.mergeData', [data], {})\r\n",
        "              })\r\n",
        "              .catch(err=> { return (google.colab.kernel.invokeFunction('notebook.error', [err], {})) })\r\n",
        "\r\n",
        "            } else {\r\n",
        "\r\n",
        "            await fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offset}&limit=${limit}&fields=${fields}`)\r\n",
        "    .then(res=> {return(res.json())})\r\n",
        "    .then(res=> {\r\n",
        "      console.log(res)\r\n",
        "      console.log(\"AQUIII\")\r\n",
        "      return(google.colab.kernel.invokeFunction('notebook.AddListItem', [res], {}))})\r\n",
        "    .catch(err=> { return (\r\n",
        "      google.colab.kernel.invokeFunction('notebook.error', [err], {})) })\r\n",
        "            }\r\n",
        "                  '''\r\n",
        "\r\n",
        "    asyncfun = \"async function asyncfun()\"\r\n",
        "\r\n",
        "    if args[0]:\r\n",
        "\r\n",
        "      main_app =  \"<script>\" + search_params.format(search=self.search, fields=self.fields, limit=self.limit, offset=self.offset) + asyncfun + \"{\" + engine + \"}\" + \"asyncfun()\" + \"</script>\"\r\n",
        "\r\n",
        "      display(IPython.display.HTML(main_app))\r\n",
        "      \r\n",
        "    else:\r\n",
        "      main_app = boxs + \"<script>\" + button + \"{\" + search_query + engine + \"}\" + \"</script>\"\r\n",
        "      \r\n",
        "      display(IPython.display.HTML(main_app))\r\n",
        "\r\n",
        "    \r\n",
        "\r\n",
        "  def _error(self,value):\r\n",
        "    try:\r\n",
        "      print(\"ERRO na API SemanticScholar:\\n\")\r\n",
        "      print(value)\r\n",
        "    except:\r\n",
        "      pass \r\n",
        "\r\n",
        "  def _searching(self):\r\n",
        "    with output.use_tags('some_outputs'):\r\n",
        "      print(\"\\n\\nPesquisando...\")\r\n",
        "      sys.stdout.flush();\r\n",
        "\r\n",
        "  def _merge_data(self, data):\r\n",
        "    output.clear(output_tags='some_outputs')\r\n",
        "    print(f\"Achou {data[0]['total']} papers.\\n\")\r\n",
        "    self.data_0 = data\r\n",
        "\r\n",
        "    self.data = pd.DataFrame(data[0]['data'])\r\n",
        "\r\n",
        "    try:\r\n",
        "      for x in data[1:len(data)]:\r\n",
        "        try:\r\n",
        "          self.merge(pd.DataFrame(x['data']))\r\n",
        "        except:\r\n",
        "          self._error(x)\r\n",
        "    except:\r\n",
        "      pass \r\n",
        "\r\n",
        "    print(f\"\\nApi devolveu >> {self.data.shape[0]} papers\\n\" )\r\n",
        "    print(self.data.head())\r\n",
        "\r\n",
        "\r\n",
        "  def merge(self, data):\r\n",
        "    self.data = pd.concat([self.data, data], ignore_index=True ) \r\n",
        "\r\n",
        "  def _add_list_item(self, value):\r\n",
        "    output.clear(output_tags='some_outputs')\r\n",
        "\r\n",
        "    print(f\"Achou {value['total']} papers.\\n\")\r\n",
        "\r\n",
        "    self.data = pd.DataFrame(value['data'])\r\n",
        "\r\n",
        "    print(f\"Api devolveu >> {self.data.shape[0]} papers\\n\" )\r\n",
        "    \r\n",
        "    print(self.data.head())\r\n",
        "\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "TF-CJR6nVaZV",
        "cellView": "form"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Consumir a classe `Search()`\n",
        "\n",
        "A duas formas de pesquisar utilizando `Search()`:\n",
        "\n",
        "1. A primeira é utilizando parâmetros na propria classe:"
      ],
      "metadata": {
        "id": "At8ZsoI1ZfbI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# \"Decision making\" AND \"optimization\" AND \"artificial intelligence\""
      ],
      "outputs": [],
      "metadata": {
        "id": "h2dKxHXrkHvf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "Resultados = Search(search = \"Machine Learning+Deep Learning\" , fields = \"title,abstract,citationCount,isOpenAccess,fieldsOfStudy\", limit = \"2000\", offset = \"0\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<script>\n",
              "            var search = \"Machine Learning\"\n",
              "            var fields = \"title,abstract,citationCount,isOpenAccess,fieldsOfStudy\"\n",
              "            var limit = parseInt(2000)\n",
              "            var offset = parseInt(0)\n",
              "                  async function asyncfun(){\n",
              "            google.colab.kernel.invokeFunction('notebook.searching', [], {});\n",
              "\n",
              "            if (limit >100) {\n",
              "              var number = limit\n",
              "              var data = \"\"\n",
              "              var promises = []\n",
              "              var offsetSearch = 0\n",
              "              var rest = 0\n",
              "\n",
              "              for (let index = 0; index < Math.floor(limit/100); index++) {\n",
              "                offsetSearch = 100*(index) + offset + 1*(index!==0)\n",
              "\n",
              "\n",
              "                promises.push(\n",
              "                  fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offsetSearch}&limit=100&fields=${fields}`)\n",
              "    .then(res=> {return(res.json())})\n",
              "    .then(res=> {return(res)})\n",
              "                )\n",
              "              }\n",
              "              \n",
              "              if (limit%100 !== 0) { \n",
              "                rest= limit%100\n",
              "                offsetSearch = offsetSearch+100\n",
              "                \n",
              "                console.log(rest)\n",
              "                console.log(offsetSearch)\n",
              "\n",
              "                promises.push(\n",
              "                fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offsetSearch}&limit=${rest}&fields=${fields}`)\n",
              "    .then(res=> {return(res.json())})\n",
              "    .then(res=> {return(res)})\n",
              "                )}\n",
              "\n",
              "              await Promise.all(promises).then(data=>{\n",
              "                google.colab.kernel.invokeFunction('notebook.mergeData', [data], {})\n",
              "              })\n",
              "              .catch(err=> { return (google.colab.kernel.invokeFunction('notebook.error', [err], {})) })\n",
              "\n",
              "            } else {\n",
              "\n",
              "            await fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offset}&limit=${limit}&fields=${fields}`)\n",
              "    .then(res=> {return(res.json())})\n",
              "    .then(res=> {\n",
              "      console.log(res)\n",
              "      console.log(\"AQUIII\")\n",
              "      return(google.colab.kernel.invokeFunction('notebook.AddListItem', [res], {}))})\n",
              "    .catch(err=> { return (\n",
              "      google.colab.kernel.invokeFunction('notebook.error', [err], {})) })\n",
              "            }\n",
              "                  }asyncfun()</script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Achou 5240039 papers.\n",
            "\n",
            "ERRO na API SemanticScholar:\n",
            "\n",
            "{'message': 'Internal Server Error'}\n",
            "ERRO na API SemanticScholar:\n",
            "\n",
            "{'message': 'Internal Server Error'}\n",
            "ERRO na API SemanticScholar:\n",
            "\n",
            "{'message': 'Internal Server Error'}\n",
            "ERRO na API SemanticScholar:\n",
            "\n",
            "{'message': 'Internal Server Error'}\n",
            "ERRO na API SemanticScholar:\n",
            "\n",
            "{'message': 'Internal Server Error'}\n",
            "ERRO na API SemanticScholar:\n",
            "\n",
            "{'message': 'Internal Server Error'}\n",
            "ERRO na API SemanticScholar:\n",
            "\n",
            "{'message': 'Internal Server Error'}\n",
            "ERRO na API SemanticScholar:\n",
            "\n",
            "{'message': 'Internal Server Error'}\n",
            "ERRO na API SemanticScholar:\n",
            "\n",
            "{'message': 'Internal Server Error'}\n",
            "\n",
            "Api devolveu >> 1009 papers\n",
            "\n",
            "                                    paperId  ...                    fieldsOfStudy\n",
            "0  46200b99c40e8586c8a0f588488ab6414119fb28  ...               [Computer Science]\n",
            "1  9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d  ...               [Computer Science]\n",
            "2  b42b1bfdc262bf99e9484e2e9df94df216b96374  ...               [Computer Science]\n",
            "3  25badc676197a70aaf9911865eb03469e402ba57  ...               [Computer Science]\n",
            "4  f9c602cc436a9ea2f9e7db48c77d924e09ce3c32  ...  [Computer Science, Mathematics]\n",
            "\n",
            "[5 rows x 6 columns]\n"
          ]
        }
      ],
      "metadata": {
        "id": "MCrYFOThaOR_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        },
        "outputId": "237cc7a0-1543-4122-b68d-8c5d176a7725"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Os dados ficam na variável data, no qual é uma tabela do tipo pandas\r\n",
        "print(Resultados.data.columns)\r\n",
        "print(Resultados.data.sort_values(\"citationCount\", ascending = False ).head())"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paperId</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>citationCount</th>\n",
              "      <th>isOpenAccess</th>\n",
              "      <th>fieldsOfStudy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>754</th>\n",
              "      <td>34f25a8704614163c4095b3ee2fc969b60de4698</td>\n",
              "      <td>Dropout: a simple way to prevent neural networ...</td>\n",
              "      <td>Deep neural nets with a large number of parame...</td>\n",
              "      <td>24510</td>\n",
              "      <td>False</td>\n",
              "      <td>[Computer Science]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>a4cec122a08216fe8a3bc19b22e78fbaea096256</td>\n",
              "      <td>Deep Learning</td>\n",
              "      <td>Machine-learning technology powers many aspect...</td>\n",
              "      <td>19531</td>\n",
              "      <td>False</td>\n",
              "      <td>[Medicine, Computer Science]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>963</th>\n",
              "      <td>5d90f06bb70a0a3dced62413346235c02b1aa086</td>\n",
              "      <td>Learning Multiple Layers of Features from Tiny...</td>\n",
              "      <td>Groups at MIT and NYU have collected a dataset...</td>\n",
              "      <td>13232</td>\n",
              "      <td>False</td>\n",
              "      <td>[Computer Science]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>988</th>\n",
              "      <td>6bdb186ec4726e00a8051119636d4df3b94043b5</td>\n",
              "      <td>Caffe: Convolutional Architecture for Fast Fea...</td>\n",
              "      <td>Caffe provides multimedia scientists and pract...</td>\n",
              "      <td>13101</td>\n",
              "      <td>False</td>\n",
              "      <td>[Computer Science]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>46200b99c40e8586c8a0f588488ab6414119fb28</td>\n",
              "      <td>TensorFlow: A system for large-scale machine l...</td>\n",
              "      <td>TensorFlow is a machine learning system that o...</td>\n",
              "      <td>10598</td>\n",
              "      <td>False</td>\n",
              "      <td>[Computer Science]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      paperId  ...                 fieldsOfStudy\n",
              "754  34f25a8704614163c4095b3ee2fc969b60de4698  ...            [Computer Science]\n",
              "17   a4cec122a08216fe8a3bc19b22e78fbaea096256  ...  [Medicine, Computer Science]\n",
              "963  5d90f06bb70a0a3dced62413346235c02b1aa086  ...            [Computer Science]\n",
              "988  6bdb186ec4726e00a8051119636d4df3b94043b5  ...            [Computer Science]\n",
              "14   46200b99c40e8586c8a0f588488ab6414119fb28  ...            [Computer Science]\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "metadata": {
        "id": "zotFl5-ficaO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "fa1bc71f-f243-4a1f-a967-172e638aa4df"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. A segunda é atravez da api de busca, searchBox, no qual é possivel colocar os campos:"
      ],
      "metadata": {
        "id": "xwfe0ZCyZ8YD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "Resultados_2 = Search()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              " \n",
              "        <label for=\"query\">Buscar: </label>\n",
              "        <input type=\"text\" id=\"query\" value=\"Machine Learning+Deep Learning\" style=\"width: 400px;\"/>\n",
              "        <br/>\n",
              "        <br/>\n",
              "        \n",
              "        <label for=\"fields\">Fields: </label>\n",
              "        <input type=\"text\" id=\"fields\" value=\"title,abstract,isOpenAccess,fieldsOfStudy\" style=\"width: 400px;\"/>\n",
              "        <br/>\n",
              "        <br/>\n",
              " \n",
              "        <label for=\"limit\">Limite: </label>\n",
              "        <input type=\"text\" id=\"limit\" value=\"10\" style=\"width: 50px;\"/><br/>\n",
              "        <br/>\n",
              "\n",
              "        <label for=\"limit\">Offset: </label>\n",
              "        <input type=\"text\" id=\"offset\" value=\"0\" style=\"width: 50px;\"/><br/>\n",
              "        <br/>\n",
              "\n",
              "        <button id='button'>Pesquisar</button>\n",
              "        <br/>\n",
              "        <br/>\n",
              "           <script> document.querySelector('#button').onclick = async () => {\n",
              "            var search = document.getElementById(\"query\").value\n",
              "            var fields = document.getElementById(\"fields\").value\n",
              "            var limit = parseInt(document.getElementById(\"limit\").value)\n",
              "            var offset = parseInt(document.getElementById(\"offset\").value)\n",
              "                  \n",
              "            google.colab.kernel.invokeFunction('notebook.searching', [], {});\n",
              "\n",
              "            if (limit >100) {\n",
              "              var number = limit\n",
              "              var data = \"\"\n",
              "              var promises = []\n",
              "              var offsetSearch = 0\n",
              "              var rest = 0\n",
              "\n",
              "              for (let index = 0; index < Math.floor(limit/100); index++) {\n",
              "                offsetSearch = 100*(index) + offset + 1*(index!==0)\n",
              "\n",
              "\n",
              "                promises.push(\n",
              "                  fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offsetSearch}&limit=100&fields=${fields}`)\n",
              "    .then(res=> {return(res.json())})\n",
              "    .then(res=> {return(res)})\n",
              "                )\n",
              "              }\n",
              "              \n",
              "              if (limit%100 !== 0) { \n",
              "                rest= limit%100\n",
              "                offsetSearch = offsetSearch+100\n",
              "                \n",
              "                console.log(rest)\n",
              "                console.log(offsetSearch)\n",
              "\n",
              "                promises.push(\n",
              "                fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offsetSearch}&limit=${rest}&fields=${fields}`)\n",
              "    .then(res=> {return(res.json())})\n",
              "    .then(res=> {return(res)})\n",
              "                )}\n",
              "\n",
              "              await Promise.all(promises).then(data=>{\n",
              "                google.colab.kernel.invokeFunction('notebook.mergeData', [data], {})\n",
              "              })\n",
              "              .catch(err=> { return (google.colab.kernel.invokeFunction('notebook.error', [err], {})) })\n",
              "\n",
              "            } else {\n",
              "\n",
              "            await fetch(`https://api.semanticscholar.org/graph/v1/paper/search?query=${search}&offset=${offset}&limit=${limit}&fields=${fields}`)\n",
              "    .then(res=> {return(res.json())})\n",
              "    .then(res=> {\n",
              "      console.log(res)\n",
              "      console.log(\"AQUIII\")\n",
              "      return(google.colab.kernel.invokeFunction('notebook.AddListItem', [res], {}))})\n",
              "    .catch(err=> { return (\n",
              "      google.colab.kernel.invokeFunction('notebook.error', [err], {})) })\n",
              "            }\n",
              "                  }</script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Achou 652200 papers.\n",
            "\n",
            "Api devolveu >> 10 papers\n",
            "\n",
            "                                    paperId  ...       fieldsOfStudy\n",
            "0  846ff7afb7670d62f88b4a8cc99d306ffb81b075  ...          [Medicine]\n",
            "1  5dc53e50148b01fe8b9536eb79fa6b1dce924174  ...          [Medicine]\n",
            "2  7cc2e148d27a7508dd23c4e35eb63cc9b3e6a58f  ...  [Computer Science]\n",
            "3  59444b096f7c8a561d540102e8b5bfb189edabc6  ...                None\n",
            "4  eee313380ccb45807ea0afa3c1df86f6b48b8867  ...  [Computer Science]\n",
            "\n",
            "[5 rows x 5 columns]\n"
          ]
        }
      ],
      "metadata": {
        "id": "jCZf9v2yzskK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "fba90541-642b-4c5d-ed98-323245132691"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(Resultados_2.data)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                    paperId  ...       fieldsOfStudy\n",
            "0  846ff7afb7670d62f88b4a8cc99d306ffb81b075  ...          [Medicine]\n",
            "1  5dc53e50148b01fe8b9536eb79fa6b1dce924174  ...          [Medicine]\n",
            "2  7cc2e148d27a7508dd23c4e35eb63cc9b3e6a58f  ...  [Computer Science]\n",
            "3  59444b096f7c8a561d540102e8b5bfb189edabc6  ...                None\n",
            "4  eee313380ccb45807ea0afa3c1df86f6b48b8867  ...  [Computer Science]\n",
            "5  46479bbea7749cb2db35b139206039531327053c  ...  [Computer Science]\n",
            "6  b69fe5a837277ddbea5215d6bacd3a902e9d11ce  ...          [Medicine]\n",
            "7  b0bf64ccbd651e8c7bc141d8aabaecff562e93a1  ...  [Computer Science]\n",
            "8  042ab08ec6782cf217f13175162bfd48f7350114  ...  [Computer Science]\n",
            "9  03e7832982986159400a8eeab148487ffcfabe56  ...  [Computer Science]\n",
            "\n",
            "[10 rows x 5 columns]\n"
          ]
        }
      ],
      "metadata": {
        "id": "l8G0F7fEFz9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50abc23f-9658-4a53-ebbc-4f9c8065a050"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **SearchWeb()**"
      ],
      "metadata": {
        "id": "MNrq2PcNtb-f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import requests\r\n",
        "import json\r\n",
        "import multiprocessing as mp\r\n",
        "import os\r\n",
        "# import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import re\r\n",
        "import ast\r\n",
        "from pathlib import Path\r\n",
        "\r\n",
        "\r\n",
        "from time import sleep, time\r\n",
        "\r\n",
        "\r\n",
        "def timer(fun):\r\n",
        "  def warper(*args,**kwargs):\r\n",
        "    start = time()\r\n",
        "    d = fun(*args,**kwargs)\r\n",
        "    end = time()\r\n",
        "    print(f\"[{fun.__name__}]>> Demorou {round(end-start,2)}s\")\r\n",
        "    return d\r\n",
        "  return warper\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class SearchWeb():\r\n",
        "  def __init__(self, search=\"Machine Learning+Deep Learning\", poolCPU = 4, sleeptry=5, save = False, **kwargs):\r\n",
        "     \r\n",
        "    self.sleeptry = sleeptry\r\n",
        "    self.poolCPU = poolCPU\r\n",
        "    self.saveName = kwargs.get('Savename', \"Data\")\r\n",
        "\r\n",
        "    self.badcall = []\r\n",
        "    self._start = True\r\n",
        "\r\n",
        "    self.saveFile = save\r\n",
        "    self._search = search\r\n",
        "    self._sort = kwargs.get('sort', \"relevance\")\r\n",
        "    self._authors = kwargs.get('authors', [])\r\n",
        "    self._coAuthors = kwargs.get('coAuthors', [])\r\n",
        "    self._venues = kwargs.get('venues', ['PloS one', 'AAAI', 'Scientific reports', 'IEEE Access', 'ArXiv', 'Expert Syst. Appl.', 'ICML', 'Neurocomputing', 'Sensors', 'Remote. Sens.'])\r\n",
        "    self._yearFilter = kwargs.get('yearFilter', None) # {\"min\": 2008,\"max\": 2021}\r\n",
        "    self._requireViewablePdf = kwargs.get('requireViewablePdf', False)\r\n",
        "    self._publicationTypes = kwargs.get('publicationTypes', [\"ClinicalTrial\", \"CaseReport\", \"Editorial\",\"Study\",\"Book\",\"News\",\"Review\",\"Conference\",\"LettersAndComments\",\"JournalArticle\"])\r\n",
        "    self._fieldsOfStudy = kwargs.get('fieldsOfStudy', [\"biology\",\"art\",\"business\",\"computer-science\",\"chemistry\",\"economics\",\"engineering\",\"environmental-science\",\"geography\",\"geology\",\"history\",\"materials-science\",\"mathematics\",\"medicine\",\"philosophy\",\"physics\",\"political-science\",\"psychology\",\"sociology\"])\r\n",
        "    self._useFallbackRankerService = kwargs.get('useFallbackRankerService', False)\r\n",
        "    self._useFallbackSearchCluster = kwargs.get('useFallbackSearchCluster', False)\r\n",
        "    self._hydrateWithDdb = kwargs.get('hydrateWithDdb', True)\r\n",
        "    self._includeTldrs = kwargs.get('includeTldrs', True)\r\n",
        "    self._performTitleMatch = kwargs.get('performTitleMatch', True)\r\n",
        "    self._includeBadges = kwargs.get('includeBadges', True)\r\n",
        "    self._tldrModelVersion = kwargs.get('tldrModelVersion', 'v2.0.0')\r\n",
        "    self._getQuerySuggestions = kwargs.get('getQuerySuggestions', False)\r\n",
        "\r\n",
        "\r\n",
        "    self.post = {\r\n",
        "    \"page\": 1, \r\n",
        "    \"pageSize\": 10,\r\n",
        "    \"queryString\": self._search,\r\n",
        "    \"sort\": self._sort,\r\n",
        "    \"authors\": self._authors,\r\n",
        "    \"coAuthors\": self._coAuthors,\r\n",
        "    \"venues\": self._venues,\r\n",
        "    \"yearFilter\": self._yearFilter,\r\n",
        "    \"requireViewablePdf\": self._requireViewablePdf,\r\n",
        "    \"publicationTypes\": self._publicationTypes,\r\n",
        "    \"externalContentTypes\": [],\r\n",
        "    \"fieldsOfStudy\": self._fieldsOfStudy,\r\n",
        "    \"useFallbackRankerService\": self._useFallbackRankerService,\r\n",
        "    \"useFallbackSearchCluster\": self._useFallbackSearchCluster,\r\n",
        "    \"hydrateWithDdb\": self._hydrateWithDdb,\r\n",
        "    \"includeTldrs\": self._includeTldrs,\r\n",
        "    \"performTitleMatch\": self._performTitleMatch,\r\n",
        "    \"includeBadges\": self._includeBadges,\r\n",
        "    \"tldrModelVersion\": \"v2.0.0\",\r\n",
        "    \"getQuerySuggestions\": self._getQuerySuggestions,\r\n",
        "    }\r\n",
        "\r\n",
        "  \r\n",
        "  \r\n",
        "  def _paperExtract(self, data):\r\n",
        "    p = {\r\n",
        "        \"authors\": [author[0]['name'] for author in data.get('authors',[{'name':None},None])],\r\n",
        "        \"id\": data.get('id',None),\r\n",
        "        \"socialLinks\": data.get('socialLinks',None),\r\n",
        "        \"title\": data.get('title',{'text':None})['text'],\r\n",
        "        \"paperAbstract\": data.get('paperAbstract',{'text':None})['text'],\r\n",
        "        \"year\": data.get('year',{'text':None})['text'],\r\n",
        "        \"venue\": data.get('venue',{'text':None})['text'],\r\n",
        "        \"citationContexts\":data.get('citationContexts',None),\r\n",
        "        \"citationStats\": data.get('citationStats',None),\r\n",
        "        \"sources\":data.get('sources',None),\r\n",
        "        \"externalContentStats\":data.get('externalContentStats',None),\r\n",
        "        \"journal\":data.get('journal',None),\r\n",
        "        \"presentationUrls\":data.get('presentationUrls',None),\r\n",
        "        \"links\": data.get('links',None),\r\n",
        "        \"primaryPaperLink\": data.get('primaryPaperLink',None),\r\n",
        "        \"alternatePaperLinks\": data.get('alternatePaperLinks',None),\r\n",
        "        \"entities\": [author['name'] for author in data.get('entities',[{'name':None}])],\r\n",
        "        \"entityRelations\": data.get('entityRelations',None),\r\n",
        "        \"blogs\":data.get('blogs',None),\r\n",
        "        \"videos\":data.get('videos',None),\r\n",
        "        \"githubReferences\": data.get('githubReferences',None),\r\n",
        "        \"scorecardStats\": data.get('scorecardStats',None),\r\n",
        "        \"fieldsOfStudy\":data.get('fieldsOfStudy',None),\r\n",
        "        \"pubDate\":data.get('pubDate',None),\r\n",
        "        \"pubUpdateDate\":data.get('pubUpdateDate',None),\r\n",
        "        \"badges\":data.get('badges',None),\r\n",
        "        \"tldr\":data.get('tldr',None)\r\n",
        "        }\r\n",
        "    return p\r\n",
        "\r\n",
        "  def _query(self, page):\r\n",
        "    url = \"https://www.semanticscholar.org/api/1/search\"\r\n",
        "    post = self.post.copy()\r\n",
        "    post[\"page\"] = page\r\n",
        "    try:\r\n",
        "      res = requests.post(url, json=post, timeout=15)\r\n",
        "      res.encoding = 'utf-8'\r\n",
        "      return [res, page, res.status_code]\r\n",
        "    except:\r\n",
        "      return [None, page, 400 ]\r\n",
        "      \r\n",
        "\r\n",
        "  def _json(self, res):\r\n",
        "    print(res.text)\r\n",
        "    return json.loads(res.text).copy()\r\n",
        "\r\n",
        "    # c['querySuggestions']\r\n",
        "    # c['totalPages']\r\n",
        "    # c['totalResults']\r\n",
        "  def save(self, name, data):\r\n",
        "    try:\r\n",
        "      with open(f'./{name}.json', 'w',encoding='UTF-8') as fp:\r\n",
        "          json.dump(data, fp)\r\n",
        "    except:\r\n",
        "      print(\"[Save]>> Error to save the data.\")\r\n",
        "  \r\n",
        "  def load_json(self, path):\r\n",
        "    try:\r\n",
        "      with open(f'{path}', 'r', encoding='UTF-8') as fp:\r\n",
        "          return json.load(fp)\r\n",
        "    except:\r\n",
        "      print(\"[load_json]>> Error to load json file.\")\r\n",
        "    \r\n",
        "  def _startFile(self, find):\r\n",
        "    jsonFile = Path(f\"./{self.saveName}.json\")\r\n",
        "    textFile = Path(f\"./{self.saveName}.text\")\r\n",
        "    \r\n",
        "    if jsonFile.is_file():\r\n",
        "      if find:\r\n",
        "        try:\r\n",
        "          print(f\"[_startFile] >> Loading ./{self.saveName}.json\")\r\n",
        "          with open(f'./{self.saveName}.json', 'r',encoding='UTF-8') as f:\r\n",
        "            data = json.load(f)\r\n",
        "          print(f\"[Create] >> Creating a ./{self.saveName}.text file to save data.\")\r\n",
        "          with open(f'./{self.saveName}.text', 'w',encoding='UTF-8') as fp:\r\n",
        "            fp.write(\"{\\\"Results\\\": [\")\r\n",
        "        \r\n",
        "        # print(data['Results'])\r\n",
        "          self._save(data['Results'])\r\n",
        "        except:\r\n",
        "          print(f\"[_startFile] >> Fail to load ./{self.saveName}.json\")\r\n",
        "          try:\r\n",
        "            # print(f\"[Create] >> Creating a ./{self.saveName}.text file to save data.\")\r\n",
        "            with open(f'./{self.saveName}.text', 'w', encoding='UTF-8') as fp:\r\n",
        "              fp.write(\"{\\\"Results\\\": [\")\r\n",
        "          except:\r\n",
        "            print(\"[Create] >> Fail\")\r\n",
        "      else:\r\n",
        "        return False\r\n",
        "    else:\r\n",
        "      try:\r\n",
        "        print(f\"[Create] >> Creating a ./{self.saveName}.text file to save data.\")\r\n",
        "        with open(f'./{self.saveName}.text', 'w',encoding='UTF-8') as fp:\r\n",
        "          fp.write(\"{\\\"Results\\\": [\")\r\n",
        "      except:\r\n",
        "        print(\"[Create] >> Fail\")\r\n",
        "    return True\r\n",
        "\r\n",
        "\r\n",
        "  def _save(self, check_point):\r\n",
        "    if str(check_point) == '[]' or str(check_point) == '[,]':\r\n",
        "      return _\r\n",
        "    else:\r\n",
        "      text = str(check_point)\r\n",
        "\r\n",
        "      text = re.sub('^\\[', '', text)\r\n",
        "      text = re.sub('\\]$', '', text)\r\n",
        "\r\n",
        "      \r\n",
        "      with open(f'./{self.saveName}.text', 'a', encoding='utf-8') as fp:\r\n",
        "        fp.write(text)\r\n",
        "          # json.dump(self.all['Results'], fp)\r\n",
        "      print(f\"[Save] >> Saving check_point at current directory, ./{self.saveName}.text\")\r\n",
        "      \r\n",
        "\r\n",
        "  def _endFile(self):\r\n",
        "    # ast.literal_eval(text)\r\n",
        "    try:\r\n",
        "      with open(f'./{self.saveName}.text', 'a', encoding='UTF-8') as fp:\r\n",
        "        fp.write(']}')\r\n",
        "      \r\n",
        "      try:\r\n",
        "        with open(f'./{self.saveName}.text', 'r', encoding='UTF-8') as fp:\r\n",
        "          text = fp.read()\r\n",
        "          text_dict = ast.literal_eval(text)\r\n",
        "        \r\n",
        "      \r\n",
        "      # os.rename(f'./{self.saveName}.text', f'./{self.saveName}.json')\r\n",
        "      # os.remove(f\"./{self.saveName}.text\")\r\n",
        "      # print(text_dict)\r\n",
        "        with open(f'./{self.saveName}.json', 'w', encoding='UTF-8') as fp:\r\n",
        "          json.dump(text_dict, fp)\r\n",
        "        print(f\"[Close] >> Closed and save in ./{self.saveName}.json file the data.\")\r\n",
        "      except:\r\n",
        "        print(f\"[Close] >> Fail to save the data ./{self.saveName}.json file.\")\r\n",
        "\r\n",
        "    except:\r\n",
        "      print('[Close] >> Fail')\r\n",
        "\r\n",
        "  \r\n",
        "\r\n",
        "  @timer\r\n",
        "  def _extract(self, pool, data):\r\n",
        "    try:\r\n",
        "      # print(\"data\")\r\n",
        "      # print(data)\r\n",
        "\r\n",
        "      # print(\"data['Response'].tolist()\")\r\n",
        "      # print(data['Response'].tolist())\r\n",
        "      self.papers_text = pool.map(self._json, data['Response'].tolist())\r\n",
        "      # print(\"self.papers_text\")\r\n",
        "      # print(self.papers_text)\r\n",
        "\r\n",
        "      if self._start:\r\n",
        "        print('\\n ---')\r\n",
        "        print(f\"Total Results: {self.papers_text[0]['totalResults']}\")\r\n",
        "        print(f\"Total Pages: {self.papers_text[0]['totalPages']}\")\r\n",
        "        print(f\"Query Suggestions: {self.papers_text[0]['querySuggestions']}\")\r\n",
        "        print('--- \\n')\r\n",
        "        self.totalPages = self.papers_text[0]['totalPages']\r\n",
        "        self.totalResults = self.papers_text[0]['totalResults']\r\n",
        "        self._start = False\r\n",
        "\r\n",
        "      \r\n",
        "      print(\"[_extract] >> extracting relevant data.\")\r\n",
        "      check_point= [{\"Page\": {\"N_Page\":page['query']['page'],\r\n",
        "                                   \"N_Papers\":len(page['results']),\r\n",
        "                                   \"Papers\": pool.map(self._paperExtract,\r\n",
        "                                                      page['results'])}} for page in self.papers_text]\r\n",
        "\r\n",
        "\r\n",
        "      # print(check_point)\r\n",
        "      if self.saveFile:\r\n",
        "        try:\r\n",
        "          self._save(check_point)\r\n",
        "        except:\r\n",
        "          print(\"_save >> [Fail] to save.\")\r\n",
        "          print(\"_extract>> [Fail], see .badcall to reextract content.\")\r\n",
        "          self.badcall.append(self.papers_text)\r\n",
        "          # print(self.badcall)\r\n",
        "      else:\r\n",
        "        self.all[\"Results\"].extend(check_point)\r\n",
        "\r\n",
        "\r\n",
        "    except:\r\n",
        "      print(\"_extract>> [Fail], see .badcall to reextract content.\")\r\n",
        "      self.badcall.append(self.papers_text)\r\n",
        "      print(self.badcall)\r\n",
        "    \r\n",
        "    \r\n",
        "\r\n",
        "  # def _data(self, data):\r\n",
        "  #   if type(self.datasource) == str:\r\n",
        "  #     self.datasource = data\r\n",
        "  #   else:\r\n",
        "  #     self.datasource = pd.concat([self.datasource, data])\r\n",
        "\r\n",
        "  @timer\r\n",
        "  def _runtime(self, pool, pages):\r\n",
        "    self.totalPages = 0\r\n",
        "\r\n",
        "    find = False\r\n",
        "    \r\n",
        "    while True:\r\n",
        "      if self.saveFile:\r\n",
        "        close = self._startFile(find)\r\n",
        "      \r\n",
        "      print('\\n')\r\n",
        "      print('[_runtime]>> Start searching...')\r\n",
        "      # print(self.totalPages)\r\n",
        "      # print(self.totalResults)\r\n",
        "      # print(self.n)\r\n",
        "\r\n",
        "      if self.totalResults < self.n:\r\n",
        "        self.n = self.totalResults\r\n",
        "        pages = list(range(self._page, self.totalPages))\r\n",
        "      \r\n",
        "      try:\r\n",
        "        res = pool.map(self._query, pages)\r\n",
        "        # print(res)\r\n",
        "        resultData = pd.DataFrame(res, columns=[\"Response\", \"Page\", \"Code\"])\r\n",
        "        resultData.set_index(\"Page\")\r\n",
        "        \r\n",
        "        if resultData.query(\"Code !=200\").size == 0:\r\n",
        "          # self._data(resultData)\r\n",
        "          self._extract(pool, resultData.query(\"Code ==200\"))\r\n",
        "          if self.saveFile:\r\n",
        "            if close:\r\n",
        "              self._endFile()\r\n",
        "              find = True\r\n",
        "          break\r\n",
        "        else:\r\n",
        "          find = False\r\n",
        "\r\n",
        "          if resultData.query(\"Code ==200\").size != 0:\r\n",
        "            self._extract(pool, resultData.query(\"Code ==200\"))\r\n",
        "            if self.saveFile:\r\n",
        "              if close:\r\n",
        "                find = True\r\n",
        "                self._endFile()\r\n",
        "              \r\n",
        "          print(\"Bad call of pages:\")\r\n",
        "          # self._data(resultData.query(\"Code == 200\"))\r\n",
        "          # self.datasource.append(resultData.query(\"Code ==200\"))\r\n",
        "          pages = resultData.query(\"Code !=200\")[\"Page\"].values.tolist()\r\n",
        "          print(pages)\r\n",
        "          try:\r\n",
        "            with open(\"./BadCalls.text\", 'w', encoding='UTF-8') as fp:\r\n",
        "              fp.write(str(pages))\r\n",
        "          except:\r\n",
        "            print(\"Fail to save badcalls\")\r\n",
        "          print(f\"Tentando de novo daqui a {self.sleeptry/60} min...\")\r\n",
        "\r\n",
        "\r\n",
        "          sleep(self.sleeptry)\r\n",
        "      except:\r\n",
        "        pass\r\n",
        "      print(\"---\")\r\n",
        "        \r\n",
        "        \r\n",
        "    \r\n",
        "    \r\n",
        "    # self._extract(pool, self.datasource)\r\n",
        "\r\n",
        "\r\n",
        "  @timer\r\n",
        "  def get(self, n = 10, page = 1, pages = []):\r\n",
        "    self._pages = pages\r\n",
        "    self.n = n\r\n",
        "    self._page = page\r\n",
        "    self.totalResults = 1000000000000000000000\r\n",
        "    self.post[\"pageSize\"] = 10\r\n",
        "    self.post[\"page\"] = page\r\n",
        "    self.all = {\"Results\": []}\r\n",
        "    print('.post >>')\r\n",
        "    print(self.post)\r\n",
        "    # self.datasource = ''\r\n",
        "    print(\"\\n\")\r\n",
        "    print(\"Searching...\")\r\n",
        "    print(self.all)\r\n",
        "\r\n",
        "    \r\n",
        "\r\n",
        "    with mp.Pool(self.poolCPU) as pool:\r\n",
        "      if self.n > 10:\r\n",
        "    \r\n",
        "        if len(pages) != 0:\r\n",
        "          self._pages = pages\r\n",
        "        else:  \r\n",
        "          self._pages = list(range(self._page, (self.n//10)+self._page))\r\n",
        "      # for page in range(n//10):\r\n",
        "        self._runtime(pool, self._pages)\r\n",
        "          \r\n",
        "        if n%10>0:\r\n",
        "          self._pages = [self.n//10+self._page]\r\n",
        "          self.post[\"pageSize\"] = self.n%10\r\n",
        "\r\n",
        "          self._runtime(pool, self._pages)\r\n",
        "          \r\n",
        "      else:\r\n",
        "        # pass\r\n",
        "        self._pages = [self._page]\r\n",
        "        self.post[\"page\"] = self._page\r\n",
        "        self.post[\"pageSize\"] = self.n\r\n",
        "\r\n",
        "        self._runtime(pool, self._pages)\r\n",
        "\r\n",
        "    \r\n",
        "      # self._extract(pool, self.datasource)\r\n",
        "\r\n",
        "\r\n",
        "##### Description #####\r\n",
        "# ex. {\"params\": value} \r\n",
        "#  \r\n",
        "##### Params that can pass in SearchWeb().get(params = value): #####\r\n",
        "#     {\r\n",
        "#     \"n\": 1000 (how much papers)\r\n",
        "#     \"page\": 1, (where start search)\r\n",
        "#      }\r\n",
        "##### Params that can pass in SearchWeb(params = value): #####\r\n",
        "# data = '''{\r\n",
        "#     \"Savename\": 'Data'\r\n",
        "#     \"sleeptry\": 3 (seconds)\r\n",
        "#     \"poolCPU\": 4 (Number of clusters, CPU)\r\n",
        "#     \"save\": False\r\n",
        "#     \"queryString\": \"Machine Learning+Deep Learning\",\r\n",
        "#     \"sort\": \"total-citations\", #influence #\"pub-date\" #relevance\r\n",
        "#     \"authors\": [],\r\n",
        "#     \"coAuthors\": [],\r\n",
        "#     \"venues\": [\r\n",
        "#         \"PloS one\",\r\n",
        "#         \"AAAI\",\r\n",
        "#         \"Scientific reports\",\r\n",
        "#         \"IEEE Access\",\r\n",
        "#         \"ArXiv\",\r\n",
        "#         \"Expert Syst. Appl.\",\r\n",
        "#         \"ICML\",\r\n",
        "#         \"Neurocomputing\",\r\n",
        "#         \"Sensors\",\r\n",
        "#         \"Remote. Sens.\"\r\n",
        "#     ],\r\n",
        "#     \"yearFilter\": {\r\n",
        "#         \"min\": 2008,\r\n",
        "#         \"max\": 2021\r\n",
        "#     },\r\n",
        "#     \"requireViewablePdf\": True,\r\n",
        "#     \"publicationTypes\": [\r\n",
        "#         \"ClinicalTrial\",\r\n",
        "#         \"CaseReport\",\r\n",
        "#         \"Editorial\",\r\n",
        "#         \"Study\",\r\n",
        "#         \"Book\",\r\n",
        "#         \"News\",\r\n",
        "#         \"Review\",\r\n",
        "#         \"Conference\",\r\n",
        "#         \"LettersAndComments\",\r\n",
        "#         \"JournalArticle\"\r\n",
        "#     ],\r\n",
        "#     \"externalContentTypes\": [],\r\n",
        "#     \"fieldsOfStudy\": [\r\n",
        "#         \"biology\",\r\n",
        "#         \"art\",\r\n",
        "#         \"business\",\r\n",
        "#         \"computer-science\",\r\n",
        "#         \"chemistry\",\r\n",
        "#         \"economics\",\r\n",
        "#         \"engineering\",\r\n",
        "#         \"environmental-science\",\r\n",
        "#         \"geography\",\r\n",
        "#         \"geology\",\r\n",
        "#         \"history\",\r\n",
        "#         \"materials-science\",\r\n",
        "#         \"mathematics\",\r\n",
        "#         \"medicine\",\r\n",
        "#         \"philosophy\",\r\n",
        "#         \"physics\",\r\n",
        "#         \"political-science\",\r\n",
        "#         \"psychology\",\r\n",
        "#         \"sociology\"\r\n",
        "#     ],\r\n",
        "#     \"useFallbackRankerService\": False,\r\n",
        "#     \"useFallbackSearchCluster\": False,\r\n",
        "#     \"hydrateWithDdb\": True,\r\n",
        "#     \"includeTldrs\": True,\r\n",
        "#     \"performTitleMatch\": True,\r\n",
        "#     \"includeBadges\": True,\r\n",
        "#     \"tldrModelVersion\": \"v2.0.0\",\r\n",
        "#     \"getQuerySuggestions\": False\r\n",
        "# }\r\n",
        "# '''\r\n",
        "# '''\r\n",
        "# Obs. Params that have a list can be a empty list\r\n",
        "# Ex. {\"venues\": []}\r\n",
        "\r\n",
        "### Discoment here to have a script\r\n",
        "# if __name__ == '__main__':\r\n",
        "#   SearchWeb(\r\n",
        "#     search= \"decision making+optimization+artificial intelligence\",\r\n",
        "#     sort= \"influence\",\r\n",
        "#     Savename = \"influence\",\r\n",
        "#     save=True,\r\n",
        "#     poolCPU = 4,\r\n",
        "#     sleeptry = 3.5*60,\r\n",
        "#     venues = [],\r\n",
        "#     publicationTypes = ['JournalArticle'],\r\n",
        "#     fieldsOfStudy = [],\r\n",
        "#     getQuerySuggestions = True\r\n",
        "#     ).get(20000, page = 1)\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "r_OkAhJHt3tS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "from_Webpage = SearchWeb(search= \"Machine Learning+Deep Learning\", sort= \"total-citations\", save=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".post >>\n",
            "{'page': 1, 'pageSize': 10, 'queryString': 'Machine Learning+Deep Learning', 'sort': 'total-citations', 'authors': [], 'coAuthors': [], 'venues': ['PloS one', 'AAAI', 'Scientific reports', 'IEEE Access', 'ArXiv', 'Expert Syst. Appl.', 'ICML', 'Neurocomputing', 'Sensors', 'Remote. Sens.'], 'yearFilter': None, 'requireViewablePdf': False, 'publicationTypes': ['ClinicalTrial', 'CaseReport', 'Editorial', 'Study', 'Book', 'News', 'Review', 'Conference', 'LettersAndComments', 'JournalArticle'], 'externalContentTypes': [], 'fieldsOfStudy': ['biology', 'art', 'business', 'computer-science', 'chemistry', 'economics', 'engineering', 'environmental-science', 'geography', 'geology', 'history', 'materials-science', 'mathematics', 'medicine', 'philosophy', 'physics', 'political-science', 'psychology', 'sociology'], 'useFallbackRankerService': False, 'useFallbackSearchCluster': False, 'hydrateWithDdb': True, 'includeTldrs': True, 'performTitleMatch': True, 'includeBadges': True, 'tldrModelVersion': 'v2.0.0', 'getQuerySuggestions': False}\n"
          ]
        }
      ],
      "metadata": {
        "id": "n_j2uslvt8A2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a6a0b66-157d-4751-a28f-bf7b2689b32d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "# Retorna 100 papers a partir da pag. 2 com base nos parametros passados em SearchWeb() que constitui ().post\r\n",
        "from_Webpage.get(100, page = 2)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching...\n",
            "{'Results': []}\n",
            "\n",
            "[Create] >> Creating a ./Data.text file to save data.\n",
            "[Save] >> Save at current directory, ./Data.text\n",
            "\n",
            " ---\n",
            "Total Results: 51139\n",
            "Total Pages: 5113\n",
            "Query Suggestions: []\n",
            "--- \n",
            "\n",
            "[_extract]>> Demorou 2.15s\n",
            "[Close] >> Close and save ./Data.json file to save data.\n",
            "[_runtime]>> Demorou 7.51s\n",
            "[get]>> Demorou 7.68s\n"
          ]
        }
      ],
      "metadata": {
        "id": "UzUBus_Vt9-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01fa0752-72f1-4726-ba46-40de43f88030"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Tudo que vem com base em 1 paper (dict.)\r\n",
        "from_Webpage.all[\"Results\"][0][\"Papers\"][0]\r\n",
        "\r\n",
        "json_file = SearchWeb().load_json(\"JsonFile.json\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "A_AxXsbJuARM"
      }
    }
  ]
}